<!DOCTYPE html>
<html lang="en" class="grade-c">
<head>
    <title>Chunking as a rational solution to the speed–accuracy trade-off in a serial reaction time task | Scientific Reports</title>
    
        
<link rel="alternate" type="application/rss+xml" href="https://www.nature.com/srep.rss"/>


    

<link rel="preconnect" href="https://cmp.nature.com" crossorigin>

<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="applicable-device" content="pc,mobile">
<meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=5,user-scalable=yes">
<meta name="360-site-verification" content="5a2dc4ab3fcb9b0393241ffbbb490480" />

<script data-test="dataLayer">
    window.dataLayer = [{"content":{"category":{"contentType":"article","legacy":{"webtrendsPrimaryArticleType":"research","webtrendsSubjectTerms":"cognitive-neuroscience;computational-neuroscience;human-behaviour;learning-and-memory;psychology","webtrendsContentCategory":null,"webtrendsContentCollection":null,"webtrendsContentGroup":"Scientific Reports","webtrendsContentGroupType":null,"webtrendsContentSubGroup":"Article","status":null}},"article":{"doi":"10.1038/s41598-023-31500-3"},"attributes":{"cms":null,"deliveryPlatform":"oscar","copyright":{"open":true,"legacy":{"webtrendsLicenceType":"http://creativecommons.org/licenses/by/4.0/"}}},"contentInfo":{"authors":["Shuchen Wu","Noémi Éltető","Ishita Dasgupta","Eric Schulz"],"publishedAt":1683763200,"publishedAtString":"2023-05-11","title":"Chunking as a rational solution to the speed–accuracy trade-off in a serial reaction time task","legacy":null,"publishedAtTime":null,"documentType":"aplusplus","subjects":"Cognitive neuroscience,Computational neuroscience,Human behaviour,Learning and memory,Psychology"},"journal":{"pcode":"srep","title":"scientific reports","volume":"13","issue":"1","id":41598,"publishingModel":"Open Access"},"authorization":{"status":true},"features":[{"name":"furtherReadingSection","present":true}],"collection":null},"page":{"category":{"pageType":"article"},"attributes":{"template":"mosaic","featureFlags":[{"name":"download-collection-test","active":false},{"name":"download-issue-test","active":false},{"name":"nature-onwards-journey","active":false}],"testGroup":null},"search":null},"privacy":{},"version":"1.0.0","product":null,"session":null,"user":null,"backHalfContent":true,"country":"DE","hasBody":true,"uneditedManuscript":false,"twitterId":["o3xnx","o43y9","o3ef7"],"baiduId":"d38bce82bcb44717ccc29a90c4b781ea","japan":false}];
    window.dataLayer.push({
        ga4MeasurementId: 'G-ERRNTNZ807',
        ga360TrackingId: 'UA-71668177-1',
        twitterId: ['3xnx', 'o43y9', 'o3ef7'],
        baiduId: 'd38bce82bcb44717ccc29a90c4b781ea',
        ga4ServerUrl: 'https://sgtm.nature.com',
        imprint: 'nature'
    });
</script>


<script>
    (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
</script>
 



     
    
    
        
    
    <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  .c-card--major .c-card__title,.u-h1,.u-h2,h1,h2,h2.app-access-wall__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-weight:700}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,.c-reading-companion__figure-title,.u-h3,.u-h4,h3,h4,h5,h6{letter-spacing:-.0117156rem}html{line-height:1.15;text-size-adjust:100%;box-sizing:border-box;font-size:100%;height:100%;overflow-y:scroll}body{background:#eee;color:#222;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;line-height:1.76;margin:0;min-height:100%}details,main{display:block}h1{font-size:2em;margin:.67em 0}a,sup{vertical-align:baseline}a{background-color:transparent;color:#069;overflow-wrap:break-word;text-decoration:underline .0625rem;text-decoration-skip-ink:auto;text-underline-offset:.08em;word-break:break-word}b{font-weight:bolder}sup{font-size:75%;line-height:0;position:relative;top:-.5em}img{border:0;height:auto;max-width:100%;vertical-align:middle}button,input,select{font-family:inherit;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}[type=submit],button{appearance:button}[type=checkbox]{box-sizing:border-box;padding:0}summary{display:list-item}[hidden]{display:none}button{border-radius:0;cursor:pointer}h1{font-size:min(max(1.5rem,4vw),2rem);font-weight:700;letter-spacing:min(max(-.0117156rem,4vw),-.0390625rem);line-height:min(max(1.6rem,4vw),2.25rem)}.c-card--major .c-card__title,.u-h1,.u-h2,button,h1,h2,h2.app-access-wall__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-card--major .c-card__title,.u-h2,h2{font-size:min(max(1.25rem,3.5vw),1.5rem);font-weight:700;letter-spacing:-.0117156rem;line-height:min(max(1.4rem,3.5vw),1.6rem)}.u-h3{letter-spacing:-.0117156rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,.c-reading-companion__figure-title,.u-h3,.u-h4,h3,h4,h5,h6{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:min(max(1.125rem,3vw),1.25rem);font-weight:700;line-height:1.4rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-reading-companion__figure-title,.u-h4,h3,h4,h5,h6{letter-spacing:-.0117156rem}.c-reading-companion__figure-title,.u-h4{font-size:min(max(1rem,2.5vw),1.125rem)}input+label{padding-left:.5em}nav ol,nav ul{list-style:none none}p:empty{display:none}.app-cta-group{display:flex;flex-direction:column;gap:0 16px;width:100%}@media only screen and (min-width:540px){.app-cta-group{align-items:baseline;flex-direction:row}}.app-cta-group>*{flex:1 1 auto}@media only screen and (min-width:540px){.app-cta-group>*{flex:0 1 auto}}.c-nature-box{background-color:#fff;border:1px solid #d5d5d5;border-radius:2px;box-shadow:0 0 5px 0 rgba(51,51,51,.1);line-height:1.3;margin-bottom:24px;padding:16px 16px 3px}.c-nature-box__text{font-size:1rem;margin-bottom:16px}.c-nature-box--access-to-pdf{display:none}@media only screen and (min-width:1024px){.c-nature-box--mobile{display:none}}.c-nature-box .c-pdf-download{margin-bottom:16px!important}.c-nature-box svg+.c-article__button-text{margin-left:8px}.c-nature-box--version{background-color:#eee}.c-nature-box__wrapper{transform:translateZ(0)}.c-nature-box__wrapper--placeholder{min-height:165px}.sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.article-page{background:#fff}p{overflow-wrap:break-word;word-break:break-word}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;padding:0}.c-article-identifiers__item{list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{margin-right:0;padding-right:0}.c-article-title{font-size:1.5rem;line-height:1.25;margin:0 0 16px}@media only screen and (min-width:768px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list svg{margin-left:4px}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:539px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#069;border-color:transparent;color:#fff}.c-article-info-details{font-size:1rem;margin-bottom:8px;margin-top:16px}.c-article-info-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3}.c-article-metrics-bar__wrapper{margin:16px 0}.c-article-metrics-bar__item{align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-style:normal;font-weight:400;margin:0 10px 0 5px}.c-article-metrics-bar__details{margin:0}.c-article-main-column{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:1.25rem;margin:0;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{font-size:1.5rem;line-height:1.24}}.c-article-associated-content__container .c-article-associated-content__title{margin-bottom:8px}.c-article-body p{margin-bottom:24px;margin-top:0}.c-article-section{clear:both}.c-article-section__content{margin-bottom:40px;padding-top:8px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-code-block{border:1px solid #fff;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#069;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-editorial-summary__container{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem}.c-article-editorial-summary__container .c-article-editorial-summary__content p:last-child{margin-bottom:0}.c-article-editorial-summary__container .c-article-editorial-summary__content--less{max-height:9.5rem;overflow:hidden}.c-article-editorial-summary__container .c-article-editorial-summary__button.active,.c-article-editorial-summary__container .c-article-editorial-summary__button.hover,.c-article-editorial-summary__container .c-article-editorial-summary__button:active,.c-article-editorial-summary__container .c-article-editorial-summary__button:hover{text-decoration:underline;text-decoration-skip-ink:auto}.c-article-editorial-summary__container .c-article-editorial-summary__button{background-color:#fff;border:0;color:#069;font-size:.875rem;margin-bottom:16px}.c-article-associated-content__container .c-article-associated-content__collection-label{font-size:.875rem;line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__sticky{max-width:389px}.c-reading-companion__scroll-pane{margin:0;min-height:200px;overflow:hidden auto}.c-reading-companion__tabs{display:flex;flex-flow:row;font-size:1rem;list-style:none;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{flex-grow:1}.c-reading-companion__tab{background-color:#eee;border:1px solid #d5d5d5;border-image:initial;border-left-width:0;color:#0067c5;font-size:1rem;padding:8px 8px 8px 15px;text-align:left;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{background-color:#fff;border-bottom:1px solid #fff;color:#222;font-weight:700}.c-reading-companion__sections-list{list-style:none;padding:0}.c-reading-companion__figures-list,.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__sections-list{margin:0 0 8px;min-height:50px}.c-reading-companion__section-item{font-size:1rem;padding:0}.c-reading-companion__section-item a{display:block;line-height:1.5;overflow:hidden;padding:8px 0 8px 16px;text-overflow:ellipsis;white-space:nowrap}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:8px 8px 8px 16px}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{align-items:center;display:flex}.c-reading-companion__figure-full-link svg{height:.8em;margin-left:2px}.c-reading-companion__panel{border-top:none;display:none;margin-top:0;padding-top:0}.c-cod,.c-reading-companion__panel--active{display:block}.c-cod{font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex:1 1 auto;margin:0;padding:13px}.c-cod__input--submit{background-color:#069;border:1px solid #069;color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out,color .2s ease-out}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff;color:#069}.c-pdf-download__link .u-icon{padding-top:2px}.c-pdf-download{display:flex;margin-bottom:24px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{display:flex;flex:1 1 0%}.c-pdf-download__link:hover{text-decoration:none}.c-pdf-download__text{padding-right:4px}@media only screen and (max-width:539px){.c-pdf-download__text{text-transform:capitalize}}@media only screen and (min-width:540px){.c-pdf-download__text{padding-right:8px}}.c-context-bar--sticky .c-pdf-download{display:block;margin-bottom:0;white-space:nowrap}@media only screen and (max-width:539px){.c-pdf-download .u-sticky-visually-hidden{border:0;clip:rect(0,0,0,0);height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}}.c-pdf-container{display:flex;justify-content:flex-end}@media only screen and (max-width:539px){.c-pdf-container .c-pdf-download{display:flex;flex-basis:100%}}.c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:16px}.c-article-extras .c-pdf-container .c-pdf-download{width:100%}.c-article-extras .c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:0}@media only screen and (min-width:540px){.c-context-bar--sticky .c-pdf-download__link{align-items:center;flex:1 1 183px}}@media only screen and (max-width:320px){.c-context-bar--sticky .c-pdf-download__link{padding:16px}}.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:none}@media only screen and (max-width:1023px){.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:block}}.c-recommendations-column-switch .c-meta{margin-top:auto}.c-context-bar{box-shadow:0 0 10px 0 rgba(51,51,51,.2);position:relative;width:100%}.c-context-bar__container{margin:0 auto;max-width:1280px;padding:0 16px}.c-context-bar__title{display:none}.app-researcher-popup__link.hover,.app-researcher-popup__link.visited,.app-researcher-popup__link:hover,.app-researcher-popup__link:visited,.c-article-metrics__heading a,.c-article-metrics__posts .c-card__title a{color:inherit}.c-article-authors-search__list{align-items:center;display:flex;flex-wrap:wrap;gap:16px;justify-content:center}@media only screen and (min-width:320px){.c-article-authors-search__list{justify-content:normal}}.c-article-authors-search__text{align-items:center;display:flex;flex-flow:column wrap;font-size:14px;justify-content:center}@media only screen and (min-width:320px){.c-article-authors-search__text{flex-direction:row;font-size:16px}}.c-article-authors-search__links-text{font-weight:700;margin-right:8px;text-align:center}@media only screen and (min-width:320px){.c-article-authors-search__links-text{text-align:left}}.c-article-authors-search__list-item--left{flex:1 1 100%}@media only screen and (min-width:320px){.c-article-authors-search__list-item--left{flex-basis:auto}}.c-article-authors-search__list-item--right{flex:1 1 auto}.c-article-identifiers{margin:0}.c-article-identifiers__item{border-right:2px solid #cedbe0;color:#222;font-size:14px}@media only screen and (min-width:320px){.c-article-identifiers__item{font-size:16px}}.c-article-identifiers__item:last-child{border-right:none}.c-article-metrics__posts .c-card__title{font-size:1.05rem}.c-article-metrics__posts .c-card__title+span{color:#6f6f6f;font-size:1rem}.app-author-list{color:#222;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;line-height:1.4;list-style:none;margin:0;padding:0}.app-author-list>li,.c-breadcrumbs>li,.c-footer__links>li,.js .app-author-list,.u-list-comma-separated>li,.u-list-inline>li{display:inline}.app-author-list>li:not(:first-child):not(:last-child):before{content:", "}.app-author-list>li:not(:only-child):last-child:before{content:" & "}.app-author-list--compact{font-size:.875rem;line-height:1.4}.app-author-list--truncated>li:not(:only-child):last-child:before{content:" ... "}.js .app-author-list__hide{display:none;visibility:hidden}.js .app-author-list__hide:first-child+*{margin-block-start:0}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}.js .u-show-following-ad+.c-ad--728x90{display:block}}.c-ad__label{color:#333;font-weight:400;line-height:1.5;margin-bottom:4px}.c-ad__label,.c-meta{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem}.c-meta{color:inherit;line-height:1.4;list-style:none;margin:0;padding:0}.c-meta--large{font-size:1rem}.c-meta--large .c-meta__item{margin-bottom:8px}.c-meta__item{display:inline-block;margin-bottom:4px}.c-meta__item:not(:last-child){border-right:1px solid #d5d5d5;margin-right:4px;padding-right:4px}@media only screen and (max-width:539px){.c-meta__item--block-sm-max{display:block}.c-meta__item--block-sm-max:not(:last-child){border-right:none;margin-right:0;padding-right:0}}@media only screen and (min-width:1024px){.c-meta__item--block-at-lg{display:block}.c-meta__item--block-at-lg:not(:last-child){border-right:none;margin-right:0;padding-right:0}}.c-meta__type{font-weight:700;text-transform:none}.c-skip-link{background:#069;color:#fff;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;inset:0 0 auto;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out}}@media print{.c-skip-link{display:none}}.c-skip-link:link{color:#fff}.c-status-message{align-items:center;box-sizing:border-box;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;position:relative;width:100%}.c-card__summary>p:last-child,.c-status-message :last-child{margin-bottom:0}.c-status-message--boxed{background-color:#fff;border:1px solid #eee;border-radius:2px;line-height:1.4;padding:16px}.c-status-message__heading{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;font-weight:700}.c-status-message__icon{display:inline-block;fill:currentcolor;flex:0 0 auto;height:1.5em;margin-right:8px;transform:translate(0);vertical-align:text-top;width:1.5em}.c-status-message__icon--top{align-self:flex-start}.c-status-message--info .c-status-message__icon{color:#003f8d}.c-status-message--boxed.c-status-message--info{border-bottom:4px solid #003f8d}.c-status-message--error .c-status-message__icon{color:#c40606}.c-status-message--boxed.c-status-message--error{border-bottom:4px solid #c40606}.c-status-message--success .c-status-message__icon{color:#00b8b0}.c-status-message--boxed.c-status-message--success{border-bottom:4px solid #00b8b0}.c-status-message--warning .c-status-message__icon{color:#edbc53}.c-status-message--boxed.c-status-message--warning{border-bottom:4px solid #edbc53}.c-breadcrumbs{color:#000;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs__link{color:#666}svg.c-breadcrumbs__chevron{fill:#888;height:10px;margin:4px 4px 0;width:10px}@media only screen and (max-width:539px){.c-breadcrumbs .c-breadcrumbs__item{display:none}.c-breadcrumbs .c-breadcrumbs__item:last-child,.c-breadcrumbs .c-breadcrumbs__item:nth-last-child(2){display:inline}}.c-card{background-color:transparent;border:0;box-shadow:none;display:flex;flex-direction:column;font-size:14px;min-width:0;overflow:hidden;padding:0;position:relative}.c-card--no-shape{background:0 0;border:0;box-shadow:none}.c-card__image{display:flex;justify-content:center;overflow:hidden;padding-bottom:56.25%;position:relative}@supports (aspect-ratio:1/1){.c-card__image{aspect-ratio:var(--card--image-aspect-ratio,16/9);padding-bottom:0}}.c-card__image img{left:0;min-height:100%;min-width:100%;position:absolute}@supports ((-o-object-fit:cover) or (object-fit:cover)){.c-card__image img{height:100%;object-fit:cover;width:100%}}.c-card__body{flex:1 1 auto;padding:16px}.c-card--no-shape .c-card__body{padding:0}.c-card--no-shape .c-card__body:not(:first-child){padding-top:16px}.c-card__title{letter-spacing:-.01875rem;margin-bottom:8px;margin-top:0}[lang=de] .c-card__title{hyphens:auto}.c-card__summary{line-height:1.4}.c-card__summary>p{margin-bottom:5px}.c-card__summary a{text-decoration:underline}.c-card__link:not(.c-card__link--no-block-link):before{content:"";inset:0;position:absolute}.c-card--flush .c-card__body{padding:0}.c-card--major{font-size:1rem}.c-card--dark{background-color:#29303c;border-width:0;color:#e3e4e5}.c-card--dark .c-card__title{color:#fff}.c-card--dark .c-card__link,.c-card--dark .c-card__summary a{color:inherit}.c-header{background-color:#fff;border-bottom:5px solid #000;font-size:1rem;line-height:1.4;margin-bottom:16px}.c-header__row{padding:0;position:relative}.c-header__row:not(:last-child){border-bottom:1px solid #eee}.c-header__split{align-items:center;display:flex;justify-content:space-between}.c-header__logo-container{flex:1 1 0px;line-height:0;margin:8px 24px 8px 0}.c-header__logo{transform:translateZ(0)}.c-header__logo img{max-height:32px}.c-header__container{margin:0 auto;max-width:1280px}@media print{.c-header__menu{display:none}}.c-header__menu{align-items:center;display:flex;flex:0 1 auto;flex-wrap:wrap;font-weight:700;gap:8px;line-height:1.4;list-style:none;margin:0 -4px;padding:0}@media only screen and (max-width:1023px){.c-header__menu--hide-lg-max{display:none;visibility:hidden}}.c-header__menu--global{font-weight:400;justify-content:flex-end}.c-header__menu--global svg{display:none;visibility:hidden}.c-header__menu--global svg:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.c-header__menu--global svg{display:block;visibility:visible}}.c-header__menu--journal{font-size:.875rem;margin:8px 0 8px -8px}@media only screen and (min-width:540px){.c-header__menu--journal{flex-wrap:nowrap;font-size:1rem}}.c-header__item{padding-bottom:0;padding-top:0;position:static}.c-header__item--pipe{border-left:2px solid #eee;padding-left:8px}.c-header__item--padding{padding-bottom:8px;padding-top:8px}@media only screen and (min-width:540px){.c-header__item--dropdown-menu{position:relative}}@media only screen and (min-width:1024px){.c-header__item--hide-lg{display:none;visibility:hidden}}@media only screen and (max-width:767px){.c-header__item--hide-md-max{display:none;visibility:hidden}.c-header__item--hide-md-max:first-child+*{margin-block-start:0}}.c-header__link{align-items:center;color:inherit;display:inline-flex;gap:4px;padding:8px;white-space:nowrap}.c-header__link svg{transition-duration:.2s}.c-header__show-text{display:none;visibility:hidden}.has-tethered .c-header__heading--js-hide:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.c-header__show-text{display:inline;visibility:visible}}.c-header__show-text-sm{display:inline;visibility:visible}@media only screen and (min-width:540px){.c-header__show-text-sm{display:none;visibility:hidden}.c-header__show-text-sm:first-child+*{margin-block-start:0}}@media print{.c-header__dropdown{display:none}}.c-header__dropdown{background-color:#000;border-bottom:1px solid #2f2f2f;color:#eee;font-size:.875rem;line-height:1.2;padding:16px 0}.c-header__heading{display:inline-block;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.25rem;font-weight:400;line-height:1.4;margin-bottom:8px}.c-header__heading--keyline{border-top:1px solid;border-color:#2f2f2f;margin-top:16px;padding-top:16px;width:100%}.c-header__list{display:flex;flex-wrap:wrap;gap:0 16px;list-style:none;margin:0 -8px}.c-header__flush{margin:0 -8px}.c-header__visually-hidden{border:0;clip:rect(0,0,0,0);height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.c-header__search-form{margin-bottom:8px}.c-header__search-layout{display:flex;flex-wrap:wrap;gap:16px}.c-header__search-layout>:first-child{flex:999 1 auto}.c-header__search-layout>*{flex:1 1 auto}.c-header__search-layout--max-width{max-width:720px}.c-header__search-button{align-items:center;background-color:transparent;background-image:none;border:1px solid #fff;border-radius:2px;color:#fff;cursor:pointer;display:flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.15;margin:0;padding:8px 16px;position:relative;text-decoration:none;transition:.25s,color .25s,border-color .25s;width:100%}.c-header__input,.c-header__select{border:1px solid;border-radius:3px;box-sizing:border-box;font-size:1rem;padding:8px 16px;width:100%}.c-header__select{appearance:none;background-image:url("data:image/svg+xml,%3Csvg height='16' viewBox='0 0 16 16' width='16' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z' fill='%23333' fill-rule='evenodd' transform='matrix(0 1 -1 0 11 3)'/%3E%3C/svg%3E");background-position:right .7em top 50%;background-repeat:no-repeat;background-size:1em;box-shadow:0 1px 0 1px rgba(0,0,0,.04);display:block;margin:0;max-width:100%;min-width:150px}@media only screen and (min-width:540px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:auto;right:0}}@media only screen and (min-width:768px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:0;right:auto}}.c-header__dropdown.has-tethered{border-bottom:0;border-radius:0 0 2px 2px;left:0;position:absolute;top:100%;transform:translateY(5px);width:100%;z-index:1}@media only screen and (min-width:540px){.c-header__dropdown.has-tethered{transform:translateY(8px);width:auto}}@media only screen and (min-width:768px){.c-header__dropdown.has-tethered{min-width:225px}}.c-header__dropdown--full-width.has-tethered{padding:32px 0 24px;transform:none;width:100%}.has-tethered .c-header__heading--js-hide{display:none;visibility:hidden}.has-tethered .c-header__list--js-stack{flex-direction:column}.has-tethered .c-header__item--keyline,.has-tethered .c-header__list~.c-header__list .c-header__item:first-child{border-top:1px solid #d5d5d5;margin-top:8px;padding-top:8px}.c-header__item--snid-account-widget{display:flex}.c-header__container{padding:0 4px}.c-header__list{padding:0 12px}.c-header__menu .c-header__link{font-size:14px}.c-header__item--snid-account-widget .c-header__link{padding:8px}.c-header__menu--journal{margin-left:0}@media only screen and (min-width:540px){.c-header__container{padding:0 16px}.c-header__menu--journal{margin-left:-8px}.c-header__menu .c-header__link{font-size:16px}.c-header__link--search{gap:13px}}.u-button{align-items:center;background-color:transparent;background-image:none;border-radius:2px;cursor:pointer;display:inline-flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-decoration:none;transition:.25s,color .25s,border-color .25s;width:auto}.u-button svg,.u-button--primary svg{fill:currentcolor}.u-button{border:1px solid #069;color:#069}.u-button--primary{background-color:#069;background-image:none;border:1px solid #069;color:#fff}.u-button--full-width{display:flex;width:100%}.u-display-none{display:none}.js .u-js-hide,.u-hide{display:none;visibility:hidden}.u-hide:first-child+*{margin-block-start:0}.u-visually-hidden{border:0;clip:rect(0,0,0,0);height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}@media print{.u-hide-print{display:none}}@media only screen and (min-width:1024px){.u-hide-at-lg{display:none;visibility:hidden}.u-hide-at-lg:first-child+*{margin-block-start:0}}.u-clearfix:after,.u-clearfix:before{content:"";display:table}.u-clearfix:after{clear:both}.u-color-open-access{color:#b74616}.u-float-left{float:left}.u-icon{display:inline-block;fill:currentcolor;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-full-height{height:100%}.u-link-inherit{color:inherit}.u-list-reset{list-style:none;margin:0;padding:0}.u-text-bold{font-weight:700}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-justify-content-space-between{justify-content:space-between}.u-mt-32{margin-top:32px}.u-mb-8{margin-bottom:8px}.u-mb-16{margin-bottom:16px}.u-mb-24{margin-bottom:24px}.u-mb-32{margin-bottom:32px}.u-mb-48{margin-bottom:48px}.u-pa-16{padding:16px}html *,html :after,html :before{box-sizing:inherit}.c-article-section__title,.c-article-title{font-weight:700}.c-card__title{line-height:1.4em}.c-article__button{background-color:#069;border:1px solid #069;border-radius:2px;color:#fff;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.4;margin-bottom:16px;padding:13px;transition:background-color .2s ease-out,color .2s ease-out}.c-article__button,.c-article__button:hover{text-decoration:none}.c-article__button--inverted,.c-article__button:hover{background-color:#fff;color:#069}.c-article__button--inverted:hover{background-color:#069;color:#fff}.c-header__link{text-decoration:inherit}.grade-c-hide{display:block}.c-pdf-download__link{padding:13px 24px} } </style>




    
        <link data-test="critical-css-handler" data-inline-css-source="critical-css" rel="stylesheet" href="/static/css/enhanced-article-26607d48a5.css" media="print" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null">
    
    <noscript>
        <link rel="stylesheet" type="text/css" href="/static/css/enhanced-article-26607d48a5.css" media="only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)">
    </noscript>

<link rel="stylesheet" type="text/css" href="/static/css/article-print-58da41a5fd.css" media="print">
    



<link rel="apple-touch-icon" sizes="180x180" href=/static/images/favicons/nature/apple-touch-icon-f39cb19454.png>
<link rel="icon" type="image/png" sizes="48x48" href=/static/images/favicons/nature/favicon-48x48-b52890008c.png>
<link rel="icon" type="image/png" sizes="32x32" href=/static/images/favicons/nature/favicon-32x32-3fe59ece92.png>
<link rel="icon" type="image/png" sizes="16x16" href=/static/images/favicons/nature/favicon-16x16-951651ab72.png>
<link rel="manifest" href=/static/manifest.json crossorigin="use-credentials">
<link rel="mask-icon" href=/static/images/favicons/nature/safari-pinned-tab-619cea1bcb.svg color="#000000">
<link rel="shortcut icon" href=/static/images/favicons/nature/favicon.ico>
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-config" content=/static/browserconfig.xml>
<meta name="theme-color" content="#000000">
<meta name="application-name" content="Nature">



<script>
    (function () {
        if ( typeof window.CustomEvent === "function" ) return false;
        function CustomEvent ( event, params ) {
            params = params || { bubbles: false, cancelable: false, detail: null };
            var evt = document.createEvent( 'CustomEvent' );
            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );
            return evt;
        }

        CustomEvent.prototype = window.Event.prototype;

        window.CustomEvent = CustomEvent;
    })();
</script>


<script>
    (function (w, d, s) {
        var urlParams = new URLSearchParams(w.location.search);
        if (urlParams.get('gptAdsTest') !== null) {
            d.addEventListener('sncc:initialise', function (e) {
                var t = d.createElement(s);
                var h = d.getElementsByTagName(s)[0];
                t.src = 'https://' + (e.detail.C03 ? 'securepubads.g.doubleclick' : 'pagead2.googlesyndication') + '.net/tag/js/gpt.js';
                t.async = false;
                t.onload = function () {
                    var n = d.createElement(s);
                    n.src = 'https://fed-libs.nature.com/production/gpt-ads-gtm.min.js';
                    n.async = false;
                    h.insertAdjacentElement('afterend', n);
                };
                h.insertAdjacentElement('afterend', t);
            })
        }
    })(window, document, 'script');
</script>
    
<!-- Google Tag Manager -->
<script data-test="gtm-head">
    window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://sgtm.nature.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
</script>
<!-- End Google Tag Manager -->

    <script>
    (function(w,d,t) {
        function cc() {
            var h = w.location.hostname;
            if (h === 'preview-www.nature.com') return;
            var e = d.createElement(t),
                s = d.getElementsByTagName(t)[0];
            e.setAttribute('crossorigin', 'anonymous');
            if (h === 'nature.com' || h.endsWith('.nature.com')) {
                e.src = 'https://cmp.nature.com/production_live/en/consent-bundle-8-102.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            } else {
                e.src = '/static/js/cookie-consent-es5-bundle-0b812e7bd9.js';
                e.setAttribute('data-consent', h);
            }
            s.insertAdjacentElement('afterend', e);
        }
        cc();
    })(window,document,'script');
</script>


<script id="js-position0">
    (function(w, d) {
        w.idpVerifyPrefix = 'https://verify.nature.com';
        w.ra21Host = 'https://wayf.springernature.com';
        var moduleSupport = (function() {
            return 'noModule' in d.createElement('script');
        })();

        if (w.config.mustardcut === true) {
            w.loader = {
                index: 0,
                registered: [],
                scripts: [
                    
                        {src: '/static/js/global-article-es6-bundle-3c95024fee.js', test: 'global-article-js', module: true},
                        {src: '/static/js/global-article-es5-bundle-2038893a49.js', test: 'global-article-js', nomodule: true},
                        {src: '/static/js/shared-es6-bundle-57e9b2d0a4.js', test: 'shared-js', module: true},
                        {src: '/static/js/shared-es5-bundle-28d92cf6c5.js', test: 'shared-js', nomodule: true},
                        {src: '/static/js/header-150-es6-bundle-a3d441cf1b.js', test: 'header-150-js', module: true},
                        {src: '/static/js/header-150-es5-bundle-5080ac4398.js', test: 'header-150-js', nomodule: true}
                    
                ].filter(function (s) {
                    if (s.src === null) return false;
                    if (moduleSupport && s.nomodule) return false;
                    return !(!moduleSupport && s.module);
                }),

                register: function (value) {
                    this.registered.push(value);
                },

                ready: function () {
                    if (this.registered.length === this.scripts.length) {
                        this.registered.forEach(function (fn) {
                            if (typeof fn === 'function') {
                                setTimeout(fn, 0); 
                            }
                        });
                        this.ready = function () {};
                    }
                },

                insert: function (s) {
                    var t = d.getElementById('js-position' + this.index);
                    if (t && t.insertAdjacentElement) {
                        t.insertAdjacentElement('afterend', s);
                    } else {
                        d.head.appendChild(s);
                    }
                    ++this.index;
                },

                createScript: function (script, beforeLoad) {
                    var s = d.createElement('script');
                    s.id = 'js-position' + (this.index + 1);
                    s.setAttribute('data-test', script.test);
                    if (beforeLoad) {
                        s.defer = 'defer';
                        s.onload = function () {
                            if (script.noinit) {
                                loader.register(true);
                            }
                            if (d.readyState === 'interactive' || d.readyState === 'complete') {
                                loader.ready();
                            }
                        };
                    } else {
                        s.async = 'async';
                    }
                    s.src = script.src;
                    return s;
                },

                init: function () {
                    this.scripts.forEach(function (s) {
                        loader.insert(loader.createScript(s, true));
                    });

                    d.addEventListener('DOMContentLoaded', function () {
                        loader.ready();
                        var conditionalScripts;
                        
                            conditionalScripts = [
                                {match: 'div[data-pan-container]', src: '/static/js/pan-zoom-es6-bundle-9055a10868.js', test: 'pan-zoom-js',  module: true },
                                {match: 'div[data-pan-container]', src: '/static/js/pan-zoom-es5-bundle-9055a10868.js', test: 'pan-zoom-js',  nomodule: true },
                                {match: 'math,span.mathjax-tex', src: '/static/js/math-es6-bundle-cccbca52af.js', test: 'math-js', module: true},
                                {match: 'math,span.mathjax-tex', src: '/static/js/math-es5-bundle-cccbca52af.js', test: 'math-js', nomodule: true}
                            ];
                        

                        if (conditionalScripts) {
                            conditionalScripts.filter(function (script) {
                                return !!document.querySelector(script.match) && !((moduleSupport && script.nomodule) || (!moduleSupport && script.module));
                            }).forEach(function (script) {
                                loader.insert(loader.createScript(script));
                            });
                        }
                    }, false);
                }
            };
            loader.init();
        }
    })(window, document);
</script>










<meta name="robots" content="noarchive">
<meta name="access" content="Yes">


<link rel="search" href="https://www.nature.com/search">
<link rel="search" href="https://www.nature.com/opensearch/opensearch.xml" type="application/opensearchdescription+xml" title="nature.com">
<link rel="search" href="https://www.nature.com/opensearch/request" type="application/sru+xml" title="nature.com">





    
    <script type="application/ld+json">{"mainEntity":{"headline":"Chunking as a rational solution to the speed–accuracy trade-off in a serial reaction time task","description":"When exposed to perceptual and motor sequences, people are able to gradually identify patterns within and form a compact internal description of the sequence. One proposal of how sequences can be compressed is people’s ability to form chunks. We study people’s chunking behavior in a serial reaction time task. We relate chunk representation with sequence statistics and task demands, and propose a rational model of chunking that rearranges and concatenates its representation to jointly optimize for accuracy and speed. Our model predicts that participants should chunk more if chunks are indeed part of the generative model underlying a task and should, on average, learn longer chunks when optimizing for speed than optimizing for accuracy. We test these predictions in two experiments. In the first experiment, participants learn sequences with underlying chunks. In the second experiment, participants were instructed to act either as fast or as accurately as possible. The results of both experiments confirmed our model’s predictions. Taken together, these results shed new light on the benefits of chunking and pave the way for future studies on step-wise representation learning in structured domains.","datePublished":"2023-05-11T00:00:00Z","dateModified":"2023-05-11T00:00:00Z","pageStart":"1","pageEnd":"17","license":"http://creativecommons.org/licenses/by/4.0/","sameAs":"https://doi.org/10.1038/s41598-023-31500-3","keywords":["Cognitive neuroscience","Computational neuroscience","Human behaviour","Learning and memory","Psychology","Science","Humanities and Social Sciences","multidisciplinary"],"image":["https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41598-023-31500-3/MediaObjects/41598_2023_31500_Fig1_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41598-023-31500-3/MediaObjects/41598_2023_31500_Fig2_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41598-023-31500-3/MediaObjects/41598_2023_31500_Fig3_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41598-023-31500-3/MediaObjects/41598_2023_31500_Fig4_HTML.png"],"isPartOf":{"name":"Scientific Reports","issn":["2045-2322"],"volumeNumber":"13","@type":["Periodical","PublicationVolume"]},"publisher":{"name":"Nature Publishing Group UK","logo":{"url":"https://www.springernature.com/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Shuchen Wu","affiliation":[{"name":"Max Planck Institute for Biological Cybernetics","address":{"name":"MPRG Computational Principles of Intelligence, Max Planck Institute for Biological Cybernetics, Tübingen, Germany","@type":"PostalAddress"},"@type":"Organization"}],"email":"shuchen.wu@tue.mpg.de","@type":"Person"},{"name":"Noémi Éltető","affiliation":[{"name":"Max Planck Institute for Biological Cybernetics","address":{"name":"Department of Computational Neuroscience, Max Planck Institute for Biological Cybernetics, Tübingen, Germany","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Ishita Dasgupta","affiliation":[{"name":"Google DeepMind","address":{"name":"Google DeepMind, New York City, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Eric Schulz","affiliation":[{"name":"Max Planck Institute for Biological Cybernetics","address":{"name":"MPRG Computational Principles of Intelligence, Max Planck Institute for Biological Cybernetics, Tübingen, Germany","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"}],"isAccessibleForFree":true,"@type":"ScholarlyArticle"},"@context":"https://schema.org","@type":"WebPage"}</script>




    
    
    


    
    <link rel="canonical" href="https://www.nature.com/articles/s41598-023-31500-3">
    
    
    <meta name="journal_id" content="41598"/>
    <meta name="dc.title" content="Chunking as a rational solution to the speed–accuracy trade-off in a serial reaction time task"/>
    <meta name="dc.source" content="Scientific Reports 2023 13:1"/>
    <meta name="dc.format" content="text/html"/>
    <meta name="dc.publisher" content="Nature Publishing Group"/>
    <meta name="dc.date" content="2023-05-11"/>
    <meta name="dc.type" content="OriginalPaper"/>
    <meta name="dc.language" content="En"/>
    <meta name="dc.copyright" content="2023 The Author(s)"/>
    <meta name="dc.rights" content="2023 The Author(s)"/>
    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="dc.description" content="When exposed to perceptual and motor sequences, people are able to gradually identify patterns within and form a compact internal description of the sequence. One proposal of how sequences can be compressed is people&#8217;s ability to form chunks. We study people&#8217;s chunking behavior in a serial reaction time task. We relate chunk representation with sequence statistics and task demands, and propose a rational model of chunking that rearranges and concatenates its representation to jointly optimize for accuracy and speed. Our model predicts that participants should chunk more if chunks are indeed part of the generative model underlying a task and should, on average, learn longer chunks when optimizing for speed than optimizing for accuracy. We test these predictions in two experiments. In the first experiment, participants learn sequences with underlying chunks. In the second experiment, participants were instructed to act either as fast or as accurately as possible. The results of both experiments confirmed our model&#8217;s predictions. Taken together, these results shed new light on the benefits of chunking and pave the way for future studies on step-wise representation learning in structured domains."/>
    <meta name="prism.issn" content="2045-2322"/>
    <meta name="prism.publicationName" content="Scientific Reports"/>
    <meta name="prism.publicationDate" content="2023-05-11"/>
    <meta name="prism.volume" content="13"/>
    <meta name="prism.number" content="1"/>
    <meta name="prism.section" content="OriginalPaper"/>
    <meta name="prism.startingPage" content="7680"/>
    <meta name="prism.endingPage" content=""/>
    <meta name="prism.copyright" content="2023 The Author(s)"/>
    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="prism.url" content="https://www.nature.com/articles/s41598-023-31500-3"/>
    <meta name="prism.doi" content="doi:10.1038/s41598-023-31500-3"/>
    <meta name="citation_pdf_url" content="https://www.nature.com/articles/s41598-023-31500-3.pdf"/>
    <meta name="citation_fulltext_html_url" content="https://www.nature.com/articles/s41598-023-31500-3"/>
    <meta name="citation_journal_title" content="Scientific Reports"/>
    <meta name="citation_journal_abbrev" content="Sci Rep"/>
    <meta name="citation_publisher" content="Nature Publishing Group"/>
    <meta name="citation_issn" content="2045-2322"/>
    <meta name="citation_title" content="Chunking as a rational solution to the speed–accuracy trade-off in a serial reaction time task"/>
    <meta name="citation_volume" content="13"/>
    <meta name="citation_issue" content="1"/>
    <meta name="citation_online_date" content="2023/05/11"/>
    <meta name="citation_firstpage" content="7680"/>
    <meta name="citation_lastpage" content=""/>
    <meta name="citation_article_type" content="Article"/>
    <meta name="citation_fulltext_world_readable" content=""/>
    <meta name="citation_language" content="en"/>
    <meta name="dc.identifier" content="doi:10.1038/s41598-023-31500-3"/>
    <meta name="DOI" content="10.1038/s41598-023-31500-3"/>
    <meta name="size" content="441244"/>
    <meta name="citation_doi" content="10.1038/s41598-023-31500-3"/>
    <meta name="citation_springer_api_url" content="http://api.springer.com/xmldata/jats?q=doi:10.1038/s41598-023-31500-3&amp;api_key="/>
    <meta name="description" content="When exposed to perceptual and motor sequences, people are able to gradually identify patterns within and form a compact internal description of the sequence. One proposal of how sequences can be compressed is people&#8217;s ability to form chunks. We study people&#8217;s chunking behavior in a serial reaction time task. We relate chunk representation with sequence statistics and task demands, and propose a rational model of chunking that rearranges and concatenates its representation to jointly optimize for accuracy and speed. Our model predicts that participants should chunk more if chunks are indeed part of the generative model underlying a task and should, on average, learn longer chunks when optimizing for speed than optimizing for accuracy. We test these predictions in two experiments. In the first experiment, participants learn sequences with underlying chunks. In the second experiment, participants were instructed to act either as fast or as accurately as possible. The results of both experiments confirmed our model&#8217;s predictions. Taken together, these results shed new light on the benefits of chunking and pave the way for future studies on step-wise representation learning in structured domains."/>
    <meta name="dc.creator" content="Wu, Shuchen"/>
    <meta name="dc.creator" content="&#201;ltet&#337;, No&#233;mi"/>
    <meta name="dc.creator" content="Dasgupta, Ishita"/>
    <meta name="dc.creator" content="Schulz, Eric"/>
    <meta name="dc.subject" content="Cognitive neuroscience"/>
    <meta name="dc.subject" content="Computational neuroscience"/>
    <meta name="dc.subject" content="Human behaviour"/>
    <meta name="dc.subject" content="Learning and memory"/>
    <meta name="dc.subject" content="Psychology"/>
    <meta name="citation_reference" content="citation_journal_title=Psychol. Rev.; citation_title=The magical number seven, plus or minus two: Some limits on our capacity for processing information; citation_author=GA Miller; citation_publication_date=1956; citation_doi=10.1037/h0043158; citation_id=CR1"/>
    <meta name="citation_reference" content="Laird, J.&#160;E., Rosenbloom, P.&#160;S. &amp; Newell, A. Towards chunking as a general learning mechanism. In AAAI, 188&#8211;192 (1984)."/>
    <meta name="citation_reference" content="citation_journal_title=Neurobiol. Learn. Mem.; citation_title=The basal ganglia and chunking of action repertoires; citation_author=AM Graybiel; citation_volume=70; citation_publication_date=1998; citation_pages=119-136; citation_doi=10.1006/nlme.1998.3843; citation_id=CR3"/>
    <meta name="citation_reference" content="citation_journal_title=J. Exp. Psychol. Learn. Mem. Cogn.; citation_title=Learning artificial grammars with competitive chunking; citation_author=E Servan-Schreiber, JR Anderson; citation_volume=16; citation_publication_date=1990; citation_pages=592; citation_doi=10.1037/0278-7393.16.4.592; citation_id=CR4"/>
    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Chunking by a pigeon in a serial learning task; citation_author=HS Terrace; citation_publication_date=1987; citation_doi=10.1038/325149a0; citation_id=CR5"/>
    <meta name="citation_reference" content="citation_journal_title=Cognition; citation_title=What&#8217;s magic about magic numbers? Chunking and data compression in short-term memory; citation_author=F Mathy, J Feldman; citation_publication_date=2012; citation_doi=10.1016/j.cognition.2011.11.003; citation_id=CR6"/>
    <meta name="citation_reference" content="citation_title=The Problem of Serial Order in Behavior; citation_publication_date=1951; citation_id=CR7; citation_author=KS Lashley; citation_publisher=Bobbs-Merrill Oxford"/>
    <meta name="citation_reference" content="citation_journal_title=Trends Cog. Sci.; citation_title=Chunking mechanisms in human learning; citation_author=F Gobet; citation_publication_date=2001; citation_doi=10.1016/S1364-6613(00)01662-4; citation_id=CR8"/>
    <meta name="citation_reference" content="citation_journal_title=Neurobiol. Learn. Memory; citation_title=The basal ganglia and chunking of action repertoires; citation_author=AM Graybiel; citation_volume=70; citation_publication_date=1998; citation_pages=1-2; citation_doi=10.1006/nlme.1998.3843; citation_id=CR9"/>
    <meta name="citation_reference" content="citation_journal_title=Memory Cogn.; citation_title=Chunking in recall of symbolic drawings; citation_author=DE Egan, BJ Schwartz; citation_publication_date=1979; citation_doi=10.3758/BF03197595; citation_id=CR10"/>
    <meta name="citation_reference" content="citation_journal_title=Stud. Second Lang. Acquis.; citation_title=Sequencing in SLA: Phonological memory, chunking, and points of order; citation_author=NC Ellis; citation_publication_date=1996; citation_doi=10.1017/S0272263100014698; citation_id=CR11"/>
    <meta name="citation_reference" content="citation_journal_title=Psychol. Res.; citation_title=Patterns, chunks, and hierarchies in serial reaction-time tasks; citation_author=I Koch, J Hoffmann; citation_publication_date=2000; citation_doi=10.1007/PL00008165; citation_id=CR12"/>
    <meta name="citation_reference" content="citation_journal_title=J. Exp. Psychol.: General; citation_title=Compression in visual working memory: Using statistical regularities to form more efficient memory representations; citation_author=TF Brady, T Konkle, GA Alvarez; citation_publication_date=2009; citation_doi=10.1037/a0016797; citation_id=CR13"/>
    <meta name="citation_reference" content="citation_journal_title=Front. Human Neurosci.; citation_title=Transfer in motor sequence learning: Effects of practice schedule and sequence context; citation_author=DM M&#252;ssgens, F Ull&#233;n; citation_publication_date=2015; citation_doi=10.3389/fnhum.2015.00642; citation_id=CR14"/>
    <meta name="citation_reference" content="citation_journal_title=Cogn. Psychol.; citation_title=Perception in chess; citation_author=WG Chase, HA Simon; citation_publication_date=1973; citation_doi=10.1016/0010-0285(73)90004-2; citation_id=CR15"/>
    <meta name="citation_reference" content="citation_journal_title=Memory; citation_title=Expert chess memory: Revisiting the chunking hypothesis; citation_author=F Gobet, HA Simon; citation_publication_date=1998; citation_doi=10.1080/741942359; citation_id=CR16"/>
    <meta name="citation_reference" content="citation_journal_title=Cogn. Psychol.; citation_title=Compositional inductive biases in function learning; citation_author=E Schulz, JB Tenenbaum, D Duvenaud, M Speekenbrink, SJ Gershman; citation_publication_date=2017; citation_doi=10.1016/j.cogpsych.2017.11.002; citation_id=CR17"/>
    <meta name="citation_reference" content="citation_journal_title=Open. Mind; citation_title=Communicating compositional patterns; citation_author=E Schulz, F Quiroga, SJ Gershman; citation_volume=4; citation_publication_date=2020; citation_pages=25-39; citation_doi=10.1162/opmi_a_00032; citation_id=CR18"/>
    <meta name="citation_reference" content="citation_journal_title=PLoS Comput. Biol.; citation_title=Discovery of hierarchical representations for efficient planning; citation_author=MS Tomov, S Yagati, A Kumar, W Yang, SJ Gershman; citation_publication_date=2020; citation_doi=10.1371/journal.pcbi.1007594; citation_id=CR19"/>
    <meta name="citation_reference" content="citation_journal_title=Acta Physiol. (Oxf); citation_title=Speed-accuracy tradeoff and information processing dynamics; citation_author=WA Wickelgren; citation_volume=41; citation_publication_date=1977; citation_pages=67-85; citation_doi=10.1016/0001-6918(77)90012-9; citation_id=CR20"/>
    <meta name="citation_reference" content="citation_journal_title=Q. J. Exp. Psychol.; citation_title=Do humans produce the speed-accuracy trade-off that maximizes reward rate?; citation_author=R Bogacz, PT Hu, PJ Holmes, JD Cohen; citation_volume=63; citation_publication_date=2010; citation_pages=863-891; citation_doi=10.1080/17470210903091643; citation_id=CR21"/>
    <meta name="citation_reference" content="citation_journal_title=Psychol. Rev.; citation_title=The problems of flexibility, fluency, and speed-accuracy trade-off in skilled behavior; citation_author=DG MacKay; citation_volume=89; citation_publication_date=1982; citation_pages=483-506; citation_doi=10.1037/0033-295X.89.5.483; citation_id=CR22"/>
    <meta name="citation_reference" content="citation_journal_title=J. Exp. Psychol.; citation_title=Cognitive aspects of information processing: III. Set for speed versus accuracy; citation_author=PM Fitts; citation_volume=71; citation_publication_date=1966; citation_pages=849-857; citation_doi=10.1037/h0023232; citation_id=CR23"/>
    <meta name="citation_reference" content="citation_journal_title=Cogn. Psychol.; citation_title=Attentional requirements of learning: Evidence from performance measures; citation_author=MJ Nissen, P Bullemer; citation_volume=19; citation_publication_date=1987; citation_pages=1-32; citation_doi=10.1016/0010-0285(87)90002-8; citation_id=CR24"/>
    <meta name="citation_reference" content="citation_journal_title=J. Exp. Psychol. Learn. Mem. Cogn.; citation_title=On the development of procedural knowledge; citation_author=DB Willingham, MJ Nissen, P Bullemer; citation_volume=15; citation_publication_date=1989; citation_pages=1047; citation_doi=10.1037/0278-7393.15.6.1047; citation_id=CR25"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=The serial reaction time task: Implicit motor skill learning?; citation_author=EM Robertson; citation_volume=27; citation_publication_date=2007; citation_pages=10073-10075; citation_doi=10.1523/JNEUROSCI.2747-07.2007; citation_id=CR26"/>
    <meta name="citation_reference" content="citation_journal_title=Psychol. Rev.; citation_title=The cognitive and neural architecture of sequence representation; citation_author=SW Keele, R Ivry, U Mayr, E Hazeltine, H Heuer; citation_volume=110; citation_publication_date=2003; citation_pages=316-339; citation_doi=10.1037/0033-295X.110.2.316; citation_id=CR27"/>
    <meta name="citation_reference" content="citation_journal_title=J. Neurophysiol.; citation_title=Direct comparison of neural systems mediating conscious and unconscious skill learning; citation_author=DB Willingham, J Salidis, JD Gabrieli; citation_volume=88; citation_publication_date=2002; citation_pages=1451-1460; citation_doi=10.1152/jn.2002.88.3.1451; citation_id=CR28"/>
    <meta name="citation_reference" content="citation_journal_title=Psychol. Aging; citation_title=Age differences in implicit learning of higher order dependencies in serial patterns; citation_author=JH Howard, DV Howard; citation_volume=12; citation_publication_date=1997; citation_pages=634-656; citation_doi=10.1037//0882-7974.12.4.634; citation_id=CR29"/>
    <meta name="citation_reference" content="citation_journal_title=Memory; citation_title=One-year retention of general and sequence-specific skills in a probabilistic, serial reaction time task; citation_author=JC Romano, JH Howard, DV Howard; citation_volume=18; citation_publication_date=2010; citation_pages=427-441; citation_doi=10.1080/09658211003742680; citation_id=CR30"/>
    <meta name="citation_reference" content="citation_journal_title=PLoS Comput. Biol.; citation_title=Cortical and hippocampal correlates of deliberation during model-based decisions for rewards in humans; citation_author=A Bornstein, N Daw; citation_volume=9; citation_publication_date=2013; citation_doi=10.1371/journal.pcbi.1003387; citation_id=CR31"/>
    <meta name="citation_reference" content="citation_journal_title=Psychol. Res.; citation_title=Attention and probabilistic sequence learning; citation_author=RW Schvaneveldt, RL Gomez; citation_volume=61; citation_publication_date=1998; citation_pages=175-190; citation_doi=10.1007/s004260050023; citation_id=CR32"/>
    <meta name="citation_reference" content="citation_journal_title=J. Exp. Psychol. Gen.; citation_title=Learning the structure of event sequences; citation_author=A Cleeremans, JL McClelland; citation_volume=120; citation_publication_date=1991; citation_pages=235-253; citation_doi=10.1037/0096-3445.120.3.235; citation_id=CR33"/>
    <meta name="citation_reference" content="citation_journal_title=Psychol.-Diss.; citation_title=Associative processes in statistical learning: Paradoxical predictions of the past; citation_author=JP Provyn; citation_volume=179; citation_publication_date=2013; citation_pages=78; citation_id=CR34"/>
    <meta name="citation_reference" content="citation_journal_title=J. Mem. Lang.; citation_title=Parser: A model for word segmentation; citation_author=P Perruchet, A Vinter; citation_volume=39; citation_publication_date=1998; citation_pages=246-263; citation_doi=10.1006/jmla.1998.2576; citation_id=CR35"/>
    <meta name="citation_reference" content="citation_journal_title=J. Exp. Psychol. Learn. Mem. Cogn.; citation_title=Learning artificial grammars with competitive chunking; citation_author=E Servan-Schreiber, J Anderson; citation_volume=16; citation_publication_date=1990; citation_pages=592-608; citation_doi=10.1037/0278-7393.16.4.592; citation_id=CR36"/>
    <meta name="citation_reference" content="citation_journal_title=Psychol. Rev.; citation_title=TRACX: A recognition-based connectionist framework for sequence segmentation and chunk extraction; citation_author=RM French, C Addyman, D Mareschal; citation_volume=118; citation_publication_date=2011; citation_pages=614-636; citation_doi=10.1037/a0025255; citation_id=CR37"/>
    <meta name="citation_reference" content="citation_journal_title=Neural Comput.; citation_title=Finite state automata and simple recurrent networks; citation_author=A Cleeremans, D Servan-Schreiber, JL McClelland; citation_volume=1; citation_publication_date=1989; citation_pages=372-381; citation_doi=10.1162/neco.1989.1.3.372; citation_id=CR38"/>
    <meta name="citation_reference" content="citation_journal_title=PLoS Comput. Biol.; citation_title=A model of human motor sequence learning explains facilitation and interference effects based on spike-timing dependent plasticity; citation_author=Q Wang, CA Rothkopf, J Triesch; citation_publication_date=2017; citation_doi=10.1371/journal.pcbi.1005632; citation_id=CR39"/>
    <meta name="citation_reference" content="citation_journal_title=Cognition; citation_title=A bayesian framework for word segmentation: Exploring the effects of context; citation_author=S Goldwater, T Griffiths, M Johnson; citation_volume=112; citation_publication_date=2009; citation_pages=21-54; citation_doi=10.1016/j.cognition.2009.03.008; citation_id=CR40"/>
    <meta name="citation_reference" content="citation_journal_title=Cognit. psychol. Cogn. Psychol.; citation_title=The simplest complete model of choice response time: Linear ballistic accumulation; citation_author=S Brown, A Heathcote; citation_volume=57; citation_publication_date=2008; citation_pages=153-78; citation_doi=10.1016/j.cogpsych.2007.12.002; citation_id=CR41"/>
    <meta name="citation_reference" content="citation_journal_title=J. Math. Psychol.; citation_title=Drawing conclusions from choice response time models: A tutorial using the linear ballistic accumulator; citation_author=C Donkin, S Brown, A Heathcote; citation_volume=55; citation_publication_date=2011; citation_pages=140-151; citation_doi=10.1016/j.jmp.2010.10.001; citation_id=CR42"/>
    <meta name="citation_reference" content="citation_journal_title=J. Exp. Psychol.; citation_title=Buffer loading and chunking in sequential keypressing; citation_author=W Verwey; citation_volume=00; citation_publication_date=1996; citation_pages=544-562; citation_doi=10.1037//0096-1523.22.3.544; citation_id=CR43"/>
    <meta name="citation_reference" content="citation_journal_title=Psychon. Bull. Rev.; citation_title=New insights into statistical learning and chunk learning in implicit sequence acquisition; citation_author=Y Du, J Clark; citation_volume=24; citation_publication_date=2017; citation_pages=1225-1233; citation_doi=10.3758/s13423-016-1193-4; citation_id=CR44"/>
    <meta name="citation_reference" content="citation_journal_title=Cogn. Sci.; citation_title=The temporal dynamics of regularity extraction in non-human primates; citation_author=L Minier, J Fagot, A Rey; citation_volume=40; citation_publication_date=2016; citation_pages=1019-1030; citation_doi=10.1111/cogs.12279; citation_id=CR45"/>
    <meta name="citation_author" content="Wu, Shuchen"/>
    <meta name="citation_author_institution" content="MPRG Computational Principles of Intelligence, Max Planck Institute for Biological Cybernetics, T&#252;bingen, Germany"/>
    <meta name="citation_author" content="&#201;ltet&#337;, No&#233;mi"/>
    <meta name="citation_author_institution" content="Department of Computational Neuroscience, Max Planck Institute for Biological Cybernetics, T&#252;bingen, Germany"/>
    <meta name="citation_author" content="Dasgupta, Ishita"/>
    <meta name="citation_author_institution" content="Google DeepMind, New York City, USA"/>
    <meta name="citation_author" content="Schulz, Eric"/>
    <meta name="citation_author_institution" content="MPRG Computational Principles of Intelligence, Max Planck Institute for Biological Cybernetics, T&#252;bingen, Germany"/>
    <meta name="access_endpoint" content="https://www.nature.com/platform/readcube-access"/>
    <meta name="twitter:site" content="@SciReports"/>
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:image:alt" content="Content cover image"/>
    <meta name="twitter:title" content="Chunking as a rational solution to the speed–accuracy trade-off in a serial reaction time task"/>
    <meta name="twitter:description" content="Scientific Reports - Chunking as a rational solution to the speed&#8211;accuracy trade-off in a serial reaction time task"/>
    <meta name="twitter:image" content="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41598-023-31500-3/MediaObjects/41598_2023_31500_Fig1_HTML.png"/>
    

    
    
    <meta property="og:url" content="https://www.nature.com/articles/s41598-023-31500-3"/>
    <meta property="og:type" content="article"/>
    <meta property="og:site_name" content="Nature"/>
    <meta property="og:title" content="Chunking as a rational solution to the speed–accuracy trade-off in a serial reaction time task - Scientific Reports"/>
    <meta property="og:image" content="https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41598-023-31500-3/MediaObjects/41598_2023_31500_Fig1_HTML.png"/>
    

    <script>
        window.eligibleForRa21 = 'false'; 
    </script>
</head>
<body class="article-page">

<div class="position-relative cleared z-index-50 background-white" data-test="top-containers">
    <a class="c-skip-link" href="#content">Skip to main content</a>



<div class="c-grade-c-banner u-hide">
    <div class="c-grade-c-banner__container">
        
        <p>Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain
            the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in
            Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles
            and JavaScript.</p>

    </div>
</div>

    

    

    
    
        <div class="u-hide u-show-following-ad"></div>

    <aside class="c-ad c-ad--728x90">
        <div class="c-ad__inner" data-container-type="banner-advert">
            <p class="c-ad__label">Advertisement</p>
            
        
            
    <div id="div-gpt-ad-top-1"
         class="div-gpt-ad advert leaderboard js-ad text-center hide-print grade-c-hide"
         data-ad-type="top"
         data-test="top-ad"
         data-pa11y-ignore
         data-gpt
         data-gpt-unitpath="/285/scientific_reports/article"
         data-gpt-sizes="728x90"
         data-gpt-targeting="type=article;pos=top;artid=s41598-023-31500-3;doi=10.1038/s41598-023-31500-3;subjmeta=116,1595,2649,2811,378,477,631;kwrd=Cognitive+neuroscience,Computational+neuroscience,Human+behaviour,Learning+and+memory,Psychology">
        
        <script>
            window.SN = window.SN || {};
            window.SN.libs = window.SN.libs || {};
            window.SN.libs.ads = window.SN.libs.ads || {};
            window.SN.libs.ads.slotConfig = window.SN.libs.ads.slotConfig || {};
            
                window.SN.libs.ads.slotConfig['top'] = {
                    'pos': 'top',
                    'type': 'article',
                    'path': 's41598-023-31500-3'
                };
            
            
            window.SN.libs.ads.slotConfig['kwrd'] = 'Cognitive+neuroscience,Computational+neuroscience,Human+behaviour,Learning+and+memory,Psychology';
            
            
            window.SN.libs.ads.slotConfig['subjmeta'] = '116,1595,2649,2811,378,477,631';
            
            
        </script>
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/scientific_reports/article&amp;sz=728x90&amp;c=-1040953206&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Ds41598-023-31500-3%26doi%3D10.1038/s41598-023-31500-3%26subjmeta%3D116,1595,2649,2811,378,477,631%26kwrd%3DCognitive+neuroscience,Computational+neuroscience,Human+behaviour,Learning+and+memory,Psychology">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/scientific_reports/article&amp;sz=728x90&amp;c=-1040953206&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Ds41598-023-31500-3%26doi%3D10.1038/s41598-023-31500-3%26subjmeta%3D116,1595,2649,2811,378,477,631%26kwrd%3DCognitive+neuroscience,Computational+neuroscience,Human+behaviour,Learning+and+memory,Psychology"
                     alt="Advertisement"
                     width="728"
                     height="90"></a>
        </noscript>
    </div>

        
    
        </div>
    </aside>


    <header class="c-header" id="header" data-header data-track-component="nature-150-split-header" style="border-color:#cedde4">
        <div class="c-header__row">
            <div class="c-header__container">
                <div class="c-header__split">
                    
                    
                    <div class="c-header__logo-container">
                        
                        <a href="/srep"
                           data-track="click" data-track-action="home" data-track-label="image">
                            <picture class="c-header__logo">
                                <source srcset="https://media.springernature.com/full/nature-cms/uploads/product/srep/header-d3c533c187c710c1bedbd8e293815d5f.svg" media="(min-width: 875px)">
                                <img src="https://media.springernature.com/full/nature-cms/uploads/product/srep/header-d3c533c187c710c1bedbd8e293815d5f.svg" height="32" alt="Scientific Reports">
                            </picture>
                        </a>
                    
                    </div>
                    
                    <ul class="c-header__menu c-header__menu--global">
                        <li class="c-header__item c-header__item--padding c-header__item--hide-md-max">
                            <a class="c-header__link" href="https://www.nature.com/siteindex" data-test="siteindex-link"
                               data-track="click" data-track-action="open nature research index" data-track-label="link">
                                <span>View all journals</span>
                            </a>
                        </li>
                        <li class="c-header__item c-header__item--padding c-header__item--pipe">
                            <a class="c-header__link c-header__link--search"
                                href="#search-menu"
                                data-header-expander
                                data-test="search-link" data-track="click" data-track-action="open search tray" data-track-label="button">
                                <svg role="img" aria-hidden="true" focusable="false" height="22" width="22" viewBox="0 0 18 18" xmlns="http://www.w3.org/2000/svg"><path d="M16.48 15.455c.283.282.29.749.007 1.032a.738.738 0 01-1.032-.007l-3.045-3.044a7 7 0 111.026-1.026zM8 14A6 6 0 108 2a6 6 0 000 12z"/></svg><span>Search</span>
                            </a>
                        </li>
                        <li class="c-header__item c-header__item--padding c-header__item--snid-account-widget c-header__item--pipe">
                            
                                <a class="c-header__link eds-c-header__link" id="identity-account-widget" data-track="click_login" data-track-context="header" href='https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41598-023-31500-3?utm_source=chatgpt.com'><span class="eds-c-header__widget-fragment-title">Log in</span></a>
                            
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        
            <div class="c-header__row">
                <div class="c-header__container" data-test="navigation-row">
                    <div class="c-header__split">
                        <ul class="c-header__menu c-header__menu--journal">
                            
                                <li class="c-header__item c-header__item--dropdown-menu" data-test="explore-content-button">
                                    <a href="#explore"
                                       class="c-header__link"
                                       data-header-expander
                                       data-test="menu-button--explore"
                                       data-track="click" data-track-action="open explore expander" data-track-label="button">
                                        <span class="c-header__show-text-sm">Content</span>
                                        <span class="c-header__show-text">Explore content</span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                    </a>
                                </li>
                            
                            
                                <li class="c-header__item c-header__item--dropdown-menu">
                                    <a href="#about-the-journal"
                                       class="c-header__link"
                                       data-header-expander
                                       data-test="menu-button--about-the-journal"
                                       data-track="click" data-track-action="open about the journal expander" data-track-label="button">
                                        <span>About <span class="c-header__show-text">the journal</span></span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                    </a>
                                </li>
                                
                                    <li class="c-header__item c-header__item--dropdown-menu" data-test="publish-with-us-button">
                                        <a href="#publish-with-us"
                                           class="c-header__link c-header__link--dropdown-menu"
                                           data-header-expander
                                           data-test="menu-button--publish"
                                           data-track="click" data-track-action="open publish with us expander" data-track-label="button">
                                            <span>Publish <span class="c-header__show-text">with us</span></span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                        </a>
                                    </li>
                                
                            
                            
                        </ul>
                        <ul class="c-header__menu c-header__menu--hide-lg-max">
                            
                                <li class="c-header__item" data-test="alert-link">
                                    <a class="c-header__link"
                                       href="https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41598"
                                       rel="nofollow"
                                       data-track="nav_sign_up_for_alerts"
                                       data-track-action="Sign up for alerts"
                                       data-track-label="link (desktop site header)"
                                       data-track-external>
                                        <span>Sign up for alerts</span><svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill="#222"/></svg>
                                    </a>
                                </li>
                            
                            
                                <li class="c-header__item c-header__item--pipe">
                                    <a class="c-header__link"
                                       href="https://www.nature.com/srep.rss"
                                       data-track="click"
                                       data-track-action="rss feed"
                                       data-track-label="link">
                                            <span>RSS feed</span>
                                    </a>
                                </li>
                            
                        </ul>
                    </div>
                </div>
            </div>
        
    </header>


    
    
        <nav class="u-mb-16" aria-label="breadcrumbs">
            <div class="u-container">
                <ol class="c-breadcrumbs" itemscope itemtype="https://schema.org/BreadcrumbList">
                    <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:nature"><span itemprop="name">nature</span></a><meta itemprop="position" content="1">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/srep" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:scientific reports"><span itemprop="name">scientific reports</span></a><meta itemprop="position" content="2">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/srep/articles?type&#x3D;article" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:articles"><span itemprop="name">articles</span></a><meta itemprop="position" content="3">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb3" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
                                    <span itemprop="name">article</span><meta itemprop="position" content="4"></li>
                </ol>
            </div>
        </nav>
    



    

</div>


<div class="u-container u-mt-32 u-mb-32 u-clearfix" id="content" data-component="article-container"  data-container-type="article">
    <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
        
            
                <div class="c-context-bar u-hide"
                     id="js-enable-context-bar"
                     data-test="context-bar"
                     data-context-bar
                     aria-hidden="true">
                    <div class="c-context-bar__container" data-track-context="sticky banner">
                        <div class="c-context-bar__title">
                            Chunking as a rational solution to the speed–accuracy trade-off in a serial reaction time task
                        </div>
                        <div class="c-context-bar__cta-container">
                            
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41598-023-31500-3.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="content_download" data-track-type="article pdf download" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

                            
                        </div>
                    </div>
                </div>
            
        
        <article lang="en">
            
                <div class="c-pdf-button__container u-mb-8 u-hide-at-lg js-context-bar-sticky-point-mobile">
                    <div class="c-pdf-container" data-track-context="article body">
                        
                            
                                <div class="app-cta-group">
                                    
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41598-023-31500-3.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="content_download" data-track-type="article pdf download" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

                                    
                                </div>
                            
                        
                    </div>
                </div>
            
            <div class="c-article-header">
                <header>
                    <ul class="c-article-identifiers" data-test="article-identifier">
                        
        <li class="c-article-identifiers__item" data-test="article-category">Article</li>
    
        <li class="c-article-identifiers__item">
            <a href="https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research" data-track="click" data-track-action="open access" data-track-label="link" class="u-color-open-access" data-test="open-access">Open access</a>
        </li>
    
    

                        <li class="c-article-identifiers__item">Published: <time datetime="2023-05-11">11 May 2023</time></li>
                    </ul>

                    <h1 class="c-article-title" data-test="article-title" data-article-title="">Chunking as a rational solution to the speed–accuracy trade-off in a serial reaction time task</h1>
                    <ul class="c-article-author-list c-article-author-list--short" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="1_4" data-track-context="researcher popup with no profile" href="#auth-Shuchen-Wu-Aff1" data-author-popup="auth-Shuchen-Wu-Aff1" data-author-search="Wu, Shuchen" data-corresp-id="c1">Shuchen Wu<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-mail-medium"></use></svg></a><sup class="u-js-hide"><a href="#Aff1">1</a></sup>, </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="2_4" data-track-context="researcher popup with no profile" href="#auth-No_mi-_ltet_-Aff2" data-author-popup="auth-No_mi-_ltet_-Aff2" data-author-search="Éltető, Noémi">Noémi Éltető</a><sup class="u-js-hide"><a href="#Aff2">2</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="3_4" data-track-context="researcher popup with no profile" href="#auth-Ishita-Dasgupta-Aff3" data-author-popup="auth-Ishita-Dasgupta-Aff3" data-author-search="Dasgupta, Ishita">Ishita Dasgupta</a><sup class="u-js-hide"><a href="#Aff3">3</a></sup> &amp; </li><li class="c-article-author-list__show-more" aria-label="Show all 4 authors for this article" title="Show all 4 authors for this article">…</li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="4_4" data-track-context="researcher popup with no profile" href="#auth-Eric-Schulz-Aff1" data-author-popup="auth-Eric-Schulz-Aff1" data-author-search="Schulz, Eric">Eric Schulz</a><sup class="u-js-hide"><a href="#Aff1">1</a></sup> </li></ul><button aria-expanded="false" class="c-article-author-list__button"><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-down-medium"></use></svg><span>Show authors</span></button>

                    

                    <p class="c-article-info-details" data-container-section="info">
                        
    <a data-test="journal-link" href="/srep" data-track="click" data-track-action="journal homepage" data-track-category="article body" data-track-label="link"><i data-test="journal-title">Scientific Reports</i></a>

                        <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 13</b>, Article number: <span data-test="article-number">7680</span> (<span data-test="article-publication-year">2023</span>)
            <a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                    </p>
                    
        
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item" data-test="access-count">
                        <p class="c-article-metrics-bar__count">5954 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item" data-test="citation-count">
                        <p class="c-article-metrics-bar__count">11 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item" data-test="altmetric-score">
                            <p class="c-article-metrics-bar__count">1 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__details"><a href="/articles/s41598-023-31500-3/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                    </li>
                
            </ul>
        </div>
    
                    
                </header>

                
    <div class="u-js-hide" data-component="article-subject-links">
        <h3 class="c-article__sub-heading">Subjects</h3>
        <ul class="c-article-subject-list">
            <li class="c-article-subject-list__subject"><a href="/subjects/cognitive-neuroscience" data-track="click" data-track-action="view subject" data-track-label="link">Cognitive neuroscience</a></li><li class="c-article-subject-list__subject"><a href="/subjects/computational-neuroscience" data-track="click" data-track-action="view subject" data-track-label="link">Computational neuroscience</a></li><li class="c-article-subject-list__subject"><a href="/subjects/human-behaviour" data-track="click" data-track-action="view subject" data-track-label="link">Human behaviour</a></li><li class="c-article-subject-list__subject"><a href="/subjects/learning-and-memory" data-track="click" data-track-action="view subject" data-track-label="link">Learning and memory</a></li><li class="c-article-subject-list__subject"><a href="/subjects/psychology" data-track="click" data-track-action="view subject" data-track-label="link">Psychology</a></li>
        </ul>
    </div>

                
    
    

    
    

                
            </div>

        <div class="c-article-body">
            <section aria-labelledby="Abs1" data-title="Abstract" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>When exposed to perceptual and motor sequences, people are able to gradually identify patterns within and form a compact internal description of the sequence. One proposal of how sequences can be compressed is people’s ability to form chunks. We study people’s chunking behavior in a serial reaction time task. We relate chunk representation with sequence statistics and task demands, and propose a rational model of chunking that rearranges and concatenates its representation to jointly optimize for accuracy and speed. Our model predicts that participants should chunk more if chunks are indeed part of the generative model underlying a task and should, on average, learn longer chunks when optimizing for speed than optimizing for accuracy. We test these predictions in two experiments. In the first experiment, participants learn sequences with underlying chunks. In the second experiment, participants were instructed to act either as fast or as accurately as possible. The results of both experiments confirmed our model’s predictions. Taken together, these results shed new light on the benefits of chunking and pave the way for future studies on step-wise representation learning in structured domains.</p></div></div></section>

            
                
            

            
                
                    
                
                
                <div class="main-content">
                    
                        <section data-title="Introduction"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>William James famously said that we are born into a “blooming, buzzing confusion”, and that we escape that confusion by gradually making sense of the series of events we perceive. How we perceive a sequence of perceptual stimuli, process them, and extract underlying structure, is a fundamental question of psychological investigations. One proposal of how the blooming, buzzing confusion of seemingly disparate sequential events can become one cognitive unit is chunking<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Miller, G. A. The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychol. Rev.&#xA;                  https://doi.org/10.1037/h0043158&#xA;                  &#xA;                 (1956)." href="#ref-CR1" id="ref-link-section-d7146304e406">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Laird, J. E., Rosenbloom, P. S. &amp; Newell, A. Towards chunking as a general learning mechanism. In AAAI, 188–192 (1984)." href="#ref-CR2" id="ref-link-section-d7146304e406_1">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Graybiel, A. M. The basal ganglia and chunking of action repertoires. Neurobiol. Learn. Mem. 70, 119–136 (1998)." href="#ref-CR3" id="ref-link-section-d7146304e406_2">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Servan-Schreiber, E. &amp; Anderson, J. R. Learning artificial grammars with competitive chunking. J. Exp. Psychol. Learn. Mem. Cogn. 16, 592 (1990)." href="/articles/s41598-023-31500-3#ref-CR4" id="ref-link-section-d7146304e409">4</a></sup>. Upon exposure to sequential stimuli, humans and animals can identify repeated patterns and segment sequences into chunks of patterns<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Terrace, H. S. Chunking by a pigeon in a serial learning task. Nature&#xA;                  https://doi.org/10.1038/325149a0&#xA;                  &#xA;                 (1987)." href="/articles/s41598-023-31500-3#ref-CR5" id="ref-link-section-d7146304e413">5</a></sup>. To this end, separate sequential elements merge into one cognitive entity. This cognitive entity is then recalled and identified as a whole<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Mathy, F. &amp; Feldman, J. What’s magic about magic numbers? Chunking and data compression in short-term memory. Cognition&#xA;                  https://doi.org/10.1016/j.cognition.2011.11.003&#xA;                  &#xA;                 (2012)." href="/articles/s41598-023-31500-3#ref-CR6" id="ref-link-section-d7146304e417">6</a></sup>: a phenomenon known as <i>chunking</i><sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Lashley, K. S. The Problem of Serial Order in Behavior Vol. 21 (Bobbs-Merrill Oxford, United Kingdom, 1951)." href="/articles/s41598-023-31500-3#ref-CR7" id="ref-link-section-d7146304e423">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Gobet, F. et al. Chunking mechanisms in human learning. Trends Cog. Sci.&#xA;                  https://doi.org/10.1016/S1364-6613(00)01662-4&#xA;                  &#xA;                 (2001)." href="/articles/s41598-023-31500-3#ref-CR8" id="ref-link-section-d7146304e426">8</a></sup>.</p><p>Chunking is a phenomenon spanning across sequence learning, grammar learning, visual and working memory tasks, and function learning, among others<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Gobet, F. et al. Chunking mechanisms in human learning. Trends Cog. Sci.&#xA;                  https://doi.org/10.1016/S1364-6613(00)01662-4&#xA;                  &#xA;                 (2001)." href="#ref-CR8" id="ref-link-section-d7146304e433">8</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Graybiel, A. M. The basal ganglia and chunking of action repertoires. Neurobiol. Learn. Memory 70, 1–2. &#xA;                  https://doi.org/10.1006/nlme.1998.3843&#xA;                  &#xA;                 (1998)." href="#ref-CR9" id="ref-link-section-d7146304e433_1">9</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Egan, D. E. &amp; Schwartz, B. J. Chunking in recall of symbolic drawings. Memory Cogn.&#xA;                  https://doi.org/10.3758/BF03197595&#xA;                  &#xA;                 (1979)." href="#ref-CR10" id="ref-link-section-d7146304e433_2">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Ellis, N. C. Sequencing in SLA: Phonological memory, chunking, and points of order. Stud. Second Lang. Acquis.&#xA;                  https://doi.org/10.1017/S0272263100014698&#xA;                  &#xA;                 (1996)." href="#ref-CR11" id="ref-link-section-d7146304e433_3">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Koch, I. &amp; Hoffmann, J. Patterns, chunks, and hierarchies in serial reaction-time tasks. Psychol. Res.&#xA;                  https://doi.org/10.1007/PL00008165&#xA;                  &#xA;                 (2000)." href="/articles/s41598-023-31500-3#ref-CR12" id="ref-link-section-d7146304e436">12</a></sup>. The ability to discover statistical regularities in sequences, and to identify them as discrete, disparate units of chunks enables us to form a compact and compressed memory representation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Brady, T. F., Konkle, T. &amp; Alvarez, G. A. Compression in visual working memory: Using statistical regularities to form more efficient memory representations. J. Exp. Psychol.: General&#xA;                  https://doi.org/10.1037/a0016797&#xA;                  &#xA;                 (2009)." href="/articles/s41598-023-31500-3#ref-CR13" id="ref-link-section-d7146304e440">13</a></sup>, readily transferable to novel domains<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Müssgens, D. M. &amp; Ullén, F. Transfer in motor sequence learning: Effects of practice schedule and sequence context. Front. Human Neurosci.&#xA;                  https://doi.org/10.3389/fnhum.2015.00642&#xA;                  &#xA;                 (2015)." href="/articles/s41598-023-31500-3#ref-CR14" id="ref-link-section-d7146304e444">14</a></sup>, and enables us to progress from novices to experts<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Chase, W. G. &amp; Simon, H. A. Perception in chess. Cogn. Psychol.&#xA;                  https://doi.org/10.1016/0010-0285(73)90004-2&#xA;                  &#xA;                 (1973)." href="/articles/s41598-023-31500-3#ref-CR15" id="ref-link-section-d7146304e448">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Gobet, F. &amp; Simon, H. A. Expert chess memory: Revisiting the chunking hypothesis. Memory&#xA;                  https://doi.org/10.1080/741942359&#xA;                  &#xA;                 (1998)." href="/articles/s41598-023-31500-3#ref-CR16" id="ref-link-section-d7146304e451">16</a></sup>. As primitive building blocks of cognitive construction units, a complex and lengthy sequence reduces to several chunks. This property facilitates the organization of actions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Lashley, K. S. The Problem of Serial Order in Behavior Vol. 21 (Bobbs-Merrill Oxford, United Kingdom, 1951)." href="/articles/s41598-023-31500-3#ref-CR7" id="ref-link-section-d7146304e455">7</a></sup>, and can subsequently help with compositionality in learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Schulz, E., Tenenbaum, J. B., Duvenaud, D., Speekenbrink, M. &amp; Gershman, S. J. Compositional inductive biases in function learning. Cogn. Psychol.&#xA;                  https://doi.org/10.1016/j.cogpsych.2017.11.002&#xA;                  &#xA;                 (2017)." href="/articles/s41598-023-31500-3#ref-CR17" id="ref-link-section-d7146304e460">17</a></sup>, communication of structure<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Schulz, E., Quiroga, F. &amp; Gershman, S. J. Communicating compositional patterns. Open. Mind 4, 25–39 (2020)." href="/articles/s41598-023-31500-3#ref-CR18" id="ref-link-section-d7146304e464">18</a></sup>, hierarchical planning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Tomov, M. S., Yagati, S., Kumar, A., Yang, W. &amp; Gershman, S. J. Discovery of hierarchical representations for efficient planning. PLoS Comput. Biol.&#xA;                  https://doi.org/10.1371/journal.pcbi.1007594&#xA;                  &#xA;                 (2020)." href="/articles/s41598-023-31500-3#ref-CR19" id="ref-link-section-d7146304e468">19</a></sup> and others. In short, chunking is a critical and universal learning phenomenon. Here we propose another benefit of chunking in sequential tasks: the ability to more easily predict future outcomes and thereby act faster. Thus, our work connects the literature of chunking with that of the speed-accuracy trade-off.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1" data-title="Figure 1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41598-023-31500-3/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-023-31500-3/MediaObjects/41598_2023_31500_Fig1_HTML.png?as=webp"><img aria-describedby="Fig1" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-023-31500-3/MediaObjects/41598_2023_31500_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="518"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>(<b>a</b>) Task structure for both experiments. Six training blocks are sandwiched between two baseline and two test blocks. The baseline and test blocks contain sequences generated from the “illusory” transition matrix in (<b>c</b>). (<b>b</b>) Participants are instructed to press the corresponding key on the keyboard according to trial-by-trial displayed instructions. They are given feedback on their performance, including accuracy and reaction times before the subsequent trial. (<b>c</b>) A non-deterministic, “illusory” transition matrix of the four possible key-presses is used to generate sequences for the baseline and test blocks for both experiments. The generative transition matrix with the two high (from A to B, C to D) and two medium transition probabilities (from B to C, D to A) produces “illusory” chunks that can be perceived as frequently occurring. To control the effect of habitual presses from consecutive fingers, a random mapping from “A”, “B”, “C”, “D” to “D”, “F”, “J”, “K”, is generated independently for each participant. (<b>d</b>) The instructions for training blocks differed between the two experiments and corresponding groups. In Experiment 1, participants were divided into three groups who learned independent, size 2, and size 3 chunks from a predefined set of chunks with equal probability. In Experiment 2, the sequences in the training blocks were also generated from the “illusory” transition matrix. One group was instructed to act as accurately as possible and the other groups was instructed to act as fast as possible.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41598-023-31500-3/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>The speed-accuracy trade-off is observed both in humans and animals across various task domains<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Wickelgren, W. A. Speed-accuracy tradeoff and information processing dynamics. Acta Physiol. (Oxf) 41, 67–85. &#xA;                  https://doi.org/10.1016/0001-6918(77)90012-9&#xA;                  &#xA;                 (1977)." href="/articles/s41598-023-31500-3#ref-CR20" id="ref-link-section-d7146304e508">20</a></sup>. When speed is emphasized, participants in both lab and naturalistic settings tend to make more mistakes while reacting faster than when accuracy is emphasized<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Bogacz, R., Hu, P. T., Holmes, P. J. &amp; Cohen, J. D. Do humans produce the speed-accuracy trade-off that maximizes reward rate?. Q. J. Exp. Psychol. 63, 863–891. &#xA;                  https://doi.org/10.1080/17470210903091643&#xA;                  &#xA;                 (2010)." href="/articles/s41598-023-31500-3#ref-CR21" id="ref-link-section-d7146304e512">21</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="MacKay, D. G. The problems of flexibility, fluency, and speed-accuracy trade-off in skilled behavior. Psychol. Rev. 89, 483–506 (1982)." href="/articles/s41598-023-31500-3#ref-CR22" id="ref-link-section-d7146304e515">22</a></sup>. While earlier work focused on analyzing reaction times and accuracy<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Bogacz, R., Hu, P. T., Holmes, P. J. &amp; Cohen, J. D. Do humans produce the speed-accuracy trade-off that maximizes reward rate?. Q. J. Exp. Psychol. 63, 863–891. &#xA;                  https://doi.org/10.1080/17470210903091643&#xA;                  &#xA;                 (2010)." href="/articles/s41598-023-31500-3#ref-CR21" id="ref-link-section-d7146304e519">21</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Fitts, P. M. Cognitive aspects of information processing: III. Set for speed versus accuracy. J. Exp. Psychol. 71, 849–857. &#xA;                  https://doi.org/10.1037/h0023232&#xA;                  &#xA;                 (1966)." href="/articles/s41598-023-31500-3#ref-CR23" id="ref-link-section-d7146304e522">23</a></sup>, little work has been done to relate the speed-accuracy trade-off to chunking and examine it affects the process and outcome of learning representations.</p><p>The serial reaction time task (SRT), a classical paradigm to study motor sequence learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Koch, I. &amp; Hoffmann, J. Patterns, chunks, and hierarchies in serial reaction-time tasks. Psychol. Res.&#xA;                  https://doi.org/10.1007/PL00008165&#xA;                  &#xA;                 (2000)." href="/articles/s41598-023-31500-3#ref-CR12" id="ref-link-section-d7146304e529">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Nissen, M. J. &amp; Bullemer, P. Attentional requirements of learning: Evidence from performance measures. Cogn. Psychol. 19, 1–32 (1987)." href="#ref-CR24" id="ref-link-section-d7146304e532">24</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Willingham, D. B., Nissen, M. J. &amp; Bullemer, P. On the development of procedural knowledge. J. Exp. Psychol. Learn. Mem. Cogn. 15, 1047 (1989)." href="#ref-CR25" id="ref-link-section-d7146304e532_1">25</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Robertson, E. M. The serial reaction time task: Implicit motor skill learning?. J. Neurosci. 27, 10073–10075 (2007)." href="/articles/s41598-023-31500-3#ref-CR26" id="ref-link-section-d7146304e535">26</a></sup>, is ideal for studying the speed-accuracy trade-off and chunking. In SRTs, sequences of instruction cues appear consecutively on the screen, after which participants react by pressing the corresponding key that maps to the cue. If particular patterns, for example, ABC, keep repeating, then grouping repeated chunks as a unit facilitates the prediction of upcoming sequences. The detection of a chunk’s beginning, in this case, A, implies that the within-chunk items B and then C will follow. This anticipation of the following elements of a given chunk can allow participants to anticipate what is coming next and thereby react faster<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Koch, I. &amp; Hoffmann, J. Patterns, chunks, and hierarchies in serial reaction-time tasks. Psychol. Res.&#xA;                  https://doi.org/10.1007/PL00008165&#xA;                  &#xA;                 (2000)." href="/articles/s41598-023-31500-3#ref-CR12" id="ref-link-section-d7146304e539">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Müssgens, D. M. &amp; Ullén, F. Transfer in motor sequence learning: Effects of practice schedule and sequence context. Front. Human Neurosci.&#xA;                  https://doi.org/10.3389/fnhum.2015.00642&#xA;                  &#xA;                 (2015)." href="/articles/s41598-023-31500-3#ref-CR14" id="ref-link-section-d7146304e542">14</a></sup>. Chunking sequence elements, however, can also come at a cost when the sequence is probabilistic. By assuming deterministic transitions between the within-chunk items AB, participants might lose fine-grained statistical information about single-item instructions and thereby occasionally miss between-chunk transitions such as AC. This, in turn, can decrease their accuracy.</p><p>We propose a model that trades off between speed and accuracy when performing SRTs. Our model calculates the utility of acquired chunk representations as a weighted sum of how well they capture the statistical structure in the SRT (accuracy) and whether they permit faster responses (speed). Our model then iteratively decides whether or not to chunk consecutive items. This model makes two distinct predictions. First, in environments where deterministic chunks exist, adding them to the representation is beneficial because they speed up reaction times without losing accuracy. Thus, people should chunk more in environments with more or longer chunks. In our first experiment, we tested this prediction by training participants on sequences containing underlying chunks. We designed a couple of analysis methods to test and verified this prediction. The results of this experiment suggest that participants adapt their chunking behavior to the underlying chunks in the sequence when they are given universal instructions to act as fast and accurately as possible. A subsequent prediction following the first experiment from the model is that when participants are given distinct instructions to perform on the task, these instructions will induce distinct chunking behavior even when the sequence have the same underlying statistics for the two groups. Specifically, this will be a rational strategy for the model to learn chunks in cases where the underlying environment is non-deterministic and does not contain any chunks. As the utility of speed increases (at the cost of accuracy), participants might also chunk consecutive elements more often and learn longer chunks. Since chunking frequently co-occurring events improves reaction time at the cost of overall accuracy, chunking can be a rational strategy to act faster. We tested and verified this prediction in a second experiment by training participants on sequences generated from a first-order Markovian transition matrix with “illusory” chunks while instructing one group to focus on speed and the other group to focus on accuracy. The results of this second experiment suggest that the group focusing on speed chunked more than the group focusing on accuracy. The fast group learns more chunks and makes more mistakes. While the accurate group learns the underlying generative model of the sequence better, but smaller chunks than the fast group. Our results shed new light on the benefits of chunking under specific task instructions and pave the way for future studies on structural inference in statistical learning domains.</p><h3 class="c-article__sub-heading" id="Sec2">Serial reaction time task</h3><p>We study chunking in a serial reaction time task (SRT, see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig1">1</a>b). Participants are instructed to press keys corresponding to a sequence of cues that appear on the screen. The instruction cross turns green after a correct keypress and red after an incorrect keypress. The subsequent trial starts after a 500ms response-to-stimulus interval. The task starts with two baseline blocks followed by six training blocks and ends with two test blocks. Each block consists of 100 trials. For both experiments, the same generative mechanism produces the baseline and the test blocks. To study whether participants’ chunking behavior adapts to task demands in an SRT task, we manipulate various properties of the training blocks to examine how they affect behavior in the test block, using the baseline block as a comparison. The observed differences between the test and baseline blocks reflect the changes in representations elicited by the training blocks.</p><p>There are various approaches to generating sequences in an SRT paradigm. One type of instruction involves repeated sequence<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Robertson, E. M. The serial reaction time task: Implicit motor skill learning?. J. Neurosci. 27, 10073–10075 (2007)." href="/articles/s41598-023-31500-3#ref-CR26" id="ref-link-section-d7146304e563">26</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Keele, S. W., Ivry, R., Mayr, U., Hazeltine, E. &amp; Heuer, H. The cognitive and neural architecture of sequence representation. Psychol. Rev. 110, 316–339. &#xA;                  https://doi.org/10.1037/0033-295X.110.2.316&#xA;                  &#xA;                 (2003)." href="/articles/s41598-023-31500-3#ref-CR27" id="ref-link-section-d7146304e566">27</a></sup>, while others avoid direct repetitions or runs such as 1234<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Willingham, D. B., Salidis, J. &amp; Gabrieli, J. D. Direct comparison of neural systems mediating conscious and unconscious skill learning. J. Neurophysiol. 88, 1451–1460. &#xA;                  https://doi.org/10.1152/jn.2002.88.3.1451&#xA;                  &#xA;                 (2002)." href="/articles/s41598-023-31500-3#ref-CR28" id="ref-link-section-d7146304e570">28</a></sup>, where 1,2,3,4 refer to 4 targets on the computer screen. One probabilistic way of generating the sequence is the alternating serial reaction time task<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Howard, J. H. &amp; Howard, D. V. Age differences in implicit learning of higher order dependencies in serial patterns. Psychol. Aging 12, 634–656. &#xA;                  https://doi.org/10.1037//0882-7974.12.4.634&#xA;                  &#xA;                 (1997)." href="/articles/s41598-023-31500-3#ref-CR29" id="ref-link-section-d7146304e574">29</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Romano, J. C., Howard, J. H. &amp; Howard, D. V. One-year retention of general and sequence-specific skills in a probabilistic, serial reaction time task. Memory 18, 427–441. &#xA;                  https://doi.org/10.1080/09658211003742680&#xA;                  &#xA;                 (2010)." href="/articles/s41598-023-31500-3#ref-CR30" id="ref-link-section-d7146304e577">30</a></sup>, where instruction patterns can be 1r4r3r2r, with r being a randomly chosen target. Other probabilistic ways of generating the presented sequences include choosing successive images according to a probabilistic first-order Markov transition process, specified by a conditional probability matrix<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Bornstein, A. &amp; Daw, N. Cortical and hippocampal correlates of deliberation during model-based decisions for rewards in humans. PLoS Comput. Biol. 9, e1003387. &#xA;                  https://doi.org/10.1371/journal.pcbi.1003387&#xA;                  &#xA;                 (2013)." href="/articles/s41598-023-31500-3#ref-CR31" id="ref-link-section-d7146304e581">31</a></sup>. Schvaneveldt and Gomez used two sequences, such as 1243 and 1342 and drew the target sequence via weighted coin flip results<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Schvaneveldt, R. W. &amp; Gomez, R. L. Attention and probabilistic sequence learning. Psychol. Res. 61, 175–190. &#xA;                  https://doi.org/10.1007/s004260050023&#xA;                  &#xA;                 (1998)." href="/articles/s41598-023-31500-3#ref-CR32" id="ref-link-section-d7146304e585">32</a></sup>. Several reasons have been put forward in the literature for using probabilistic transitions to generate SRT sequences. One is that probabilistic transitions allow continuous and flexible assessment of learning progression. Another one is that the probabilistic nature of the sequences allows for a larger variety of sequence chunks to be generated and learned<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Cleeremans, A. &amp; McClelland, J. L. Learning the structure of event sequences. J. Exp. Psychol. Gen. 120, 235–253 (1991)." href="/articles/s41598-023-31500-3#ref-CR33" id="ref-link-section-d7146304e590">33</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Provyn, J. P. Associative processes in statistical learning: Paradoxical predictions of the past. Psychol.-Diss. 179, 78 (2013)." href="/articles/s41598-023-31500-3#ref-CR34" id="ref-link-section-d7146304e593">34</a></sup>.</p><p>In both of our experiments, the sequences in the baseline and test blocks are generated from a non-deterministic, first-order Markovian transition matrix between the four instruction keys. In particular, out of all 16 transitions specified between the four keys, the transitions from A to B and C to D are highly probable (<i>P</i> = 0.9), and the transitions from B to C and from D to A are medium probable (<i>P</i> = 0.7) (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig1">1</a>c). In this way, participants often observe reoccurring sequence segments such as AB and CD and could possibly perceive them as “illusory” chunks, even though the generative model is nondeterministic first-order Markovian.</p><p>We manipulate the training block sequences across the two experiments. In Experiment 1, three groups of participants were trained on sequences containing no chunks (independent), chunk AB (size 2 chunk), or chunk ABC (size 3 chunk). In Experiment 2, the same “illusory” transition matrix generates the training block sequences but the instructions differ across the two experimental groups. One group is instructed to respond as accurately as possible, while the other is instructed to respond as fast as possible. In order to control for motor effects due to hand and finger dominance, the instructions “A”, “B”, “C”, “D” are randomly mapped to the keys “D”, “F”, “J”, “K” for individual participants. In the next section, we discuss the predictions of our rational model of chunking for the two different experiments and their conditions.</p></div></div></section><section data-title="Related work"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Related work</h2><div class="c-article-section__content" id="Sec3-content"><p>Three major types of chunking models have been proposed in the cognitive science literature. The first type are symbolic models, including PARSER and CCN (competitive chunker)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Perruchet, P. &amp; Vinter, A. Parser: A model for word segmentation. J. Mem. Lang. 39, 246–263. &#xA;                  https://doi.org/10.1006/jmla.1998.2576&#xA;                  &#xA;                 (1998)." href="/articles/s41598-023-31500-3#ref-CR35" id="ref-link-section-d7146304e621">35</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Servan-Schreiber, E. &amp; Anderson, J. Learning artificial grammars with competitive chunking. J. Exp. Psychol. Learn. Mem. Cogn. 16, 592–608. &#xA;                  https://doi.org/10.1037/0278-7393.16.4.592&#xA;                  &#xA;                 (1990)." href="/articles/s41598-023-31500-3#ref-CR36" id="ref-link-section-d7146304e624">36</a></sup>. Symbolic models learn chunks from already-encountered items and constructs a hierarchy of chunks as participants remember sentences. Sevan-Schreiber and Anderson showed that these models can replicate the behavior of participants’ judgment of grammaticality from sequences with distinct hierarchy levels (e.g., word level vs. phrase level)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Servan-Schreiber, E. &amp; Anderson, J. Learning artificial grammars with competitive chunking. J. Exp. Psychol. Learn. Mem. Cogn. 16, 592–608. &#xA;                  https://doi.org/10.1037/0278-7393.16.4.592&#xA;                  &#xA;                 (1990)." href="/articles/s41598-023-31500-3#ref-CR36" id="ref-link-section-d7146304e628">36</a></sup>. Additionally, they replicate the participants’ tendency to overtly chunk the training sentences even when they are presented in an unstructured way. Another model of this kind is PARSER<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Perruchet, P. &amp; Vinter, A. Parser: A model for word segmentation. J. Mem. Lang. 39, 246–263. &#xA;                  https://doi.org/10.1006/jmla.1998.2576&#xA;                  &#xA;                 (1998)." href="/articles/s41598-023-31500-3#ref-CR35" id="ref-link-section-d7146304e632">35</a></sup>. Proposed by Perruchet and Vinter, PARSER randomly samples the size of the next chunk of syllables and parses the sequence by disjunctive chunks. Each chunk learned by the model is associated with a weight, which increments with observational frequency and decrements via a forgetting mechanism. PARSER can produce artificial language stream segmentations of continuous input streams without episodic cues such as pauses.</p><p>The second type are connectionist models of chunk learning. This includes TRACX<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="French, R. M., Addyman, C. &amp; Mareschal, D. TRACX: A recognition-based connectionist framework for sequence segmentation and chunk extraction. Psychol. Rev. 118, 614–636. &#xA;                  https://doi.org/10.1037/a0025255&#xA;                  &#xA;                 (2011)." href="/articles/s41598-023-31500-3#ref-CR37" id="ref-link-section-d7146304e639">37</a></sup> and SRN (simple recurrent network)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Cleeremans, A., Servan-Schreiber, D. &amp; McClelland, J. L. Finite state automata and simple recurrent networks. Neural Comput. 1, 372–381. &#xA;                  https://doi.org/10.1162/neco.1989.1.3.372&#xA;                  &#xA;                 (1989)." href="/articles/s41598-023-31500-3#ref-CR38" id="ref-link-section-d7146304e643">38</a></sup>. TRACX uses a three-layer feedforward backpropagation autoassociator and adapts the autoassociator’s weights to the difference between its prediction and actual sequential units when this difference exceeds a pre-defined threshold. Wang et al. trained a self-organized recurrent spiking neural network with spike-timing-dependent plasticity and homeostatic plasticity on sequences. The model was shown to reproduce several sequence learning effects<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Wang, Q., Rothkopf, C. A. &amp; Triesch, J. A model of human motor sequence learning explains facilitation and interference effects based on spike-timing dependent plasticity. PLoS Comput. Biol.&#xA;                  https://doi.org/10.1371/journal.pcbi.1005632&#xA;                  &#xA;                 (2017)." href="/articles/s41598-023-31500-3#ref-CR39" id="ref-link-section-d7146304e647">39</a></sup>.</p><p>The two model types mentioned above are process models. In contrast to process models stand normative statistical models, which model the ideal observers’ behavior. This approach includes variants of the Bayesian ideal observer framework<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Goldwater, S., Griffiths, T. &amp; Johnson, M. A bayesian framework for word segmentation: Exploring the effects of context. Cognition 112, 21–54. &#xA;                  https://doi.org/10.1016/j.cognition.2009.03.008&#xA;                  &#xA;                 (2009)." href="/articles/s41598-023-31500-3#ref-CR40" id="ref-link-section-d7146304e654">40</a></sup>. Given a linguistic corpus, these models find a segmentation with the highest probability that contains relatively few word types, exploiting the minimal description length principle. These models are also rational because their inference is evaluated on observational instances. They provide accounts for high-level computation required for chunk learning. As normative and process models rely on different principles, they are usually not compared against each other.</p><p>While these models focused on the benefit of chunking in memory compressibility and grammaticality sensibility, little work relates task instruction with the chunks acquired during learning. Since sequence statistics was the main guidance for chunk learning in these models, instruction modulation has rarely been taken into account.</p><p>One typical instruction that changes participants learning behavior is to focus on either speed or accuracy<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Wickelgren, W. A. Speed-accuracy tradeoff and information processing dynamics. Acta Physiol. (Oxf) 41, 67–85. &#xA;                  https://doi.org/10.1016/0001-6918(77)90012-9&#xA;                  &#xA;                 (1977)." href="/articles/s41598-023-31500-3#ref-CR20" id="ref-link-section-d7146304e665">20</a></sup>. When task instructions emphasize speed, participants in both lab and naturalistic settings tend to make more mistakes while reacting faster than when instructions emphasize accuracy<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Bogacz, R., Hu, P. T., Holmes, P. J. &amp; Cohen, J. D. Do humans produce the speed-accuracy trade-off that maximizes reward rate?. Q. J. Exp. Psychol. 63, 863–891. &#xA;                  https://doi.org/10.1080/17470210903091643&#xA;                  &#xA;                 (2010)." href="/articles/s41598-023-31500-3#ref-CR21" id="ref-link-section-d7146304e669">21</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="MacKay, D. G. The problems of flexibility, fluency, and speed-accuracy trade-off in skilled behavior. Psychol. Rev. 89, 483–506 (1982)." href="/articles/s41598-023-31500-3#ref-CR22" id="ref-link-section-d7146304e672">22</a></sup>. While earlier work focused on reaction time and accuracy of decision-making tasks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Bogacz, R., Hu, P. T., Holmes, P. J. &amp; Cohen, J. D. Do humans produce the speed-accuracy trade-off that maximizes reward rate?. Q. J. Exp. Psychol. 63, 863–891. &#xA;                  https://doi.org/10.1080/17470210903091643&#xA;                  &#xA;                 (2010)." href="/articles/s41598-023-31500-3#ref-CR21" id="ref-link-section-d7146304e676">21</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Fitts, P. M. Cognitive aspects of information processing: III. Set for speed versus accuracy. J. Exp. Psychol. 71, 849–857. &#xA;                  https://doi.org/10.1037/h0023232&#xA;                  &#xA;                 (1966)." href="/articles/s41598-023-31500-3#ref-CR23" id="ref-link-section-d7146304e679">23</a></sup>, little work relates chunking to the speed-accuracy trade-off. It is unclear how instruction will affect chunking and what type of models can take this particular aspect of the task into account.</p><p>Here we propose a rational chunking model that takes sequence statistics and task instruction as two parts of a utility function for learning. The model tries to find rational ways of chunking the sequence under task demands, trading off speed with accuracy. Each aspect of the utility function implies a specific prediction: the same task instruction but different sequence statistics should lead to distinct chunking behavior; the same sequence statistics but different instructions should also lead to differently learned chunks. We propose two experiments to test the two aspects of our model’s predictions. For the first experiment, we look at the case when three groups of participants learn from sequences with varying underlying chunks, how chunking changes with varying underlying sequential statistics with different embedded chunks, and show that our model captures participants’ learned chunks. For the second experiment, we look at how participants’ behavior differs when the task instructions focus separately on speed versus accuracy with the same underlying sequence. In doing so, we also propose several novel ways of analyzing RT data based on the speed-up of reaction time, thereby giving insights into how chunks build up across practice trials.</p></div></div></section><section data-title="A rational model of chunking"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">A rational model of chunking</h2><div class="c-article-section__content" id="Sec4-content"><p>In the SRT, single instructions <i>z</i> out of an instruction set <i>Z</i> are presented sequentially. We told participants to press the corresponding key as soon as a new instruction appears. The subsequent instruction shows up in a fixed interval after a participant’s completion of the previous trial. The model learns a set of chunks <span class="mathjax-tex">\(C = \{c_1, ..., c_n\}\)</span> and uses the set to parse the sequence. It evaluates the probability <i>P</i>(<i>c</i>) of parsing each chunk <i>c</i> and the conditional probability <span class="mathjax-tex">\(P(c_j|c_i)\)</span> that <span class="mathjax-tex">\(c_j\)</span> follows <span class="mathjax-tex">\(c_i\)</span> for every pair of chunks.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2" data-title="Figure 2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41598-023-31500-3/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-023-31500-3/MediaObjects/41598_2023_31500_Fig2_HTML.png?as=webp"><img aria-describedby="Fig2" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-023-31500-3/MediaObjects/41598_2023_31500_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="507"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>(<b>a</b>) Chunking mechanism of rational model. The model keeps track of marginal and transitional probabilities among every pair of pre-existing chunks, and combines chunk pairs that yield the greatest joint probability as the next candidate to be chunked together. At the start, the four different keys are initialized to be the primitive chunks. A loss function that trades off reaction times and accuracy is evaluated on the pre-existing set of chunks. If a chunk update reduces the loss function, then the two pre-existing chunks are combined together. A parameter <i>w</i> determines how much more the model weighs an decrease of reaction times compared to an increase in accuracy. (<b>b</b>) Example model simulations of learning sequences of Experiment 1. A, B, C, D, are randomly mapped to D, F, J, K for individual participants. Because the transition AB occurred frequently, the model proposes this transition as a possible chunk. (<b>c</b>) Model simulation for Experiment 1. Bars represent the probability of a particular chunk parsed in a simulation over the whole experiment. The bars for the independent group on chunk AB, the independent and the size 2 group on chunk BC, and the independent and size 2 group on chunk ABC contain the probability of 0 and are therefore not visible in the graph. Note that these bars can be arbitrarily increased by changing <span class="mathjax-tex">\(w\)</span> while the qualitative results remain the same. (<b>d</b>) Model simulation for Experiment 2. Top: Average chunk length of different simulations when increasing <span class="mathjax-tex">\(w\)</span> from 0 (optimizing only accuracy) to 1 (optimizing only speed). As <span class="mathjax-tex">\(w\)</span> increases the average chunk length increases, indicating that the model learns longer chunks when asked to care more about acting fast. Bottom: Transition probabilities learned by model with <span class="mathjax-tex">\(w = 0\)</span> and <span class="mathjax-tex">\(w = 1\)</span>, corresponding to the rational maximization of accuracy and speed. If the model tries to act as accurately as possible, then it recovers the true transition probabilities of the “illusory” transition matrix. If the model tries to act as fast as possible, then sets the medium and high transition probabilities to be 1, i.e. deterministic. All results are averaged across 120 independent simulations. Error bars represent the standard error of the mean.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41598-023-31500-3/figures/2" data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><p>The set of chunks <i>C</i> is initialized as the set of available single instructions <i>Z</i> at the beginning of all simulations. The model updates this set by potentially concatenating existing pairs of chunks in <i>C</i>. Adding a chunk expands the parsing horizon as the rest of the within-chunk items are predicted to deterministically follow the initiation item of the chunk. Therefore, the subsequent within-chunk items are anticipated in the following trials. The model’s accuracy might diminish if the subsequent instructions are inconsistent with the predicted within-chunk items. We relate subsequent item predictions to reaction times in the next section, and then explain the process by which a rational model updates chunks based on the trade-off between reaction times and accuracy.</p><h3 class="c-article__sub-heading" id="Sec5">Accounting for reaction times</h3><p>We use a linear ballistic accumulator (LBA) model to simulate reaction times (RT). LBAs are a common class of multi-choice models<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Brown, S. &amp; Heathcote, A. The simplest complete model of choice response time: Linear ballistic accumulation. Cognit. psychol. Cogn. Psychol. 57, 153–78. &#xA;                  https://doi.org/10.1016/j.cogpsych.2007.12.002&#xA;                  &#xA;                 (2008)." href="/articles/s41598-023-31500-3#ref-CR41" id="ref-link-section-d7146304e1003">41</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Donkin, C., Brown, S. &amp; Heathcote, A. Drawing conclusions from choice response time models: A tutorial using the linear ballistic accumulator. J. Math. Psychol. 55, 140–151 (2011)." href="/articles/s41598-023-31500-3#ref-CR42" id="ref-link-section-d7146304e1006">42</a></sup>. In the LBA, each choice corresponds to an evidence accumulator, translated to each four possible key-presses in our task. At every trial of the SRT task, each evidence accumulator starts with an initial evidence <span class="mathjax-tex">\(k = \log (P(z_i))\)</span>, which reflects the model’s prediction on the upcoming instructions. The trials are divided into within-chunk trials and between-chunk trials. For a within-chunk trial, the prediction for the within-chunk item is the initial evidence for the accumulator <span class="mathjax-tex">\(\log (1)\)</span>, the rest being <span class="mathjax-tex">\(\log (\epsilon )\)</span>. Note that the model still integrates information from the SRT instructions but with a high offset which biases it to choose the response which is consistent with the chunk, even if it is inconsistent with the instructed item. This term encourages the model to create longer chunks to reduce the average reaction time.</p><p>For a between-chunk trial, the initial evidence for each accumulator <span class="mathjax-tex">\(z_i\)</span> is determined by the transition probability <span class="mathjax-tex">\(P(c_i|c_j)\)</span> of the chunk <span class="mathjax-tex">\(c_i\)</span> that initiates with the accumulator <span class="mathjax-tex">\(z_i\)</span>, given the previously parsed chunk <span class="mathjax-tex">\(c_j\)</span>. All response accumulators start from the initial evidence, and drift towards the decision threshold with positive drift rates <span class="mathjax-tex">\(v_A, v_B, v_C, v_D\)</span> sampled from a normal distribution with mean <span class="mathjax-tex">\(v_{instruction}\)</span> and standard deviation <span class="mathjax-tex">\(\sigma\)</span>. To simulate the Rt of a particular trial, the current instruction carries the highest drift rate <span class="mathjax-tex">\(v_{instruction} = 0.5\)</span> and the evidence accumulators corresponding to the other instructions have an equal but lower drift rate <span class="mathjax-tex">\(v_{\lnot instruction} = \frac{1 - v_{instruction}}{3}\)</span>. The drift rates for all accumulators sum up to 1. For example, if the current instruction is <i>A</i>, then <span class="mathjax-tex">\(v_A = 0.5\)</span>, <span class="mathjax-tex">\(v_B = v_C = v_D = \frac{0.5}{3}\)</span>. Evidence accumulation terminates when a positive response threshold <i>b</i> is first crossed by any accumulator. The accumulator that crosses the decision threshold first becomes the overt response, and the time it takes to reach the decision threshold is the simulated RT on that trial. In all of the model simulations, we use the same <span class="mathjax-tex">\(v_{instruction}=0.5\)</span>, decision threshold <span class="mathjax-tex">\(b = 1\)</span>, <span class="mathjax-tex">\(\epsilon = 0.01\)</span>, and standard deviation <span class="mathjax-tex">\(\sigma = 0.03\)</span> across all accumulators.</p><h3 class="c-article__sub-heading" id="Sec6">Balancing speed and accuracy</h3><p>We assume that chunking enables participants to predict upcoming instructions further into the future and thereby to react faster by initializing their evidence at a higher starting point. However, chunking also bears a risk of making mistakes when the upcoming instructions are not the subsequent items within a chunk. We formulate this speed-accuracy trade-off using the loss function</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\mathscr {L}} = w Rt + (1-w) Err , \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where <span class="mathjax-tex">\(Rt\)</span> is the average reaction time in the SRT, given a learned chunk representation and sequence, and <span class="mathjax-tex">\(Err\)</span> is the average error rate. <i>w</i> is a parameter that specifies the trade-off between accuracy and reaction time. When <span class="mathjax-tex">\(w = 0\)</span>, only the reaction time term <span class="mathjax-tex">\(Rt\)</span> occupies the loss, and when <span class="mathjax-tex">\(w = 1\)</span>, the error term <span class="mathjax-tex">\(Err\)</span> dominates.</p><p>Based on the LBA reaction time simulation, the average reaction time of parsing chunk <span class="mathjax-tex">\(c_j\)</span> after previously having parsed the chunk <span class="mathjax-tex">\(c_i\)</span> is <span class="mathjax-tex">\(\frac{rt_{between}(c_j,c_i)+ (|c_j|-1)rt_{within}}{|c_j|}\)</span>. <span class="mathjax-tex">\(|c_j|\)</span> is the length of the chunk. The reaction time on the first item is denoted as <span class="mathjax-tex">\(rt_{between}(c_j,c_i)\)</span>, since <span class="mathjax-tex">\(P(c_j|c_i)\)</span> influences the evidence accumulation for this between-chunk key press and only the boundary of the chunk contains transition uncertainty and contributes to the slow down of reaction times. As the initial chunk item determines the chunk identification, the subsequent reaction time to press within-chunk keys in <span class="mathjax-tex">\(c_i\)</span> is denoted as <span class="mathjax-tex">\(rt_{within}\)</span>. This term does not depend on <span class="mathjax-tex">\(c_i\)</span> as the procession to the within-chunk items contains no uncertainty. Taken together, the average reaction time can be formulated as follow, averaging the probability of parsing each acquired chunk <span class="mathjax-tex">\(c_j\)</span> given the previously parsed chunk <span class="mathjax-tex">\(c_i\)</span></p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} Rt = \sum _{c_i \in C}P_C(c_i) \sum _{c_j \in C}P_C(c_j|c_i) \left[ \frac{rt_{between}(c_j,c_i)+ (|c_j|-1)rt_{within}}{|c_j|}\right] , \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>Similarly, if we formulate <span class="mathjax-tex">\(R_{LBA}(z_j)\)</span> as the response choice of the LBA model when the instruction is <span class="mathjax-tex">\(z_j\)</span>, then we can denote an error occurrence as <span class="mathjax-tex">\(\mathbbm {1}\left[ z_j \ne response(z_j)\right]\)</span>, which is an indicator function that becomes 1 when the instruction <span class="mathjax-tex">\(z_j\)</span> is inconsistent with the LBA response. The average error rate can be evaluated by averaging the error rate with the probability of single-element transitions from the generative model <span class="mathjax-tex">\(P_I\)</span>, enabling the formulation of the expected error rate as</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \text {Err} = \sum _{z_i \in \{A,B,C,D\}}P_I(z_i) \sum _{z_j \in \{A,B,C,D\}}P_I(z_j|z_i) \mathbbm {1}\left[ z_j \ne R_{LBA}(z_j)\right] \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p>This utility function, therefore, induces a trade-off between being accurate (predicting elements correctly) and being fast (finding a chunk representation to predict further ahead and speed up one’s reaction time). Together, these parts of the loss are used to evaluate the utility of a chunk representation under specific task demands.</p><h3 class="c-article__sub-heading" id="Sec7">The rational update of chunking</h3><p>The model updates the chunk representation rationally by concatenating chunks within the chunk set <i>C</i> that induce a lower loss. <i>C</i> is initialized with single sequential items <span class="mathjax-tex">\(\{A,B,C,D\}\)</span>. For one set of chunks <i>C</i>, the model evaluates the marginal probabilities of each chunk <span class="mathjax-tex">\(P_C(c_i), c_i \in C\)</span> and the transition probability <span class="mathjax-tex">\(P_C(c_j|c_i)\)</span> of parsing chunk <span class="mathjax-tex">\(c_j\)</span> after having parsed chunk <span class="mathjax-tex">\(c_i\)</span>. <span class="mathjax-tex">\(P_C(c_i)\)</span> and <span class="mathjax-tex">\(P_C(c_j|c_i)\)</span> are stored in the marginal and transition probability matrices as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig2">2</a>b. The marginal and transition probability is evaluated empirically over an entire sequence parse using chunks in <i>C</i>.</p><p>We can calculate the joint occurrence probability of concatenating chunk <span class="mathjax-tex">\(c_i\)</span> with <span class="mathjax-tex">\(c_j\)</span> as <span class="mathjax-tex">\(P(c_i, c_j) = P_C(c_j|c_i) P_C(c_i)\)</span>. The chunk pair <span class="mathjax-tex">\(c_i\)</span>, <span class="mathjax-tex">\(c_j\)</span> with the highest joint probability is suggested as a new chunk to replace <span class="mathjax-tex">\(c_i\)</span> to form a new to the set of chunks <span class="mathjax-tex">\(C_{new}\)</span>. As the initiation of <span class="mathjax-tex">\(c_i\)</span> is predictive of the subsequent chunk items. For example, an addition to <span class="mathjax-tex">\(\{A, B, C ,D\}\)</span> could be a new chunk <i>AB</i>. The new chunk <i>AB</i> then replaces <i>A</i> and the new proposed set of chunks <span class="mathjax-tex">\(C_{new}\)</span> becomes <span class="mathjax-tex">\(\{AB, B, C, D\}\)</span>.</p><p>We then compute whether <span class="mathjax-tex">\(C_{new}\)</span> is accepted to replace the original set of chunks <i>C</i>. The acceptance depends on whether the new set of chunks <span class="mathjax-tex">\(C_{new}\)</span> and the induced reaction time in addition to the marginal and transition probabilities upon parsing the sequence lead to a lower loss. In case it is so, <span class="mathjax-tex">\(C_{new}\)</span> replaces <i>C</i>, which becomes the basis of proposing the next chunk. This chunk proposal process continues until a fixed iteration number, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig2">2</a>a.</p><h3 class="c-article__sub-heading" id="Sec8">Model predictions for experiment 1</h3><p>We first examined the model’s chunk learning behavior on the three groups of Experiment 1. In this simulation, the underlying generative model either contained no chunks (independent), the chunk AB (size 2 chunks), or the chunk ABC (size 3 chunks). We then fixed the trade-off parameter <span class="mathjax-tex">\(w\)</span> to optimize accuracy more than speed by setting it to <span class="mathjax-tex">\(w = 0.2\)</span>. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig2">2</a>c shows the probability of chunk AB, BC, and ABC being learned as subchunks by the rational model of chunking over 120 simulations in total. The model uses the entire sequence to learn its chunk representation in each simulation. With the same trade-off between speed and accuracy, the rational chunking model trained on sequences with size 2 and 3 chunks has a higher probability of learning AB as a subchunk than a model trained on the independent sequence. Chunk BC has a higher probability of being learned by the model trained on sequences containing size 3 chunks than models trained on sequences with independent instructions or sequences with size 2 chunks. Only the model trained on sequences containing size 3 chunks learned about the chunk ABC. Taken together, these simulations predict that participants in the different conditions will be more likely to learn the corresponding chunks than participants for whom a chunk is not part of the training sequence.</p><h3 class="c-article__sub-heading" id="Sec9">Model predictions for experiment 2</h3><p>We examined the model’s chunk learning behavior for Experiment 2. According to our model, changing the trade-off between accuracy and speed translates to changing the cost function’s <span class="mathjax-tex">\(w\)</span> away from 0 and towards 1. We therefore simulated the behavior of our model with changing <span class="mathjax-tex">\(w\)</span> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig2">2</a>d). As <span class="mathjax-tex">\(w\)</span> goes from 0 to 1, i.e. the cost function shifts from minimizing the model’s error rate to minimizing its reaction time, the average length of chunks learned by the model increases. Thus, our model predicts that participants in the fast group, which demands speedier responses, should learn longer chunks as compared to participants in the accurate group. Evaluating the single-element transition probability with <span class="mathjax-tex">\(w = 0\)</span> and 1 (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig2">2</a>d) shows that if only accuracy is the optimization goal of the cost function, then the model preserves the original transition matrix. However, if the model optimizes for speed, then it learns a polarized transition probability where all the high and medium single element transitions attract more probability mass, i.e. are closer to 1. Correspondingly, the remaining probabilities are closer to 0. Thus, as the high and medium transitions are more integrated into the chunks, this gives the model a speed-up in its reaction times, because it can start its evidence accumulation at a higher initial point. This comes at the cost of accuracy, because the initialization may be incorrect.</p></div></div></section><section data-title="Experiment 1: learning about true chunks"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Experiment 1: learning about true chunks</h2><div class="c-article-section__content" id="Sec10-content"><p>In Experiment 1, we test the model’s prediction that chunking behavior adapts to the statistics of the sequence. When chunks are used to generate the sequence, participants should learn more than those trained on sequences without chunks.</p><p>Experiment 1 was conducted using a between-groups designs in which 122 participants were randomly assigned to one of three groups at the beginning of the experiment. These groups were the independent, size 2, and and size 3 conditions. The experiment was comprised of 10 blocks in total. The middle six blocks were the training blocks where participants practised the independent, size 2 or size 3 sequences. The first two and the last two blocks were the baseline and test blocks. In those blocks, all three groups of participants received the same sequence generated from the “illusory” transition matrix. Training blocks differed amongst the three groups, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig1">1</a>, while the baseline and the test blocks remained the same. For the training blocks, the independent group practiced sequences that contained no chunks, the size 2 group practiced sequences with chunk AB, and the size 3 group practiced sequences with chunk ABC, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig1">1</a>d. The sequence for the independent group was randomly and independently sampled from single-item elements A, B, C, and D with equal probability. This means that this sequence contained no chunks. The sequence for the size 2 group was generated by sampling AB, C, and D with an equal probability of 1/3. In other words, this sequence contained the chunk AB. The sequence for the size 3 was generated by sampling ABC and D with an equal probability of 1/2. Thus, this sequence contained the chunk ABC. All three groups received the same instruction to act “as fast and accurately as possible” throughout the experiment. On each trial, participants received feedback on the reaction time and correctness of the previous trial, followed by a 500ms response-to-stimulus interval. Participants were informed that their performance bonus on top of a base-pay was based on a mixture of their reaction times and accuracy. The baseline and test blocks were sequences generated by the illusory transition matrix in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig1">1</a>c. The main prediction was that if people have learned chunks present in the training blocks, then they will use them even in the test blocks. We measured this by examining differences in accuracy and reaction times from the baseline block. We also used Experiment 1 to validate several of our empirical measures of chunking which we will use in Experiment 2.</p><p>We decided to examine model prediction on multifaceted prospects of participants’ chunk learning behavior by proposing and applying various chunk learning measures at distinct stages of the task. Since many aspects of the measure are novel, we conduct these measures in the hope that their results complement each other.</p><p>The first two measures, chunky boost, and chunkiness, evaluate indicators of learning size 2 and size 3 chunks by comparing the performance of the baseline and test blocks. The regression on chunky RT evaluated on the test block examines the transition probability’s influence on reaction time. The last three measures rely on chunks as identified by the mixture of the Gaussian method. They are directly-measured from the chunking profile of the participants. The chunk growth rate evaluates chunk size increase during training. The chunk increase measure shows the quantitative differences between counted chunks between the baseline and the test blocks. The last measure on chunk reuse probability looks for the character of reusing previously learned chunks to construct new chunks, as demonstrated by participants’ chunk learning.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3" data-title="Figure 3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41598-023-31500-3/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-023-31500-3/MediaObjects/41598_2023_31500_Fig3_HTML.png?as=webp"><img aria-describedby="Fig3" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-023-31500-3/MediaObjects/41598_2023_31500_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="507"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Results of Experiment 1. (<b>a</b>) Manipulation check. The number of chunks AB and ABC learned by participants during the training blocks by group. Chunks were retrieved using a categorization of between- and within-chunk transitions by a mixture of Gaussians analysis of participants’ reaction times. (<b>b</b>) Chunky Boost of size 2 chunks AB and BC by group. A chunky boost is measured by the relative change of Cohen’s <i>d</i> between baseline and test blocks for the highly and medium probable transitions. (<b>c</b>) Chunkiness of size-3 chunks ABC. Chunkiness is measured by the relative change of Wasserstein distance between the baseline and test blocks of between-chunk reaction times of all possible size 3 chunks. (<b>d</b>) Regression coefficients of interaction effects between condition and size 2, size 3, and true transition probabilities on reaction times during the final test blocks. (<b>e</b>) Chunk increase from the baseline to the test blocks by group for chunk AB and chunk ABC. Chunk increase is measured by the number of returned chunks from the mixture of Gaussians analysis. (<b>f</b>) Chunk reuse probability by group. Chunk reuse probability was calculated based on whether or not part of an earlier chunk were used in a later chunk that occurred within the next 30 trials. For all plots, error bars indicate the standard error of the mean.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41598-023-31500-3/figures/3" data-track-dest="link:Figure3 Full size image" aria-label="Full size image figure 3" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec11">Manipulation check</h3><p>We first checked if participants’ behavior during the training blocks reflected the underlying chunks in the generative model. In particular, we tested whether the size 2 group showed evidence for learning chunk AB, and the size 3 group learning chunk ABC. We used a Gaussian mixture model to categorize reaction times of each response from the same participant into fast “within” or slow “between” chunk transitions, based on the assumption of a within-chunk speed-up. This method gave us a glimpse into how the action sequence was partitioned by participants, reflecting their internal representation of chunks (more details in Methods). We then counted the number of times chunks AB and ABC showed up in the training block, denoted as <span class="mathjax-tex">\(N_{AB}\)</span> and <span class="mathjax-tex">\(N_{ABC}\)</span>. If the size 2 group and the size 3 group had learned chunk AB and ABC, separately, then <span class="mathjax-tex">\(N_{AB}\)</span> should be higher for these two groups than the independent group, and <span class="mathjax-tex">\(N_{ABC}\)</span> should be higher for size 3 group than the other two. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig3">3</a>a shows the average <span class="mathjax-tex">\(N_{AB}\)</span> and <span class="mathjax-tex">\(N_{ABC}\)</span> returned by this analysis across the three conditions during the training blocks.</p><p>For <span class="mathjax-tex">\(N_{AB}\)</span>, fitting a linear regression model using condition as the independent variable and the number of chunks <span class="mathjax-tex">\(N_{AB}\)</span> as the dependent variable showed a significant effect of condition (<span class="mathjax-tex">\(F(2) = 45.02\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span>). <span class="mathjax-tex">\(N_{AB}\)</span> was higher for both the size 2 group (<span class="mathjax-tex">\(\hat{\beta } = 59.43\)</span>, <span class="mathjax-tex">\(t(139) = 8.20\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span>) and the size 3 group (<span class="mathjax-tex">\(\hat{\beta } = 59.39\)</span>, <span class="mathjax-tex">\(t(139) = 8.32\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span>) than for the independent group. This means that training on sequences that contained either AB or ABC chunks induced participants to learn AB as a chunk.</p><p>To investigate differences in the acquisition of the ABC chunk between groups, we repeated the same regression with <span class="mathjax-tex">\(N_{ABC}\)</span> as the dependent variable. We found a significant effect of groups (<span class="mathjax-tex">\(F(2) = 71.45\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span>), indicating that participants’ responses reflected ABC chunks more often in both the size 2 (<span class="mathjax-tex">\(\hat{\beta } = 9.40\)</span>, <span class="mathjax-tex">\(t(139) = 1.89\)</span>, <span class="mathjax-tex">\(p = 0.06\)</span>) and size 3 (<span class="mathjax-tex">\(\hat{\beta } = 54.17\)</span>, <span class="mathjax-tex">\(t(139) = 11.07\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span>) than in the independent group. Interestingly, we observed higher <span class="mathjax-tex">\(N_{ABC}\)</span> with the size 2 group than in independent group. This can be because building on top of a previously learned chunk (AB <span class="mathjax-tex">\(\rightarrow\)</span> ABC) is more accessible for the size 2 group than the independent group (as the independent group needs to learn chunk AB first, then ABC). Furthermore, <span class="mathjax-tex">\(N_{ABC}\)</span> was significantly higher for the size 3 than the size 2 group (<span class="mathjax-tex">\(\hat{\beta } = 44.76\)</span>, <span class="mathjax-tex">\(t(139) = 9.25\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span>), suggesting that training on sequences that contained ABC chunks resulted in the strongest tendency of participants to learn ABC as a chunk.</p><p>Given these results, we conclude that our experimental manipulation of the three groups induced the intended behavior during the training blocks.</p><h3 class="c-article__sub-heading" id="Sec12">Chunky boost</h3><p>When trained on sequences with underlying chunks ABC and BC, the rational chunking model learns chunk ABC and BC separately. To check this prediction of our model, we look at participants learning of size 2 chunks, i.e. AB and BC, separately. In particular, we look at participants’ reaction time of pressing within chunk items, B in AB and C in BC, and how these items speed up differently across the three groups from the baseline to the test blocks. In SRT tasks, the reaction time difference before and after training is usually used as a sensitive measure of skill<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Willingham, D. B., Nissen, M. J. &amp; Bullemer, P. On the development of procedural knowledge. J. Exp. Psychol. Learn. Mem. Cogn. 15, 1047 (1989)." href="/articles/s41598-023-31500-3#ref-CR25" id="ref-link-section-d7146304e4840">25</a></sup>. If participants’ behavior is consistent with our model’s prediction, then the size 2 and size 3 groups should have a stronger sign of learning chunk AB than the independent group. The size 3 group should have a stronger sign of learning chunk BC than the size 2 group.</p><p>We look at how the training schedule changes the value of within-chunk (value marked by the red boundary) reaction time for AB and BC (since a sign of chunking is that the reaction time of within-chunk items is typically faster than between-chunk items<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Gobet, F. et al. Chunking mechanisms in human learning. Trends Cog. Sci.&#xA;                  https://doi.org/10.1016/S1364-6613(00)01662-4&#xA;                  &#xA;                 (2001)." href="/articles/s41598-023-31500-3#ref-CR8" id="ref-link-section-d7146304e4847">8</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Koch, I. &amp; Hoffmann, J. Patterns, chunks, and hierarchies in serial reaction-time tasks. Psychol. Res.&#xA;                  https://doi.org/10.1007/PL00008165&#xA;                  &#xA;                 (2000)." href="/articles/s41598-023-31500-3#ref-CR12" id="ref-link-section-d7146304e4850">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Verwey, W. et al. Buffer loading and chunking in sequential keypressing. J. Exp. Psychol. 00, 544–562. &#xA;                  https://doi.org/10.1037//0096-1523.22.3.544&#xA;                  &#xA;                 (1996)." href="/articles/s41598-023-31500-3#ref-CR43" id="ref-link-section-d7146304e4853">43</a></sup>); a figurative explanation of this method can be found in Fig. 6 in the appendix. We look at the within-chunk reaction time of AB and BC for all groups at the baseline and the test blocks and compute the difference by the signed effect size, Cohen’s d, of the baseline blocks, compared to the test block <span class="mathjax-tex">\(d_{AB}\)</span>. Cohen’s d is a standardized measure of how far the means of two probability distributions are apart. In this case, these two distributions are the reaction time in the baseline blocks and the reaction time in the test blocks. We used a signed version of Cohen’s d to convey the relative change of the reaction time distributions. <span class="mathjax-tex">\(d_{AB}\)</span> is positive when, on average, the reaction time of B in AB at the test block is faster than the reaction time in the training block – a sign of learning. However, solely looking at AB and BC is not enough, as a general learning factor will speed up participants’ reaction time naturally. Therefore we compared the signed effect size AB and BC with the reaction time speed up of the control chunks. For the controlled between-chunk items, we evaluated the signed Cohen’s d on AA, AB, and AC for chunk AB; and on BA, BB, and BD for chunk BC. Finally, we arrived at the chunky boost measure <span class="mathjax-tex">\(\Delta d\)</span> by subtracting the relative speed-up of AB and BC from their corresponding control chunks. We named this a chunky boost measure. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig3">3</a>b shows the Chunky Boost of AB and BC across the three groups.</p><p>For chunk AB, fitting a linear model onto participants signed Cohen’s d change showed a significant effect of group (<span class="mathjax-tex">\(F (2) = 10.613\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span>); participants in the size 2 group had a higher relative change of Cohen’s d than the independent group (<span class="mathjax-tex">\(\hat{\beta } = 0.41\)</span>, <span class="mathjax-tex">\(t = 4.41\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span>). Thus, training on the chunks with size 2 made the size 2 group respond to B faster after having seen item A. Additionally, participants in the size 3 group also had a higher relative change of reaction times responding to chunk AB than the independent group (<span class="mathjax-tex">\(\hat{\beta } = 0.31\)</span>, <span class="mathjax-tex">\(t = 3.35\)</span>, <span class="mathjax-tex">\(p = 0.001\)</span>), showing that their reaction to B also sped up relative to control. These results are consistent with the model prediction that chunk AB should be acquired by the size 2 and size 3 group, separately.</p><p>For chunk BC, fitting a linear model onto the chunky boost measure <span class="mathjax-tex">\(\Delta d\)</span> on BC with group as the independent variable also showed a significant effect (<span class="mathjax-tex">\(F (2) = 10.802\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span>). Interestingly, the size 2 group had a negative chunky boost to BC (<span class="mathjax-tex">\(\hat{\beta } = -0.23\)</span>, <span class="mathjax-tex">\(t = -2.44\)</span>, <span class="mathjax-tex">\(p = 0.02\)</span>), showing a relative reaction time slow-down compared to control. This effect was expected because identifying B as the end of a chunk will result in the transition to C as a “between-chunk” transition. In other previous SRT experiments, a slow-down in between-chunk reaction times was also observed<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Du, Y. &amp; Clark, J. New insights into statistical learning and chunk learning in implicit sequence acquisition. Psychon. Bull. Rev. 24, 1225–1233 (2017)." href="/articles/s41598-023-31500-3#ref-CR44" id="ref-link-section-d7146304e5318">44</a></sup>. This slow-down can contribute to the negative chunky boost of the size 2 group. Relative to the independent group, the size 3 group had a significantly higher chunky boost <span class="mathjax-tex">\(\Delta d\)</span> (<span class="mathjax-tex">\(\hat{\beta } = 0.20\)</span>, <span class="mathjax-tex">\(t = 2.14\)</span>, <span class="mathjax-tex">\(p = 0.03\)</span>). This shows that learning chunks changes the reaction time profile of this group. Their response to C upon previous instruction B was speeding up their reaction times much more from the training blocks to the test blocks compared to control. This is consistent with the model prediction that the size 3 group should be more likely to learn chunk ABC.</p><p>In summary, participants’ reaction times changed in a predictable fashion, with the independent group not getting faster for either AB or BC, the size 2 group becoming faster for AB and slower for BC, and the size 3 group becoming faster for both AB and BC. These observations confirmed previous work studying chunking in SRT tasks, which has argued that RTs in structured sequences decrease more quickly than in non-structured sequences<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Koch, I. &amp; Hoffmann, J. Patterns, chunks, and hierarchies in serial reaction-time tasks. Psychol. Res.&#xA;                  https://doi.org/10.1007/PL00008165&#xA;                  &#xA;                 (2000)." href="/articles/s41598-023-31500-3#ref-CR12" id="ref-link-section-d7146304e5427">12</a></sup> and are consistent with the predictions from the rational chunking model.</p><h3 class="c-article__sub-heading" id="Sec13">Chunkiness</h3><p>The rational chunking model predicts that the size 2 group should learn more chunks AB, and the size 3 group should learn more chunks ABC, compared to the independent group. To access this prediction, we formulated a measure of chunkiness as an indicator of learning size 3 chunks. If participants have learned a size 3 chunk, such as ABC, then the distributions of within-chunk reaction times (i.e. the reaction time of B and C) should become more similar to each other<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Gobet, F. et al. Chunking mechanisms in human learning. Trends Cog. Sci.&#xA;                  https://doi.org/10.1016/S1364-6613(00)01662-4&#xA;                  &#xA;                 (2001)." href="/articles/s41598-023-31500-3#ref-CR8" id="ref-link-section-d7146304e5439">8</a></sup>. We use the Wasserstein distance to evaluate the homogeneity of reaction time distribution of B and C, <span class="mathjax-tex">\(rt_B\)</span> and <span class="mathjax-tex">\(rt_C\)</span>, following the presentation of A. The Wasserstein distance is also known as the “earth mover’s” distance. It can be seen as the minimum amount of “work” required to transform one distribution into another. “Work” is the amount of distributional weight that must be moved multiplied by the distance (see also Supporting Information). This is simply just measuring how similar the two reaction time distributions are.</p><p>We evaluated the Wasserstein distance between the distribution of <span class="mathjax-tex">\(rt_B\)</span> and <span class="mathjax-tex">\(rt_C\)</span> on the baseline blocks, when all groups of participants are trained on the illusory transition sequences, to arrive at <span class="mathjax-tex">\(Wasserstein(rt_B, rt_C)_{baseline}\)</span>. This assesses the initial separation of the two distributions, how participants learn from the illusory transition sequences, when all groups of participants have not been exposed to any training that involves chunks. Then we evaluate the same Wasserstein distance in the test blocks, also during the illusory transition sequence, to arrive at <span class="mathjax-tex">\(Wasserstein(rt_B, rt_C)_{test}\)</span>, to assess how much training influences <span class="mathjax-tex">\(rt_B\)</span> and <span class="mathjax-tex">\(rt_C\)</span> in the test blocks. If participants have learned ABC as a chunk during the training blocks, then <span class="mathjax-tex">\(rt_B\)</span> and <span class="mathjax-tex">\(rt_C\)</span> should become more homogeneous in the test blocks, resulting in a smaller Wasserstein distance, as compared to the baseline blocks. We subtracted <span class="mathjax-tex">\(Wasserstein(rt_B, rt_C)_{test}\)</span> from <span class="mathjax-tex">\(Wasserstein(rt_B, rt_C)_{baseline}\)</span> to calculate this change of reaction time homogeneity: <span class="mathjax-tex">\(\Delta W_{chunk}\)</span>. This relative change of Wasserstein should be positive if reaction times became more homogeneous in the test blocks. Since training may result in an overall increase of reaction time homogeneity for all items, we compared this change of <span class="mathjax-tex">\(Wasserstein\)</span> with size 3 sub-sequences that were not ABC, as a control. <span class="mathjax-tex">\(\Delta W_{control}\)</span> as the difference between <span class="mathjax-tex">\(W_{test}\)</span> and <span class="mathjax-tex">\(W_{train}\)</span> is evaluated on the control sequences.</p><p>Finally, we subtract <span class="mathjax-tex">\(\Delta W_{control}\)</span> from <span class="mathjax-tex">\(\Delta W_{chunk}\)</span>, to arrive at the resulting measure of “chunkiness”. Chunkiness can be seen as the relative change of the Wasserstein distance of chunk ABC <span class="mathjax-tex">\(\Delta W_{ABC}\)</span> compared to control <span class="mathjax-tex">\(\Delta W_{control}\)</span>. The resulting evaluation of chunkiness on the three groups is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig3">3</a>c. Chunkiness differed significantly between the three conditions (<span class="mathjax-tex">\(F(2) = 3.20\)</span>, <span class="mathjax-tex">\(p = .04\)</span>).</p><p>In particular, the size 2 group had a negative relative Wasserstein shift (<span class="mathjax-tex">\(\hat{\beta } = -26.92\)</span>, <span class="mathjax-tex">\(t(137) = -2.37\)</span>, <span class="mathjax-tex">\(p = 0.02\)</span>). This means that the size 2 group’s reaction time distribution became less homogeneous after training, indicating that the reaction time to press B deviated more from C. This was expected as for the size 2 group, pressing B and C after A should be one within and one between-chunk reaction time. On the other hand, the change of Wasserstein distance between the size 3 and the independent group condition was not significant, even though we would have expected this group to become more homogeneous in their reaction times. One reason for this surprising result could be that, while the reaction time distribution upon the instructions “B” and “C” became closer to each other relative to control, the shift may have not uniformly impacted the calculation of Wasserstein distance. It could also be that positions at the end of a chunk can be learned faster than the intermediate elements, as found in<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Minier, L., Fagot, J. &amp; Rey, A. The temporal dynamics of regularity extraction in non-human primates. Cogn. Sci. 40, 1019–1030. &#xA;                  https://doi.org/10.1111/cogs.12279&#xA;                  &#xA;                 (2016)." href="/articles/s41598-023-31500-3#ref-CR45" id="ref-link-section-d7146304e6413">45</a></sup>.</p><p>In summary, we verified the prediction that the size 2 group had less homogeneous transition times within the chunk ABC than the other groups. However, we did not observe an increased chunkiness for the size 3 group, possibly due to non-uniform speed-ups of RTs.</p><h3 class="c-article__sub-heading" id="Sec14">Reaction time regression</h3><p>The learning of chunks during the training blocks will influence how participants perceive the transition from one instruction to another. As the rational chunking model predicts that participants in the size 3 group will learn chunk ABC, size 2 group will learn chunk AB, and no such chunks in the independent group. This chunk learning will influence how participants perceive the items in the test blocks in a way that size 2 group and size 3 group may react to the sequence in a more deterministic manner. Therefore we studied the influence of transition probabilities on participants’ reaction times during the test blocks (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig3">3</a>d) on the correct trials. We use three transition probability matrices as regressors. One is the true transition (TR), which is the ground truth transition probability used to generate a sequence in the test block. The second one is C2 transition matrix that contains a deterministic transition from A to B. And the third one is C3 transition matrix, with a deterministic transition from A to B, and B to C. The rest of the entries of C2 and C3 are the same as TR.</p><p>We fitted a linear mixed-effects regression using log-reaction times as the dependent variable, assuming a random intercept for each participant. The independent variables were the TR, C2, and C3 transition probabilities, group, as well as interaction effects between group and each of the transition probabilities.</p><p>The best regression contained the predicted transition probabilities as well as three interaction effects with groups (<span class="mathjax-tex">\(\chi ^2(8) = 129.6\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span>). The first interaction was between TR and the size 2 group (<span class="mathjax-tex">\(\hat{\beta } = 0.13\)</span>, <span class="mathjax-tex">\(t(25040) = 5.24\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span>), showing that the effect of TR learned by the size 2 group was significantly up-weighted. The interaction between TR and the size 3 group was also significantly up-weighted (<span class="mathjax-tex">\(\hat{\beta } = 0.10\)</span>, <span class="mathjax-tex">\(t(25040) = 3.84\)</span>, <span class="mathjax-tex">\(p &lt;0.001\)</span>). TR transition probabilities as an independent variable slowed down the reaction times for the size 2 and size 3 groups more than for the independent group.</p><p>The interaction was significantly down-weighted between the C2 chunky transition probabilities and the size 2 group (<span class="mathjax-tex">\(\hat{\beta } = -0.08\)</span>, <span class="mathjax-tex">\(t(25040) = -7.51\)</span>, <span class="mathjax-tex">\(p &lt; .001\)</span>) and the size 3 group (<span class="mathjax-tex">\(\hat{\beta } = -0.03\)</span>, <span class="mathjax-tex">\(t(25040) = -2.68\)</span>, <span class="mathjax-tex">\(p = 0.007\)</span>). This indicates that C2 transition probabilities sped up the reaction times for the size 2 and the size 3 group more than for the independent group, and the effect is stronger for the size 2 group.</p><p>Finally, the interaction was down-weighted between the C3 transition probabilities and the size 3 group (<span class="mathjax-tex">\(\hat{\beta } = -0.12\)</span>, <span class="mathjax-tex">\(t(25040) = -6.27\)</span>, <span class="mathjax-tex">\(p &lt; .001\)</span>), as well as an interaction between the C3 transitions and the size 2 group (<span class="mathjax-tex">\(\hat{\beta } = -0.11\)</span>, <span class="mathjax-tex">\(t(25040) = -5.34\)</span>, <span class="mathjax-tex">\(p &lt;0.001\)</span>). These significant interaction effects indicate that C3 transition probabilities sped up the reaction times for the size 2 and the size 3 group more than that for the independent group. In summary, we found predictable relations between participants’ reaction times and the chunk-implied transition probabilities across groups. In particular, C2 transitions were more significantly related to speed-ups for the size 2 group than the size 3 group, while C3 transitions significantly related to speed-ups for the size 3 group more than size 2 group. This relation is consistent with the prediction generated by the rational chunking model.</p><h3 class="c-article__sub-heading" id="Sec15">Chunk increase</h3><p>The rational model of chunking, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig2">2</a>c, predicts that participants’ learned chunks should reflect the underlying chunks used to generate the sequence. That is, the chunk 3 group should learn more chunk ABC than the chunk 2 group and the independent group. Additionally, both chunk 3 and chunk 2 group should learn more chunk AB than the independent group. This acquisition of chunks during the 6 training blocks is going to influence how participants behave in the baseline and test blocks. We here test this prediction concretely by examining how often chunk AB and chunk ABC are used by the three groups in the test blocks, using baseline blocks as a control.</p><p>We look at the exact chunks used by participants by classifying within and between chunk reaction time using the mixture of Gaussian method (see section method). As an illustration, Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41598-023-31500-3#MOESM1">5</a> shows the participants’ data. The reaction time, instruction displayed, and this participant’s actual key press is shown on the first, second, and third row. Instruction for A, B, C, D, is separately color-coded by green, blue, magenta, and orange boxes. When this participant has pressed a key incorrectly, that trial is marked by a red box. Using the distribution of reaction time data accumulated for this participant over all trials, we classify individual trials into within or between chunk key press, whichever results in a higher likelihood in the mixture. Once each trial is classified as within or between chunk trials, we can mark the chunks learned by this participant by connecting all within-chunk trials and the first between chunk trial that starts before those within-chunk trials as belonging to the same chunk (thereby, we can mark the size of chunks by connecting black round dots shown on the fourth row of the above). In this way, we can identify the content of the chunks learned by this participant as reflected by their reaction time speed-up, in addition to the chunk size learned by each participant, as illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41598-023-31500-3#MOESM1">5</a> in supplementary information.</p><p>We measured the number of times each participant chunked AB and ABC in baseline and test blocks and evaluated the increase <span class="mathjax-tex">\(\Delta N = N_{test} - N_{baseline}\)</span>. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig3">3</a>e shows <span class="mathjax-tex">\(\Delta N_{AB}\)</span> and <span class="mathjax-tex">\(\Delta N_{ABC}\)</span>, measured separately for the three groups.</p><p>Fitting a linear model setting <span class="mathjax-tex">\(\Delta N\)</span> of AB as the dependent variable and group as an independent variable showed a significant effect of group (<span class="mathjax-tex">\(F(2) = 8.64\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span>). Compared to the independent group, the size 2 group (<span class="mathjax-tex">\(\hat{\beta } = 7.16\)</span>, <span class="mathjax-tex">\(t(139) = 3.68\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span>) and the size 3 group (<span class="mathjax-tex">\(\hat{\beta } = 6.80\)</span>, <span class="mathjax-tex">\(t(139) = 3.55\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span>) chunked AB significantly more often in the test blocks than in the baseline blocks.</p><p>The same analysis for <span class="mathjax-tex">\(\Delta N\)</span> of chunk ABC also showed a significant effect of group (<span class="mathjax-tex">\(F(2) = 10.63\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span>). The size 3 group chunked significantly more ABC as chunks than the independent group (<span class="mathjax-tex">\(\hat{\beta } = 5.36\)</span>, <span class="mathjax-tex">\(t(139) = 4.53\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span>). Participants in the size 2 group also chunked more ABC than the participants in the independent group (<span class="mathjax-tex">\(\hat{\beta } = 1.94\)</span>, <span class="mathjax-tex">\(t(139) = 1.62\)</span>, <span class="mathjax-tex">\(p = 0.10\)</span>). Compared with the size 2 group, the size 3 group chunked ABC also significantly more often (<span class="mathjax-tex">\(\hat{\beta } = 3.42\)</span>, <span class="mathjax-tex">\(t(139) = 2.92\)</span>, <span class="mathjax-tex">\(p = 0.004\)</span>). Overall, participants’ behavior is qualitatively consistent with model prediction. Training on sequences with chunks increased participants’ tendency to use those chunks in the test blocks.</p><h3 class="c-article__sub-heading" id="Sec16">Chunk reuse</h3><p>Since the rational chunking model reuses previously learned chunks to construct new ones, we wanted to study whether participants’ chunking behavior reflected this feature of our model. As explained in the method section, the mixture of the Gaussian rt classification method returns the estimated learning progress of chunks for each participant throughout the experiment. We examined participants’ chunk reuse probability based on how individual participants learned the chunks during the training blocks (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig3">3</a>f).</p><p>The chunk reuse probability was evaluated on chunks of size three or bigger (excluding chunks with single-item repetitions). Every time such a chunk occurs in the sequence, we check whether it reuses any of the 30 previous chunks. Figure 8 in SI shows an example of chunk CDABCD reusing BCD as one of its previous chunks. By tagging each chunk learned by the participant as reusing one of the previous chunks or not, we arrived at a chunk reuse probability for each participant. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig3">3</a>f shows the average chunk reuse probability across the three groups.</p><p>We found that the chunk reuse probability differed significantly between the groups. Fitting a linear model taking the reuse probability as the dependent variable and group as the independent variable showed a significant effect of condition (<span class="mathjax-tex">\(F(2) = 13.99\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span>). Both the size 2 (<span class="mathjax-tex">\(\hat{\beta } = 0.17\)</span>, <span class="mathjax-tex">\(t(139) = 3.63\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span>) and the size 3 group (<span class="mathjax-tex">\(\hat{\beta } = 0.24\)</span>, <span class="mathjax-tex">\(t(139) = 5.17\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span>)  reused chunks significantly more often than the independent group. There was no significant difference between the size 2 and the size 3 group (<span class="mathjax-tex">\(\hat{\beta } = 0.07\)</span>, <span class="mathjax-tex">\(t(139) = 1.51\)</span>, <span class="mathjax-tex">\(p = 0.13\)</span>). The tendency to reuse previously learned chunks is consistent with how our model creates chunks, i.e., creating a new chunk by combining previously learned chunks. Interestingly, sequence statistics modulated participants’ tendency to reuse chunks. When the sequence contained embedded chunks that render reuse beneficial to performance, participants tended to reuse previously acquired chunks more often than when the sequence only contained independent item instantiations. The observation that participants reused previously acquired chunks echoes previous findings in the literature on transferring motor skills, which showed that people transfer chunks from a practiced sequence to a test sequence when shared chunks between the two<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Müssgens, D. M. &amp; Ullén, F. Transfer in motor sequence learning: Effects of practice schedule and sequence context. Front. Human Neurosci.&#xA;                  https://doi.org/10.3389/fnhum.2015.00642&#xA;                  &#xA;                 (2015)." href="/articles/s41598-023-31500-3#ref-CR14" id="ref-link-section-d7146304e8139">14</a></sup>. The reuse and transfer process in the current task was an ongoing learning behavior while participants practiced the training sequence.</p></div></div></section><section data-title="Experiment 2: learning chunks of different sizes to balance the speed–accuracy trade-off"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">Experiment 2: learning chunks of different sizes to balance the speed–accuracy trade-off</h2><div class="c-article-section__content" id="Sec17-content"><p>In Experiment 2, we test the model prediction that chunk learning adapts to task demands. Participants should chunk more under time pressure, even given a sequence without chunks within.</p><p>We randomly assigned participants to one of two groups: the fast group and the accurate group, creating a two-groups between-subjects design. Both groups were trained on sequences generated from the “illusory” matrix that contained no true chunks but high and medium single item transitions (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig1">1</a>c). The experiment structure was identical to the structure of Experiment 1: 10 blocks with 100 trials each. The training blocks were sandwiched between baseline and test blocks, see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig1">1</a>a. In those blocks, accuracy and reaction times were displayed right at the end of each trial. In the middle 6 blocks, from block 3 to block 8 (i.e. the training blocks), participants in the fast group were instructed to act “as fast as possible even if it might lead to mistakes”, and participants in the accurate group were instructed to act “as accurate as possible even it might slow you down”. The fast group was told that their reward depended on how fast they pressed the instructed key and were given trial-by-trial feedback on their reaction times. The accurate group was told that their reward depended on their accuracy and were given trial-by-trial feedback on the correctness of their responses. Both groups received the same instruction to act “as fast and accurately” as possible during the baseline and the test blocks (block 1-2 and 9-10, see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig1">1</a>a.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4" data-title="Figure 4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41598-023-31500-3/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-023-31500-3/MediaObjects/41598_2023_31500_Fig4_HTML.png?as=webp"><img aria-describedby="Fig4" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-023-31500-3/MediaObjects/41598_2023_31500_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="618"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Results of Experiment 2. (<b>a</b>) Manipulation check. Average reaction times and average response accuracy during training blocks by group. (<b>b</b>) Chunky Boost of size 2 chunks as measured by change of Cohen’s <i>d</i> by group evaluated on baseline and test blocks. The size 2 chunks include AB, BC, CD, and DA. (<b>c</b>) Chunkiness measured by a relative change of Wasserstein distance of size 3 chunks including ABC, BCD, CDA, DAB between the baseline and the test blocks. (<b>d</b>) Coefficient of interaction effect between chunky and true transition probabilities on reaction times during the test blocks. (<b>e</b>) Chunk increase from the baseline to the test blocks by condition for size-2 (AB, CD, BC, DA) and for size-3 chunks (ABC, BCD, CDA, DAB). (<b>f</b>) Chunk reuse probability by group. For all plots, error bars indicate the standard error of the mean.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41598-023-31500-3/figures/4" data-track-dest="link:Figure4 Full size image" aria-label="Full size image figure 4" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec18">Manipulation check</h3><p>We first assessed whether the instructions to be fast or accurate influenced participants’ reaction times and accuracy during the training blocks. Shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig4">4</a>a are the average reaction time and accuracy for the two groups. Fitting a linear mixed-effects regression onto participants’ reaction times assuming a random intercept over individual participants showed a significant effect of group (<span class="mathjax-tex">\(\chi ^2(1)=9.84\)</span>, <span class="mathjax-tex">\(p=.002\)</span>), showing that participants in the fast group responded faster during the training blocks than participants in the accurate group (<span class="mathjax-tex">\(\hat{\beta }=81.71\)</span>, <span class="mathjax-tex">\(t(113.93)=3.19\)</span>, <span class="mathjax-tex">\(p=.0001\)</span>). We also fitted a mixed-effects logistic regression of group to test whether participants responded correctly on each trial, adding a random intercept for each participant. This analysis also showed a significant effect of group (<span class="mathjax-tex">\(\chi ^2(1)=9.67\)</span>, <span class="mathjax-tex">\(p=.002\)</span>), with participants in the accurate group responding on average more accurately during the training blocks than participants in the fast group (<span class="mathjax-tex">\(\hat{\beta }= 0.54\)</span>, <span class="mathjax-tex">\(z=3.18\)</span>, <span class="mathjax-tex">\(p=.001\)</span>). Thus, we conclude that our experimental manipulation induced the intended behavior for the two groups during the training blocks.</p><h3 class="c-article__sub-heading" id="Sec19">Chunky boost</h3><p>The rational chunking model predicts that the fast group, compared to the accurate group, should learn more chunks. This influence of different instructions will affect the behavioral change of both groups’ performance in the test blocks relative to the baseline blocks. We again look at an indicator of learning size-2 chunks by evaluating chunky boost on the within-chunk reaction times of the size 2 chunks. This time, the chunky boost was evaluated on the most frequently occurring size-2 chunks in the sequence produced by the “illusory” transition matrix: AB, BC, CD, and DA. AB and CD are the size-2 chunks with high transition probability (<span class="mathjax-tex">\(p = 0.9\)</span>). BC and DA are size 2 chunks with medium transition probability (<span class="mathjax-tex">\(p = 0.7\)</span>). The corresponding control chunks were size two subsequences that did not begin with the first chunk items. As an example, the control chunks for AB were BB, CB, and DB.</p><p>We conjectured that the fast group would learn more size two chunks with high and medium probability. We look at how the training schedule changes the value of within-chunk (value marked by red boundary) reaction time for size 2 chunks in the test blocks compared to the baseline blocks. We calculate the signed effect size, Cohen’s d, on the within-chunk reaction time from the baseline to the test block. The same procedure was applied for the control chunks. Then the Cohen’s d of the control chunks was subtracted from the size 2 chunks to arrive at the chunky boost measure. The chunky boost measured by a change of Cohen’s d <span class="mathjax-tex">\(\Delta d\)</span> is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig4">4</a>b. Fitting a linear mixed-effects regression onto participants’ change of Cohen’s d, assuming a random intercept over participants showed a significant effect of the group (<span class="mathjax-tex">\(\chi ^2(1) = 7.25\)</span>, <span class="mathjax-tex">\(p = .007\)</span>). Participants in the fast condition showed a greater relative boost in reaction times to chunky transitions as compared to participants in the accurate group (<span class="mathjax-tex">\(\hat{\beta } = 0.24\)</span>, <span class="mathjax-tex">\(t(73) = 2.71\)</span>, <span class="mathjax-tex">\(p = .008\)</span>). We, therefore, concluded that participants in the fast group chunked more size two chunks than participants in the accurate group, as was predicted by our model.</p><h3 class="c-article__sub-heading" id="Sec20">Chunkiness</h3><p>The rational chunking model that exerts a speed-accuracy trade-off predicts that the fast group should learn longer chunks than the accurate group. Similar to the analysis in experiment 1, we examined this prediction from the model by evaluating the chunkiness measure as an indicator of participants learning size-3 chunks (ABC, BCD, CDA, DAB) that can occur in the training sequence generated by the “illusory” transition matrix. If participants have learned any of those size-3 chunks, then the distributions of within-chunk reaction times <span class="mathjax-tex">\(rt_2\)</span> and <span class="mathjax-tex">\(rt_3\)</span> should become more similar to each other following the presentation of the first item. We use the Wasserstein distance to evaluate the homogeneity of this reaction time distribution, illustrated in Fig. 7.</p><p>To assess the initial separation of the two distributions for both groups before training, we evaluated the Wasserstein distance between the distribution of <span class="mathjax-tex">\(rt_2\)</span> and <span class="mathjax-tex">\(rt_3\)</span> on the baseline blocks, <span class="mathjax-tex">\(Wasserstein(rt_2, rt_3)_{baseline}\)</span>, when both groups of participants are trained on the illusory transition sequences. To assess how much training influences <span class="mathjax-tex">\(rt_2\)</span> and <span class="mathjax-tex">\(rt_3\)</span> in the test blocks, we evaluate the same Wasserstein distance on both groups in the test blocks, to arrive at <span class="mathjax-tex">\(Wasserstein(rt_2, rt_3)_{test}\)</span>. An indicator of learning the frequent size-3 chunk (ABC, BCD, CDA, or DAB) during the training blocks, is that <span class="mathjax-tex">\(Wasserstein(rt_2, rt_3)_{test}\)</span> should become smaller in the test blocks than in the baseline blocks, resulting in more homogeneous <span class="mathjax-tex">\(rt_2\)</span> and <span class="mathjax-tex">\(rt_3\)</span>. We subtracted <span class="mathjax-tex">\(Wasserstein(rt_2, rt_3)_{test}\)</span> from <span class="mathjax-tex">\(Wasserstein(rt_2, rt_3)_{baseline}\)</span> to calculate <span class="mathjax-tex">\(\Delta W_{chunk}\)</span>, the change of reaction time homogeneity.</p><p>To control for the effect of training resulting in an overall increase of reaction time homogeneity, we compared this change of Wasserstein with size 3 sub-sequences that are not the frequent size-3 chunks (ABC, BCD, CDA, DAB), as a control, to arrive at <span class="mathjax-tex">\(\Delta W_{control}\)</span> as the difference between <span class="mathjax-tex">\(W_{test}\)</span> and <span class="mathjax-tex">\(W_{train}\)</span>.</p><p>Finally, we subtracted <span class="mathjax-tex">\(\Delta W_{control}\)</span> from <span class="mathjax-tex">\(\Delta W_{chunk}\)</span>, to arrive at the resulting measure of “chunkiness”. This is the relative change of the Wasserstein distance of size 3 chunks <span class="mathjax-tex">\(\Delta W_{chunk}\)</span> compared to control <span class="mathjax-tex">\(\Delta W_{control}\)</span>. According to the model prediction, if participants in the fast group learned more size three chunks than those in the accurate group, one would expect the fast group to have a higher measure of chunkiness than those in the accurate group. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig4">4</a>c shows the resulting chunkiness measure. The change <span class="mathjax-tex">\(\Delta W\)</span> on size 3 chunks differed significantly between the two groups (<span class="mathjax-tex">\(\chi ^2(1) = 4.71\)</span>, <span class="mathjax-tex">\(p = .02\)</span>), with the fast group showing a higher chunkiness compared to the accurate group (<span class="mathjax-tex">\(\hat{\beta } = 12.33\)</span>, <span class="mathjax-tex">\(t(88) = 2.15\)</span>, <span class="mathjax-tex">\(p = .03\)</span>). Thus, participants in the fast condition showed a higher relative chunkiness in their reaction times to size three chunks in the sequence than participants in the accurate group, as indicated by the chunkiness measure.</p><h3 class="c-article__sub-heading" id="Sec21">Reaction time regression</h3><p>Our model simulations showed that when the speed-accuracy trade-off parameter <span class="mathjax-tex">\(w \rightarrow 0\)</span> and accuracy becomes the only optimizing term, the model learns about the original transition matrix. However, as <span class="mathjax-tex">\(w \rightarrow 1\)</span> and speed is the only optimization term, the model learns a polarized transition probability where all the high and medium single element transitions become 1.</p><p>We aimed to test whether or not the <span class="mathjax-tex">\(w\)</span> parameter of our model captured how the “fast” versus “accurate” instructions affected participants’ chunking behavior. In this way, the instruction will influence how participants perceive the items in the test block, the polarized transition should resemble more of the fast group, and the original transition matrix shall resemble more of the reaction time in the accurate group. An illustration of this procedure is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41598-023-31500-3#MOESM1">6</a> in SI.</p><p>To this end, we fitted a linear mixed-effects regression to participants’ log reaction times in the correct trials of the test blocks (so that the skew of RT distribution as deviating from a normal distribution is removed), assuming a random intercept for each participant. The independent variables are the group, the true transition probabilities learned with <span class="mathjax-tex">\(w = 0\)</span> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig2">2</a>d left), and chunky transition probabilities that correspond to the learning result of the model with <span class="mathjax-tex">\(w = 1\)</span> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig2">2</a>d right). The best regression model contained the main effects of the chunky and true transition probabilities as well as two interaction effects with the given condition (<span class="mathjax-tex">\(\chi ^2(3) = 34.86\)</span>, <span class="mathjax-tex">\(p &lt; 0.001\)</span>). The first interaction was between the true probabilities and group (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig4">4</a>d; <span class="mathjax-tex">\(\hat{\beta } = -0.03\)</span>, <span class="mathjax-tex">\(t(16740) = -2.74\)</span>, <span class="mathjax-tex">\(p = 0.006\)</span>), as the true transition probabilities were more consistent with participants’ responses in the accurate group than with those in the fast group. The second interaction was between the chunky transition probabilities and group (<span class="mathjax-tex">\(\hat{\beta } = 0.05\)</span>, <span class="mathjax-tex">\(t(16740) = 4.13\)</span>, <span class="mathjax-tex">\(p &lt; .001\)</span>), indicating that the simulated effect of a higher tendency to chunk was more predictive of the behavior of the fast group than that of the accurate group. Thus, the chunking bias induced by the speed-accuracy trade-off parameter of our model matched the bias observed in the reaction time pattern of the participants under speed demands.</p><h3 class="c-article__sub-heading" id="Sec22">Chunk increase</h3><p>The rational model of chunking that trades off speed with accuracy learns longer chunks as the emphasis on speed weights more than accuracy. It predicts that the fast group should learn longer chunks more often than the accurate group. This acquisition of longer chunks during the 6 training blocks should influence participants’ chunking behavior in the test blocks relative to the baseline blocks. A concrete examination of this prediction is to take the frequency of concrete size 2 and size 3 chunks in the test block to see if they are used more often compared to the baseline blocks.</p><p>Similar to experiment 1, the exact chunks learned by participants are tagged using the mixture of Gaussian method. We studied the number of times size 2 chunks appear in participants’ chunking profiles (AB, BC, CD, DA) and size 3 chunks (chunk ABC, BCD, CDA, DAB). We compared the increase in those chunks from the baseline and the test blocks and compared ﻿<span class="mathjax-tex">\(\Delta N\)</span> across the two groups. Shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig4">4</a>e is the increase in the number of size 2 and size 3 chunks.</p><p>Fitting a linear model on <span class="mathjax-tex">\(\Delta N\)</span> with group as the independent variable revealed a significant effect of group (<span class="mathjax-tex">\(F(1) = 8.13\)</span>, <span class="mathjax-tex">\(p = 0.005\)</span>). Compared to the accurate group, the fast group acquired more chunks from the baseline to the test block (<span class="mathjax-tex">\(\hat{\beta } = 6.41\)</span>, <span class="mathjax-tex">\(t(358) = 2.85\)</span>, <span class="mathjax-tex">\(p = 0.005\)</span>). Consistent with the model prediction that the fast group should chunk longer chunks, participants in the fast group indeed has a higher increase in the number of size 2 and size 3 chunks compared to the accurate group.</p><h3 class="c-article__sub-heading" id="Sec23">Chunk reuse</h3><p>We also look at whether participants tend to reuse previously learned chunks to construct new chunks during the training blocks, similar to what we tried in experiment 1, to see whether the participant’s behavior reflects the feature of this model. The progress of chunk learning as identified by the mixture of Gaussian method is used to examine participants’ chunk reuse probability (illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/s41598-023-31500-3#MOESM1">8</a> in SI). The chunk reuse probability was evaluated on chunks of size three or bigger (excluding chunks with single-item repetitions). Every time such a chunk occurs in the sequence, we check whether it reuses any of the 30 previous chunks. By tagging each chunk learned by the participant as reusing one of the previous chunks or not, we arrived at a chunk reuse probability for each participant across the fast and accurate group. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-023-31500-3#Fig4">4</a>f shows the average chunk reuse probability across the three groups. Participants’ high reuse probability echoes the model feature of reusing previously learned chunks to construct new ones. Interestingly, instruction also influences the tendency of chunk reuse. Fitting a linear model to participants’ chunk reuse probability showed that chunk reuse differed significantly between the two groups (<span class="mathjax-tex">\(F(1) = 4.75\)</span>, <span class="mathjax-tex">\(p = .03\)</span>). Participants in the fast group reuse chunks more frequently than those in the accurate group (<span class="mathjax-tex">\(\hat{\beta } = 0.07\)</span>, <span class="mathjax-tex">\(t(114) = 2.18\)</span>, <span class="mathjax-tex">\(p = .03\)</span>). This may show that reuse is especially prominent when participants are trying to be fast, since recycling the previously learned chunks can make more progress towards reaction time speed-up.</p></div></div></section><section data-title="Discussion"><div class="c-article-section" id="Sec24-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec24">Discussion</h2><div class="c-article-section__content" id="Sec24-content"><p>How people perceive and extract structure from a sequence of perceptual stimuli has been a longstanding question of psychological investigations. <i>Chunking</i> has been proposed as a mechanism to identify repeated patterns and segment sequences into those patterns. This way of segregating patterns into discrete chunks can improve storage, retrieval, and planning across multiple psychological domains.</p><p>In the current work, we have proposed that chunking benefits the timely and accurate execution of sequential actions. We used a rational model of chunking that adapts its representation to optimize a trade-off between speed and accuracy to simulate chunk learning in a serial reaction time task. Our simulations predicted that participants should chunk more if chunks are indeed part of the generative model and should, on average, learn longer chunks when optimizing for speed than accuracy. We tested these predictions in two experiments. In Experiment 1, participants learned from sequences with different embedded chunks. In Experiment 2, participants were instructed to act as fast or accurately as possible. Multiple measures of chunking confirmed our model’s predictions in both experiments. In summary, our results shed new light on the benefits of chunking and pave the way for future studies on step-wise representation learning in structured domains.
</p><p>The model’s prediction relating chunking to reaction time speed up relied partially on the Linear Ballistic Accumulator framework to translate within-chunk action prediction to an elevated starting point of the evidence accumulation, making the within-chunk action more likely to cross the decision threshold. Yet it remains challenging to explicitly fit a hierarchical LBA model over all participants, trials, and between-subject differences using our current data. This divergence is potentially due to a large number of observations. Therefore, one part of our analyses used model-predicted transition probabilities with accuracy and speed extremes to fit participants’ reaction times. Nonetheless, future studies should look into the influence of chunking on the starting point of the LBA model in a fully Bayesian and hierarchically-structured model.</p><p>Currently, our model’s predictions were primarily qualitative, and we did not compare across a more extensive set of alternative models. Even though we tested model-specific predictions such as the reuse of previously created chunks to parse the sequence and the speed-up and increased homogeneity of reaction times for within-chunk reaction times, future studies should further compare explicit predictions of different chunking models. We believe that our current work is a concrete first step towards building fine-grained models of human chunking in SRTs. We plan to compare our model to several alternatives in future tasks requiring participants to learn increasingly more hierarchically-structured chunks.</p><p>Furthermore, it would be very hard to exclude the contribution of associative learning to the effect observed in Experiment 1, as the rational chunk learning model also learns chunks by association. However, an associative learning model does not explain our observation in Experiment 2, which can only be accounted for by a rational chunking model that trades off speed with accuracy.</p><p>Finally, not all of our measures of increased chunking provided evidence for our model’s predictions. In particular, in Experiment 1, the measure of chunkiness did not increase for the size 3 group even though we would have a priori expected such an increase. We believe that this increase did not appear because participants’ speed-up of within-chunk reaction times was not uniform across both transitions of the size three chunks. Moreover, we did find a decrease of homogeneity for the size 2 group, which was as expected because learning about the size 2 chunk should make the reaction time discrepancy between B and C larger. Importantly, we did find systematic differences across all other measures in both experiments and, therefore, believe that the current data support our model’s predictions.</p></div></div></section><section data-title="Conclusion"><div class="c-article-section" id="Sec25-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec25">Conclusion</h2><div class="c-article-section__content" id="Sec25-content"><p>We investigated chunking behavior across two experiments and several measures. We found that chunking behavior depends on sequence statistics and task demands. When there are chunks in the training sequence, participants learn the underlying embedded chunks. Additionally, task demands modulate chunking behavior. Participants tend to chunk more when they are optimizing for speed rather than accuracy. Such chunking behavior occurs even in sequences lacking any deterministic transition probabilities. Our results suggest characteristics of chunking and how they interact with task demands. Our rational model of chunking captures and predicts these findings. The success of model predictions depends primarily on the gradual change in previously acquired representations to rationally adapt to sequence structure and task demands. We hope that our findings and model are a good step towards understanding human chunk learning across multiple domains.</p></div></div></section><section data-title="Methods"><div class="c-article-section" id="Sec26-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec26">Methods</h2><div class="c-article-section__content" id="Sec26-content"><h3 class="c-article__sub-heading" id="Sec27">Ethics statement</h3><p>Informed consent was obtained from all participants before participation, and the experiments were performed in accordance with the relevant guidelines and regulations approved by the ethic committee of the University of Tuebingen (Ethik-Kommission an der Medizinischen Fakultät der Eberhard-Karls-Universität und am Universitätsklinikum Tübingen), under the study title: Experimente zum Sequenz- und Belohnungslernen, with application number 701/2020BO.</p><p>Participants’ data were analyzed anonymously. Upon agreement to participate in the study, they consented on a data protection sheet approved by the data protection officer of the MPG (Datenschutzbeauftragte der MPG, Max-Planck-Gesellschaft zur Förderung der Wissenschaften).</p><h3 class="c-article__sub-heading" id="Sec28">Recruitment of participants</h3><p>For Experiment 1, we recruited 142 participants from Amazon Mechanical Turk, out of which sixty-nine were female. Their median age was between 30 and 40, and the overall age ranged from 18 to above 50. This experiment took around 25 minutes to complete. After completing the task, participants received a base pay of $2 and a performance-dependent bonus of up to $6.</p><p>For Experiment 2, we recruited a total of 116 participants for our study, again from Amazon Mechanical Turk. Forty-eight participants were female; participants’ median age was between 30 and 40, and the overall age ranged from 18 to above 50. After completing the task, participants received a base pay of $2 and a performance-dependent bonus of up to $4.</p><h3 class="c-article__sub-heading" id="Sec29">Payment</h3><p>For Experiment 1, a performance-dependent bonus was calculated as the weighted sum of participants’ accuracy and reaction times. When the average accuracy was below 70%, the bonus was set to 0. The bonus for being fast was calculated as <span class="mathjax-tex">\(bonusfast = bonusmax - (\overline{rt} - 600) \times 0.025\)</span>, where <span class="mathjax-tex">\(\overline{rt}\)</span> indicates the average reaction time and <i>bonusmax</i> indicates the maximal bonus. Participants were rewarded with a reaction time bonus when their average reaction time was below 600ms. Additionally, an accuracy bonus was calculated as the mean performance accuracy times the maximal bonus, <span class="mathjax-tex">\(bonusacc = \overline{acc} \times bonusmax\)</span>. At the end of the experiment, the total bonus was calculated as a weighted average between the bonus for participants’ accuracy and the bonus for their reaction times, <span class="mathjax-tex">\(bonus = 0.5 \times bonusacc + 0.5 \times bonusfast\)</span>. If the final bonus was below 0, it was set to 0. If it was above the maximal bonus, it was set to the maximal bonus. On average, participants received $5.64 for their participation.</p><p>For Experiment 2, USD 2 were awarded as a base pay for every participant who completed the experiment. Additionally, participants received a performance-dependent bonus, ranging from 0 to the maximum of USD 4. This bonus was calculated separately for the fast and accurate groups. For the fast group, the bonus was 0 when their average accuracy was below 60%. If the average accuracy was above 60%, then the bonus was calculated as <span class="mathjax-tex">\(bonusfast = (1000 - \overline{rt})/800 \times bonusmax\)</span> This reward function penalized average reaction times that were slower than 1000ms. For the accurate group, the bonus was calculated as the percentage of their accuracy multiplied by the maximal bonus for the accuracy group, <span class="mathjax-tex">\(bonusacc = \overline{acc} \times bonusmax\)</span>. Finally, the final bonus was again forcet to be between 0 and <i>bonusmax</i> (USD 4). The mean reward earned by participants for this experiment was $4.16.</p><h3 class="c-article__sub-heading" id="Sec30">Filtering criteria</h3><p>We decided to discard participants using a fixed RT threshold based on independent pilot data we had collected earlier. Since the study was conducted on MTurk, and we do not have access to the conditions on how the experiment was conducted, we applied stricter filtering criteria. For Experiment 1, we excluded participants with an average reaction time longer than 1000ms or an average accuracy lower than 90%. 95.1% of participants had an accuracy above 90%. 91% of participants had an average reaction time below 1000 ms. Out of 142 participants who participated in Experiment 1, 20 participants were excluded and 122 remained after applying this exclusion criterion.</p><p>For Experiment 2, the same exclusion criteria were applied on the baseline and test blocks when both groups were asked to be as fast and accurate as possible. 96.7% of participants had an accuracy above 90%, and 96.7% had an average reaction time below 1000 ms. The exclusion criteria differed between the two groups on the training blocks. Participants in the fast group were excluded when the average reaction time was above 750ms (<span class="mathjax-tex">\(n=13\)</span>). Those in the accurate group were excluded when their accuracy was below 90% (<span class="mathjax-tex">\(n=10\)</span>). Additionally, we excluded participants who repeatedly failed attention checks before and after the experiment (<span class="mathjax-tex">\(n=3\)</span>). Out of 116 participants, 26 were excluded in total. All of the following analyses were performed on the data of the remaining 90 participants.</p><p>For the reaction-time based analysis including the chunky boost, chunkiness, and the mixture of Gaussians classification, we further excluded trials in which participants took more than 1000ms to respond. This amounted to <span class="mathjax-tex">\(8.4\%\)</span> of all trials in Experiment 1, and <span class="mathjax-tex">\(3.3\%\)</span> of all trials in Experiment 2.</p><h3 class="c-article__sub-heading" id="Sec31">Mixture of Gaussians model</h3><p>We used a mixture of Gaussians model to retrieve chunky transitions for each participant’s responses from their reaction times. Chunks are classified based on participants’ responses, irrespective of their correctness. In the case of an error where a participant has pressed A B D C although the instruction was A B D B, and the reaction time classification for each of these trials are between, within, within, within (and the subsequent trial is between again), then A B D C is classified as a chunk. In the case of errors, we consider their erroneous response rather than the instruction because the response reflect their underlying prediction.</p><p>The reaction time distribution for individual participants was used to classify individual trials as between or within-chunk reaction times. These reaction time distributions were fitted by a mixture of Gaussians model. The likelihood of belonging to the smallest mixture component was used to classify a reaction time as a within-chunk reaction time. This classification was then used for the identification of chunks for all experimental trials of every participant.</p><p>The classification of RTs by using multi-modal distributions was motivated by the idea that distinct processes might generate the within-chunk and between-chunk reaction times. During an SRT trial, if the participant has no expectation for the next upcoming instruction, she will first have to identify the instructed key on the screen before beginning to press a key. This will make her between-chunk reaction times larger. In contrast, if a participant has learned chunks, she anticipates the next instruction before it is even shown. If the upcoming instruction is within her expected chunk, the action to look for instructions displayed on the screen can be omitted, and she can directly engage in pressing their expected subsequent key. This will make her within-chunk reaction times smaller. Additionally, the mixture of Gaussians model also takes into account participants’ post-error slow-downs. These correspond to the trials when a participant has made or almost made a mistake and corrects this tendency to press an expected key upon the observation of a conflicting instruction. The behavior of modifying the wrong key-press is slowing down the reaction time even more.</p><p>Because these three processes contribute to distinct components of participants’ reaction times, a mixture of 3 Gaussian distributions was used to fit their reaction time distributions. We assumed that the within-chunk reaction time distributions had the lowest mean, the between-chunk reaction time distributions had a higher mean, and the post-error slow-down reaction time distribution had the highest mean. We fitted the mixture of Gaussian model to individual participants’ reaction times, filtering out RTs above 1000ms. A likelihood estimate belonging to each distribution amongst the mixture was assigned to the reaction times of each trial. A validation of this method has been included to the Supplementary Information.</p></div></div></section>
                    
                </div>
            

            <div class="u-mt-32">
                <section data-title="Data and code availability"><div class="c-article-section" id="data-and-code-availability-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="data-and-code-availability">Data and code availability</h2><div class="c-article-section__content" id="data-and-code-availability-content">
              
              <p>The data collected and code used for analyzing this study can be found in this github repository: <a href="https://github.com/swu32/experimental_chunking">https://github.com/swu32/experimental_chunking</a>.</p>
            </div></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references" data-track-component="outbound reference" data-track-context="references section"><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="1."><p class="c-article-references__text" id="ref-CR1">Miller, G. A. The magical number seven, plus or minus two: Some limits on our capacity for processing information. <i>Psychol. Rev.</i><a href="https://doi.org/10.1037/h0043158" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1037/h0043158">https://doi.org/10.1037/h0043158</a> (1956).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1037/h0043158" data-track-item_id="10.1037/h0043158" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1037%2Fh0043158" aria-label="Article reference 1" data-doi="10.1037/h0043158">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=13310704" aria-label="PubMed reference 1">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20magical%20number%20seven%2C%20plus%20or%20minus%20two%3A%20Some%20limits%20on%20our%20capacity%20for%20processing%20information&amp;journal=Psychol.%20Rev.&amp;doi=10.1037%2Fh0043158&amp;publication_year=1956&amp;author=Miller%2CGA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="2."><p class="c-article-references__text" id="ref-CR2">Laird, J. E., Rosenbloom, P. S. &amp; Newell, A. Towards chunking as a general learning mechanism. In AAAI, 188–192 (1984).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="3."><p class="c-article-references__text" id="ref-CR3">Graybiel, A. M. The basal ganglia and chunking of action repertoires. <i>Neurobiol. Learn. Mem.</i> <b>70</b>, 119–136 (1998).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1006/nlme.1998.3843" data-track-item_id="10.1006/nlme.1998.3843" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1006%2Fnlme.1998.3843" aria-label="Article reference 3" data-doi="10.1006/nlme.1998.3843">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaK1cvivVKrtA%3D%3D" aria-label="CAS reference 3">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=9753592" aria-label="PubMed reference 3">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20basal%20ganglia%20and%20chunking%20of%20action%20repertoires&amp;journal=Neurobiol.%20Learn.%20Mem.&amp;doi=10.1006%2Fnlme.1998.3843&amp;volume=70&amp;pages=119-136&amp;publication_year=1998&amp;author=Graybiel%2CAM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="4."><p class="c-article-references__text" id="ref-CR4">Servan-Schreiber, E. &amp; Anderson, J. R. Learning artificial grammars with competitive chunking. <i>J. Exp. Psychol. Learn. Mem. Cogn.</i> <b>16</b>, 592 (1990).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1037/0278-7393.16.4.592" data-track-item_id="10.1037/0278-7393.16.4.592" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1037%2F0278-7393.16.4.592" aria-label="Article reference 4" data-doi="10.1037/0278-7393.16.4.592">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20artificial%20grammars%20with%20competitive%20chunking&amp;journal=J.%20Exp.%20Psychol.%20Learn.%20Mem.%20Cogn.&amp;doi=10.1037%2F0278-7393.16.4.592&amp;volume=16&amp;publication_year=1990&amp;author=Servan-Schreiber%2CE&amp;author=Anderson%2CJR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="5."><p class="c-article-references__text" id="ref-CR5">Terrace, H. S. Chunking by a pigeon in a serial learning task. <i>Nature</i><a href="https://doi.org/10.1038/325149a0" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1038/325149a0">https://doi.org/10.1038/325149a0</a> (1987).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/325149a0" data-track-item_id="10.1038/325149a0" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2F325149a0" aria-label="Article reference 5" data-doi="10.1038/325149a0">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=3808071" aria-label="PubMed reference 5">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=Chunking%20by%20a%20pigeon%20in%20a%20serial%20learning%20task&amp;journal=Nature&amp;doi=10.1038%2F325149a0&amp;publication_year=1987&amp;author=Terrace%2CHS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="6."><p class="c-article-references__text" id="ref-CR6">Mathy, F. &amp; Feldman, J. What’s magic about magic numbers? Chunking and data compression in short-term memory. <i>Cognition</i><a href="https://doi.org/10.1016/j.cognition.2011.11.003" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1016/j.cognition.2011.11.003">https://doi.org/10.1016/j.cognition.2011.11.003</a> (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.cognition.2011.11.003" data-track-item_id="10.1016/j.cognition.2011.11.003" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.cognition.2011.11.003" aria-label="Article reference 6" data-doi="10.1016/j.cognition.2011.11.003">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22176752" aria-label="PubMed reference 6">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=What%E2%80%99s%20magic%20about%20magic%20numbers%3F%20Chunking%20and%20data%20compression%20in%20short-term%20memory&amp;journal=Cognition&amp;doi=10.1016%2Fj.cognition.2011.11.003&amp;publication_year=2012&amp;author=Mathy%2CF&amp;author=Feldman%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="7."><p class="c-article-references__text" id="ref-CR7">Lashley, K. S. <i>The Problem of Serial Order in Behavior</i> Vol. 21 (Bobbs-Merrill Oxford, United Kingdom, 1951).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 7" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20Problem%20of%20Serial%20Order%20in%20Behavior&amp;publication_year=1951&amp;author=Lashley%2CKS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="8."><p class="c-article-references__text" id="ref-CR8">Gobet, F. <i>et al.</i> Chunking mechanisms in human learning. <i>Trends Cog. Sci.</i><a href="https://doi.org/10.1016/S1364-6613(00)01662-4" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1016/S1364-6613(00)01662-4">https://doi.org/10.1016/S1364-6613(00)01662-4</a> (2001).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/S1364-6613(00)01662-4" data-track-item_id="10.1016/S1364-6613(00)01662-4" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2FS1364-6613%2800%2901662-4" aria-label="Article reference 8" data-doi="10.1016/S1364-6613(00)01662-4">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 8" href="http://scholar.google.com/scholar_lookup?&amp;title=Chunking%20mechanisms%20in%20human%20learning&amp;journal=Trends%20Cog.%20Sci.&amp;doi=10.1016%2FS1364-6613%2800%2901662-4&amp;publication_year=2001&amp;author=Gobet%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="9."><p class="c-article-references__text" id="ref-CR9">Graybiel, A. M. The basal ganglia and chunking of action repertoires. <i>Neurobiol. Learn. Memory</i> <b>70</b>, 1–2. <a href="https://doi.org/10.1006/nlme.1998.3843" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1006/nlme.1998.3843">https://doi.org/10.1006/nlme.1998.3843</a> (1998).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1006/nlme.1998.3843" data-track-item_id="10.1006/nlme.1998.3843" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1006%2Fnlme.1998.3843" aria-label="Article reference 9" data-doi="10.1006/nlme.1998.3843">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 9" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20basal%20ganglia%20and%20chunking%20of%20action%20repertoires&amp;journal=Neurobiol.%20Learn.%20Memory&amp;doi=10.1006%2Fnlme.1998.3843&amp;volume=70&amp;pages=1-2&amp;publication_year=1998&amp;author=Graybiel%2CAM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="10."><p class="c-article-references__text" id="ref-CR10">Egan, D. E. &amp; Schwartz, B. J. Chunking in recall of symbolic drawings. <i>Memory Cogn.</i><a href="https://doi.org/10.3758/BF03197595" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.3758/BF03197595">https://doi.org/10.3758/BF03197595</a> (1979).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.3758/BF03197595" data-track-item_id="10.3758/BF03197595" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.3758%2FBF03197595" aria-label="Article reference 10" data-doi="10.3758/BF03197595">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Chunking%20in%20recall%20of%20symbolic%20drawings&amp;journal=Memory%20Cogn.&amp;doi=10.3758%2FBF03197595&amp;publication_year=1979&amp;author=Egan%2CDE&amp;author=Schwartz%2CBJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="11."><p class="c-article-references__text" id="ref-CR11">Ellis, N. C. Sequencing in SLA: Phonological memory, chunking, and points of order. <i>Stud. Second Lang. Acquis.</i><a href="https://doi.org/10.1017/S0272263100014698" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1017/S0272263100014698">https://doi.org/10.1017/S0272263100014698</a> (1996).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1017/S0272263100014698" data-track-item_id="10.1017/S0272263100014698" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1017%2FS0272263100014698" aria-label="Article reference 11" data-doi="10.1017/S0272263100014698">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=Sequencing%20in%20SLA%3A%20Phonological%20memory%2C%20chunking%2C%20and%20points%20of%20order&amp;journal=Stud.%20Second%20Lang.%20Acquis.&amp;doi=10.1017%2FS0272263100014698&amp;publication_year=1996&amp;author=Ellis%2CNC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="12."><p class="c-article-references__text" id="ref-CR12">Koch, I. &amp; Hoffmann, J. Patterns, chunks, and hierarchies in serial reaction-time tasks. <i>Psychol. Res.</i><a href="https://doi.org/10.1007/PL00008165" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1007/PL00008165">https://doi.org/10.1007/PL00008165</a> (2000).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/PL00008165" data-track-item_id="10.1007/PL00008165" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/PL00008165" aria-label="Article reference 12" data-doi="10.1007/PL00008165">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=10743384" aria-label="PubMed reference 12">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=Patterns%2C%20chunks%2C%20and%20hierarchies%20in%20serial%20reaction-time%20tasks&amp;journal=Psychol.%20Res.&amp;doi=10.1007%2FPL00008165&amp;publication_year=2000&amp;author=Koch%2CI&amp;author=Hoffmann%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="13."><p class="c-article-references__text" id="ref-CR13">Brady, T. F., Konkle, T. &amp; Alvarez, G. A. Compression in visual working memory: Using statistical regularities to form more efficient memory representations. <i>J. Exp. Psychol.: General</i><a href="https://doi.org/10.1037/a0016797" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1037/a0016797">https://doi.org/10.1037/a0016797</a> (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1037/a0016797" data-track-item_id="10.1037/a0016797" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1037%2Fa0016797" aria-label="Article reference 13" data-doi="10.1037/a0016797">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=Compression%20in%20visual%20working%20memory%3A%20Using%20statistical%20regularities%20to%20form%20more%20efficient%20memory%20representations&amp;journal=J.%20Exp.%20Psychol.%3A%20General&amp;doi=10.1037%2Fa0016797&amp;publication_year=2009&amp;author=Brady%2CTF&amp;author=Konkle%2CT&amp;author=Alvarez%2CGA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="14."><p class="c-article-references__text" id="ref-CR14">Müssgens, D. M. &amp; Ullén, F. Transfer in motor sequence learning: Effects of practice schedule and sequence context. <i>Front. Human Neurosci.</i><a href="https://doi.org/10.3389/fnhum.2015.00642" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.3389/fnhum.2015.00642">https://doi.org/10.3389/fnhum.2015.00642</a> (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.3389/fnhum.2015.00642" data-track-item_id="10.3389/fnhum.2015.00642" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.3389%2Ffnhum.2015.00642" aria-label="Article reference 14" data-doi="10.3389/fnhum.2015.00642">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 14" href="http://scholar.google.com/scholar_lookup?&amp;title=Transfer%20in%20motor%20sequence%20learning%3A%20Effects%20of%20practice%20schedule%20and%20sequence%20context&amp;journal=Front.%20Human%20Neurosci.&amp;doi=10.3389%2Ffnhum.2015.00642&amp;publication_year=2015&amp;author=M%C3%BCssgens%2CDM&amp;author=Ull%C3%A9n%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="15."><p class="c-article-references__text" id="ref-CR15">Chase, W. G. &amp; Simon, H. A. Perception in chess. <i>Cogn. Psychol.</i><a href="https://doi.org/10.1016/0010-0285(73)90004-2" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1016/0010-0285(73)90004-2">https://doi.org/10.1016/0010-0285(73)90004-2</a> (1973).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/0010-0285(73)90004-2" data-track-item_id="10.1016/0010-0285(73)90004-2" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2F0010-0285%2873%2990004-2" aria-label="Article reference 15" data-doi="10.1016/0010-0285(73)90004-2">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 15" href="http://scholar.google.com/scholar_lookup?&amp;title=Perception%20in%20chess&amp;journal=Cogn.%20Psychol.&amp;doi=10.1016%2F0010-0285%2873%2990004-2&amp;publication_year=1973&amp;author=Chase%2CWG&amp;author=Simon%2CHA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="16."><p class="c-article-references__text" id="ref-CR16">Gobet, F. &amp; Simon, H. A. Expert chess memory: Revisiting the chunking hypothesis. <i>Memory</i><a href="https://doi.org/10.1080/741942359" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1080/741942359">https://doi.org/10.1080/741942359</a> (1998).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1080/741942359" data-track-item_id="10.1080/741942359" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1080%2F741942359" aria-label="Article reference 16" data-doi="10.1080/741942359">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=9709441" aria-label="PubMed reference 16">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 16" href="http://scholar.google.com/scholar_lookup?&amp;title=Expert%20chess%20memory%3A%20Revisiting%20the%20chunking%20hypothesis&amp;journal=Memory&amp;doi=10.1080%2F741942359&amp;publication_year=1998&amp;author=Gobet%2CF&amp;author=Simon%2CHA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="17."><p class="c-article-references__text" id="ref-CR17">Schulz, E., Tenenbaum, J. B., Duvenaud, D., Speekenbrink, M. &amp; Gershman, S. J. Compositional inductive biases in function learning. <i>Cogn. Psychol.</i><a href="https://doi.org/10.1016/j.cogpsych.2017.11.002" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1016/j.cogpsych.2017.11.002">https://doi.org/10.1016/j.cogpsych.2017.11.002</a> (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.cogpsych.2017.11.002" data-track-item_id="10.1016/j.cogpsych.2017.11.002" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.cogpsych.2017.11.002" aria-label="Article reference 17" data-doi="10.1016/j.cogpsych.2017.11.002">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29154187" aria-label="PubMed reference 17">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 17" href="http://scholar.google.com/scholar_lookup?&amp;title=Compositional%20inductive%20biases%20in%20function%20learning&amp;journal=Cogn.%20Psychol.&amp;doi=10.1016%2Fj.cogpsych.2017.11.002&amp;publication_year=2017&amp;author=Schulz%2CE&amp;author=Tenenbaum%2CJB&amp;author=Duvenaud%2CD&amp;author=Speekenbrink%2CM&amp;author=Gershman%2CSJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="18."><p class="c-article-references__text" id="ref-CR18">Schulz, E., Quiroga, F. &amp; Gershman, S. J. Communicating compositional patterns. <i>Open. Mind</i> <b>4</b>, 25–39 (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1162/opmi_a_00032" data-track-item_id="10.1162/opmi_a_00032" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1162%2Fopmi_a_00032" aria-label="Article reference 18" data-doi="10.1162/opmi_a_00032">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34485791" aria-label="PubMed reference 18">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8412198" aria-label="PubMed Central reference 18">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 18" href="http://scholar.google.com/scholar_lookup?&amp;title=Communicating%20compositional%20patterns&amp;journal=Open.%20Mind&amp;doi=10.1162%2Fopmi_a_00032&amp;volume=4&amp;pages=25-39&amp;publication_year=2020&amp;author=Schulz%2CE&amp;author=Quiroga%2CF&amp;author=Gershman%2CSJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="19."><p class="c-article-references__text" id="ref-CR19">Tomov, M. S., Yagati, S., Kumar, A., Yang, W. &amp; Gershman, S. J. Discovery of hierarchical representations for efficient planning. <i>PLoS Comput. Biol.</i><a href="https://doi.org/10.1371/journal.pcbi.1007594" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1371/journal.pcbi.1007594">https://doi.org/10.1371/journal.pcbi.1007594</a> (2020).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1371/journal.pcbi.1007594" data-track-item_id="10.1371/journal.pcbi.1007594" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1371%2Fjournal.pcbi.1007594" aria-label="Article reference 19" data-doi="10.1371/journal.pcbi.1007594">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32251444" aria-label="PubMed reference 19">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7162548" aria-label="PubMed Central reference 19">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 19" href="http://scholar.google.com/scholar_lookup?&amp;title=Discovery%20of%20hierarchical%20representations%20for%20efficient%20planning&amp;journal=PLoS%20Comput.%20Biol.&amp;doi=10.1371%2Fjournal.pcbi.1007594&amp;publication_year=2020&amp;author=Tomov%2CMS&amp;author=Yagati%2CS&amp;author=Kumar%2CA&amp;author=Yang%2CW&amp;author=Gershman%2CSJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="20."><p class="c-article-references__text" id="ref-CR20">Wickelgren, W. A. Speed-accuracy tradeoff and information processing dynamics. <i>Acta Physiol. (Oxf)</i> <b>41</b>, 67–85. <a href="https://doi.org/10.1016/0001-6918(77)90012-9" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1016/0001-6918(77)90012-9">https://doi.org/10.1016/0001-6918(77)90012-9</a> (1977).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/0001-6918(77)90012-9" data-track-item_id="10.1016/0001-6918(77)90012-9" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2F0001-6918%2877%2990012-9" aria-label="Article reference 20" data-doi="10.1016/0001-6918(77)90012-9">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 20" href="http://scholar.google.com/scholar_lookup?&amp;title=Speed-accuracy%20tradeoff%20and%20information%20processing%20dynamics&amp;journal=Acta%20Physiol.%20%28Oxf%29&amp;doi=10.1016%2F0001-6918%2877%2990012-9&amp;volume=41&amp;pages=67-85&amp;publication_year=1977&amp;author=Wickelgren%2CWA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="21."><p class="c-article-references__text" id="ref-CR21">Bogacz, R., Hu, P. T., Holmes, P. J. &amp; Cohen, J. D. Do humans produce the speed-accuracy trade-off that maximizes reward rate?. <i>Q. J. Exp. Psychol.</i> <b>63</b>, 863–891. <a href="https://doi.org/10.1080/17470210903091643" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1080/17470210903091643">https://doi.org/10.1080/17470210903091643</a> (2010).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1080/17470210903091643" data-track-item_id="10.1080/17470210903091643" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1080%2F17470210903091643" aria-label="Article reference 21" data-doi="10.1080/17470210903091643">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 21" href="http://scholar.google.com/scholar_lookup?&amp;title=Do%20humans%20produce%20the%20speed-accuracy%20trade-off%20that%20maximizes%20reward%20rate%3F&amp;journal=Q.%20J.%20Exp.%20Psychol.&amp;doi=10.1080%2F17470210903091643&amp;volume=63&amp;pages=863-891&amp;publication_year=2010&amp;author=Bogacz%2CR&amp;author=Hu%2CPT&amp;author=Holmes%2CPJ&amp;author=Cohen%2CJD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="22."><p class="c-article-references__text" id="ref-CR22">MacKay, D. G. The problems of flexibility, fluency, and speed-accuracy trade-off in skilled behavior. <i>Psychol. Rev.</i> <b>89</b>, 483–506 (1982).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1037/0033-295X.89.5.483" data-track-item_id="10.1037/0033-295X.89.5.483" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1037%2F0033-295X.89.5.483" aria-label="Article reference 22" data-doi="10.1037/0033-295X.89.5.483">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 22" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20problems%20of%20flexibility%2C%20fluency%2C%20and%20speed-accuracy%20trade-off%20in%20skilled%20behavior&amp;journal=Psychol.%20Rev.&amp;doi=10.1037%2F0033-295X.89.5.483&amp;volume=89&amp;pages=483-506&amp;publication_year=1982&amp;author=MacKay%2CDG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="23."><p class="c-article-references__text" id="ref-CR23">Fitts, P. M. Cognitive aspects of information processing: III. Set for speed versus accuracy. <i>J. Exp. Psychol.</i> <b>71</b>, 849–857. <a href="https://doi.org/10.1037/h0023232" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1037/h0023232">https://doi.org/10.1037/h0023232</a> (1966).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1037/h0023232" data-track-item_id="10.1037/h0023232" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1037%2Fh0023232" aria-label="Article reference 23" data-doi="10.1037/h0023232">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaF287ltleitw%3D%3D" aria-label="CAS reference 23">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=5939364" aria-label="PubMed reference 23">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 23" href="http://scholar.google.com/scholar_lookup?&amp;title=Cognitive%20aspects%20of%20information%20processing%3A%20III.%20Set%20for%20speed%20versus%20accuracy&amp;journal=J.%20Exp.%20Psychol.&amp;doi=10.1037%2Fh0023232&amp;volume=71&amp;pages=849-857&amp;publication_year=1966&amp;author=Fitts%2CPM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="24."><p class="c-article-references__text" id="ref-CR24">Nissen, M. J. &amp; Bullemer, P. Attentional requirements of learning: Evidence from performance measures. <i>Cogn. Psychol.</i> <b>19</b>, 1–32 (1987).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/0010-0285(87)90002-8" data-track-item_id="10.1016/0010-0285(87)90002-8" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2F0010-0285%2887%2990002-8" aria-label="Article reference 24" data-doi="10.1016/0010-0285(87)90002-8">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 24" href="http://scholar.google.com/scholar_lookup?&amp;title=Attentional%20requirements%20of%20learning%3A%20Evidence%20from%20performance%20measures&amp;journal=Cogn.%20Psychol.&amp;doi=10.1016%2F0010-0285%2887%2990002-8&amp;volume=19&amp;pages=1-32&amp;publication_year=1987&amp;author=Nissen%2CMJ&amp;author=Bullemer%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="25."><p class="c-article-references__text" id="ref-CR25">Willingham, D. B., Nissen, M. J. &amp; Bullemer, P. On the development of procedural knowledge. <i>J. Exp. Psychol. Learn. Mem. Cogn.</i> <b>15</b>, 1047 (1989).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1037/0278-7393.15.6.1047" data-track-item_id="10.1037/0278-7393.15.6.1047" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1037%2F0278-7393.15.6.1047" aria-label="Article reference 25" data-doi="10.1037/0278-7393.15.6.1047">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaK3c%2FjvFWlsw%3D%3D" aria-label="CAS reference 25">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=2530305" aria-label="PubMed reference 25">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 25" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20the%20development%20of%20procedural%20knowledge&amp;journal=J.%20Exp.%20Psychol.%20Learn.%20Mem.%20Cogn.&amp;doi=10.1037%2F0278-7393.15.6.1047&amp;volume=15&amp;publication_year=1989&amp;author=Willingham%2CDB&amp;author=Nissen%2CMJ&amp;author=Bullemer%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="26."><p class="c-article-references__text" id="ref-CR26">Robertson, E. M. The serial reaction time task: Implicit motor skill learning?. <i>J. Neurosci.</i> <b>27</b>, 10073–10075 (2007).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.2747-07.2007" data-track-item_id="10.1523/JNEUROSCI.2747-07.2007" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.2747-07.2007" aria-label="Article reference 26" data-doi="10.1523/JNEUROSCI.2747-07.2007">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD2sXhtFahsbbM" aria-label="CAS reference 26">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=17881512" aria-label="PubMed reference 26">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6672677" aria-label="PubMed Central reference 26">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 26" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20serial%20reaction%20time%20task%3A%20Implicit%20motor%20skill%20learning%3F&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.2747-07.2007&amp;volume=27&amp;pages=10073-10075&amp;publication_year=2007&amp;author=Robertson%2CEM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="27."><p class="c-article-references__text" id="ref-CR27">Keele, S. W., Ivry, R., Mayr, U., Hazeltine, E. &amp; Heuer, H. The cognitive and neural architecture of sequence representation. <i>Psychol. Rev.</i> <b>110</b>, 316–339. <a href="https://doi.org/10.1037/0033-295X.110.2.316" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1037/0033-295X.110.2.316">https://doi.org/10.1037/0033-295X.110.2.316</a> (2003).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1037/0033-295X.110.2.316" data-track-item_id="10.1037/0033-295X.110.2.316" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1037%2F0033-295X.110.2.316" aria-label="Article reference 27" data-doi="10.1037/0033-295X.110.2.316">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=12747526" aria-label="PubMed reference 27">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 27" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20cognitive%20and%20neural%20architecture%20of%20sequence%20representation&amp;journal=Psychol.%20Rev.&amp;doi=10.1037%2F0033-295X.110.2.316&amp;volume=110&amp;pages=316-339&amp;publication_year=2003&amp;author=Keele%2CSW&amp;author=Ivry%2CR&amp;author=Mayr%2CU&amp;author=Hazeltine%2CE&amp;author=Heuer%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="28."><p class="c-article-references__text" id="ref-CR28">Willingham, D. B., Salidis, J. &amp; Gabrieli, J. D. Direct comparison of neural systems mediating conscious and unconscious skill learning. <i>J. Neurophysiol.</i> <b>88</b>, 1451–1460. <a href="https://doi.org/10.1152/jn.2002.88.3.1451" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1152/jn.2002.88.3.1451">https://doi.org/10.1152/jn.2002.88.3.1451</a> (2002).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1152/jn.2002.88.3.1451" data-track-item_id="10.1152/jn.2002.88.3.1451" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1152%2Fjn.2002.88.3.1451" aria-label="Article reference 28" data-doi="10.1152/jn.2002.88.3.1451">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=12205165" aria-label="PubMed reference 28">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 28" href="http://scholar.google.com/scholar_lookup?&amp;title=Direct%20comparison%20of%20neural%20systems%20mediating%20conscious%20and%20unconscious%20skill%20learning&amp;journal=J.%20Neurophysiol.&amp;doi=10.1152%2Fjn.2002.88.3.1451&amp;volume=88&amp;pages=1451-1460&amp;publication_year=2002&amp;author=Willingham%2CDB&amp;author=Salidis%2CJ&amp;author=Gabrieli%2CJD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="29."><p class="c-article-references__text" id="ref-CR29">Howard, J. H. &amp; Howard, D. V. Age differences in implicit learning of higher order dependencies in serial patterns. <i>Psychol. Aging</i> <b>12</b>, 634–656. <a href="https://doi.org/10.1037//0882-7974.12.4.634" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1037//0882-7974.12.4.634">https://doi.org/10.1037//0882-7974.12.4.634</a> (1997).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1037//0882-7974.12.4.634" data-track-item_id="10.1037//0882-7974.12.4.634" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1037%2F%2F0882-7974.12.4.634" aria-label="Article reference 29" data-doi="10.1037//0882-7974.12.4.634">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=9416632" aria-label="PubMed reference 29">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 29" href="http://scholar.google.com/scholar_lookup?&amp;title=Age%20differences%20in%20implicit%20learning%20of%20higher%20order%20dependencies%20in%20serial%20patterns&amp;journal=Psychol.%20Aging&amp;doi=10.1037%2F%2F0882-7974.12.4.634&amp;volume=12&amp;pages=634-656&amp;publication_year=1997&amp;author=Howard%2CJH&amp;author=Howard%2CDV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="30."><p class="c-article-references__text" id="ref-CR30">Romano, J. C., Howard, J. H. &amp; Howard, D. V. One-year retention of general and sequence-specific skills in a probabilistic, serial reaction time task. <i>Memory</i> <b>18</b>, 427–441. <a href="https://doi.org/10.1080/09658211003742680" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1080/09658211003742680">https://doi.org/10.1080/09658211003742680</a> (2010).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1080/09658211003742680" data-track-item_id="10.1080/09658211003742680" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1080%2F09658211003742680" aria-label="Article reference 30" data-doi="10.1080/09658211003742680">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20408037" aria-label="PubMed reference 30">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2894701" aria-label="PubMed Central reference 30">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 30" href="http://scholar.google.com/scholar_lookup?&amp;title=One-year%20retention%20of%20general%20and%20sequence-specific%20skills%20in%20a%20probabilistic%2C%20serial%20reaction%20time%20task&amp;journal=Memory&amp;doi=10.1080%2F09658211003742680&amp;volume=18&amp;pages=427-441&amp;publication_year=2010&amp;author=Romano%2CJC&amp;author=Howard%2CJH&amp;author=Howard%2CDV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="31."><p class="c-article-references__text" id="ref-CR31">Bornstein, A. &amp; Daw, N. Cortical and hippocampal correlates of deliberation during model-based decisions for rewards in humans. <i>PLoS Comput. Biol.</i> <b>9</b>, e1003387. <a href="https://doi.org/10.1371/journal.pcbi.1003387" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1371/journal.pcbi.1003387">https://doi.org/10.1371/journal.pcbi.1003387</a> (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1371/journal.pcbi.1003387" data-track-item_id="10.1371/journal.pcbi.1003387" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1371%2Fjournal.pcbi.1003387" aria-label="Article reference 31" data-doi="10.1371/journal.pcbi.1003387">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC2cXjtV2gtL0%3D" aria-label="CAS reference 31">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24339770" aria-label="PubMed reference 31">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3854511" aria-label="PubMed Central reference 31">PubMed Central</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2013PLSCB...9E3387B" aria-label="ADS reference 31">ADS</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 31" href="http://scholar.google.com/scholar_lookup?&amp;title=Cortical%20and%20hippocampal%20correlates%20of%20deliberation%20during%20model-based%20decisions%20for%20rewards%20in%20humans&amp;journal=PLoS%20Comput.%20Biol.&amp;doi=10.1371%2Fjournal.pcbi.1003387&amp;volume=9&amp;publication_year=2013&amp;author=Bornstein%2CA&amp;author=Daw%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="32."><p class="c-article-references__text" id="ref-CR32">Schvaneveldt, R. W. &amp; Gomez, R. L. Attention and probabilistic sequence learning. <i>Psychol. Res.</i> <b>61</b>, 175–190. <a href="https://doi.org/10.1007/s004260050023" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1007/s004260050023">https://doi.org/10.1007/s004260050023</a> (1998).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/s004260050023" data-track-item_id="10.1007/s004260050023" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s004260050023" aria-label="Article reference 32" data-doi="10.1007/s004260050023">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 32" href="http://scholar.google.com/scholar_lookup?&amp;title=Attention%20and%20probabilistic%20sequence%20learning&amp;journal=Psychol.%20Res.&amp;doi=10.1007%2Fs004260050023&amp;volume=61&amp;pages=175-190&amp;publication_year=1998&amp;author=Schvaneveldt%2CRW&amp;author=Gomez%2CRL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="33."><p class="c-article-references__text" id="ref-CR33">Cleeremans, A. &amp; McClelland, J. L. Learning the structure of event sequences. <i>J. Exp. Psychol. Gen.</i> <b>120</b>, 235–253 (1991).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1037/0096-3445.120.3.235" data-track-item_id="10.1037/0096-3445.120.3.235" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1037%2F0096-3445.120.3.235" aria-label="Article reference 33" data-doi="10.1037/0096-3445.120.3.235">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="/articles/cas-redirect/1:STN:280:DyaK38%2FptF2kug%3D%3D" aria-label="CAS reference 33">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=1836490" aria-label="PubMed reference 33">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 33" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20the%20structure%20of%20event%20sequences&amp;journal=J.%20Exp.%20Psychol.%20Gen.&amp;doi=10.1037%2F0096-3445.120.3.235&amp;volume=120&amp;pages=235-253&amp;publication_year=1991&amp;author=Cleeremans%2CA&amp;author=McClelland%2CJL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="34."><p class="c-article-references__text" id="ref-CR34">Provyn, J. P. Associative processes in statistical learning: Paradoxical predictions of the past. <i>Psychol.-Diss.</i> <b>179</b>, 78 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="mathscinet reference" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=3153375" aria-label="MathSciNet reference 34">MathSciNet</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 34" href="http://scholar.google.com/scholar_lookup?&amp;title=Associative%20processes%20in%20statistical%20learning%3A%20Paradoxical%20predictions%20of%20the%20past&amp;journal=Psychol.-Diss.&amp;volume=179&amp;publication_year=2013&amp;author=Provyn%2CJP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="35."><p class="c-article-references__text" id="ref-CR35">Perruchet, P. &amp; Vinter, A. Parser: A model for word segmentation. <i>J. Mem. Lang.</i> <b>39</b>, 246–263. <a href="https://doi.org/10.1006/jmla.1998.2576" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1006/jmla.1998.2576">https://doi.org/10.1006/jmla.1998.2576</a> (1998).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1006/jmla.1998.2576" data-track-item_id="10.1006/jmla.1998.2576" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1006%2Fjmla.1998.2576" aria-label="Article reference 35" data-doi="10.1006/jmla.1998.2576">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 35" href="http://scholar.google.com/scholar_lookup?&amp;title=Parser%3A%20A%20model%20for%20word%20segmentation&amp;journal=J.%20Mem.%20Lang.&amp;doi=10.1006%2Fjmla.1998.2576&amp;volume=39&amp;pages=246-263&amp;publication_year=1998&amp;author=Perruchet%2CP&amp;author=Vinter%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="36."><p class="c-article-references__text" id="ref-CR36">Servan-Schreiber, E. &amp; Anderson, J. Learning artificial grammars with competitive chunking. <i>J. Exp. Psychol. Learn. Mem. Cogn.</i> <b>16</b>, 592–608. <a href="https://doi.org/10.1037/0278-7393.16.4.592" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1037/0278-7393.16.4.592">https://doi.org/10.1037/0278-7393.16.4.592</a> (1990).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1037/0278-7393.16.4.592" data-track-item_id="10.1037/0278-7393.16.4.592" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1037%2F0278-7393.16.4.592" aria-label="Article reference 36" data-doi="10.1037/0278-7393.16.4.592">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 36" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20artificial%20grammars%20with%20competitive%20chunking&amp;journal=J.%20Exp.%20Psychol.%20Learn.%20Mem.%20Cogn.&amp;doi=10.1037%2F0278-7393.16.4.592&amp;volume=16&amp;pages=592-608&amp;publication_year=1990&amp;author=Servan-Schreiber%2CE&amp;author=Anderson%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="37."><p class="c-article-references__text" id="ref-CR37">French, R. M., Addyman, C. &amp; Mareschal, D. TRACX: A recognition-based connectionist framework for sequence segmentation and chunk extraction. <i>Psychol. Rev.</i> <b>118</b>, 614–636. <a href="https://doi.org/10.1037/a0025255" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1037/a0025255">https://doi.org/10.1037/a0025255</a> (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1037/a0025255" data-track-item_id="10.1037/a0025255" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1037%2Fa0025255" aria-label="Article reference 37" data-doi="10.1037/a0025255">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22003842" aria-label="PubMed reference 37">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 37" href="http://scholar.google.com/scholar_lookup?&amp;title=TRACX%3A%20A%20recognition-based%20connectionist%20framework%20for%20sequence%20segmentation%20and%20chunk%20extraction&amp;journal=Psychol.%20Rev.&amp;doi=10.1037%2Fa0025255&amp;volume=118&amp;pages=614-636&amp;publication_year=2011&amp;author=French%2CRM&amp;author=Addyman%2CC&amp;author=Mareschal%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="38."><p class="c-article-references__text" id="ref-CR38">Cleeremans, A., Servan-Schreiber, D. &amp; McClelland, J. L. Finite state automata and simple recurrent networks. <i>Neural Comput.</i> <b>1</b>, 372–381. <a href="https://doi.org/10.1162/neco.1989.1.3.372" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1162/neco.1989.1.3.372">https://doi.org/10.1162/neco.1989.1.3.372</a> (1989).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1162/neco.1989.1.3.372" data-track-item_id="10.1162/neco.1989.1.3.372" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1162%2Fneco.1989.1.3.372" aria-label="Article reference 38" data-doi="10.1162/neco.1989.1.3.372">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=Finite%20state%20automata%20and%20simple%20recurrent%20networks&amp;journal=Neural%20Comput.&amp;doi=10.1162%2Fneco.1989.1.3.372&amp;volume=1&amp;pages=372-381&amp;publication_year=1989&amp;author=Cleeremans%2CA&amp;author=Servan-Schreiber%2CD&amp;author=McClelland%2CJL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="39."><p class="c-article-references__text" id="ref-CR39">Wang, Q., Rothkopf, C. A. &amp; Triesch, J. A model of human motor sequence learning explains facilitation and interference effects based on spike-timing dependent plasticity. <i>PLoS Comput. Biol.</i><a href="https://doi.org/10.1371/journal.pcbi.1005632" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1371/journal.pcbi.1005632">https://doi.org/10.1371/journal.pcbi.1005632</a> (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1371/journal.pcbi.1005632" data-track-item_id="10.1371/journal.pcbi.1005632" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1371%2Fjournal.pcbi.1005632" aria-label="Article reference 39" data-doi="10.1371/journal.pcbi.1005632">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29281633" aria-label="PubMed reference 39">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5760091" aria-label="PubMed Central reference 39">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20model%20of%20human%20motor%20sequence%20learning%20explains%20facilitation%20and%20interference%20effects%20based%20on%20spike-timing%20dependent%20plasticity&amp;journal=PLoS%20Comput.%20Biol.&amp;doi=10.1371%2Fjournal.pcbi.1005632&amp;publication_year=2017&amp;author=Wang%2CQ&amp;author=Rothkopf%2CCA&amp;author=Triesch%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="40."><p class="c-article-references__text" id="ref-CR40">Goldwater, S., Griffiths, T. &amp; Johnson, M. A bayesian framework for word segmentation: Exploring the effects of context. <i>Cognition</i> <b>112</b>, 21–54. <a href="https://doi.org/10.1016/j.cognition.2009.03.008" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1016/j.cognition.2009.03.008">https://doi.org/10.1016/j.cognition.2009.03.008</a> (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.cognition.2009.03.008" data-track-item_id="10.1016/j.cognition.2009.03.008" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.cognition.2009.03.008" aria-label="Article reference 40" data-doi="10.1016/j.cognition.2009.03.008">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19409539" aria-label="PubMed reference 40">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 40" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20bayesian%20framework%20for%20word%20segmentation%3A%20Exploring%20the%20effects%20of%20context&amp;journal=Cognition&amp;doi=10.1016%2Fj.cognition.2009.03.008&amp;volume=112&amp;pages=21-54&amp;publication_year=2009&amp;author=Goldwater%2CS&amp;author=Griffiths%2CT&amp;author=Johnson%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="41."><p class="c-article-references__text" id="ref-CR41">Brown, S. &amp; Heathcote, A. The simplest complete model of choice response time: Linear ballistic accumulation. <i>Cognit. psychol. Cogn. Psychol.</i> <b>57</b>, 153–78. <a href="https://doi.org/10.1016/j.cogpsych.2007.12.002" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1016/j.cogpsych.2007.12.002">https://doi.org/10.1016/j.cogpsych.2007.12.002</a> (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.cogpsych.2007.12.002" data-track-item_id="10.1016/j.cogpsych.2007.12.002" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.cogpsych.2007.12.002" aria-label="Article reference 41" data-doi="10.1016/j.cogpsych.2007.12.002">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18243170" aria-label="PubMed reference 41">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20simplest%20complete%20model%20of%20choice%20response%20time%3A%20Linear%20ballistic%20accumulation&amp;journal=Cognit.%20psychol.%20Cogn.%20Psychol.&amp;doi=10.1016%2Fj.cogpsych.2007.12.002&amp;volume=57&amp;pages=153-78&amp;publication_year=2008&amp;author=Brown%2CS&amp;author=Heathcote%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="42."><p class="c-article-references__text" id="ref-CR42">Donkin, C., Brown, S. &amp; Heathcote, A. Drawing conclusions from choice response time models: A tutorial using the linear ballistic accumulator. <i>J. Math. Psychol.</i> <b>55</b>, 140–151 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.jmp.2010.10.001" data-track-item_id="10.1016/j.jmp.2010.10.001" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.jmp.2010.10.001" aria-label="Article reference 42" data-doi="10.1016/j.jmp.2010.10.001">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="mathscinet reference" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=2781535" aria-label="MathSciNet reference 42">MathSciNet</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1231.91387" aria-label="MATH reference 42">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 42" href="http://scholar.google.com/scholar_lookup?&amp;title=Drawing%20conclusions%20from%20choice%20response%20time%20models%3A%20A%20tutorial%20using%20the%20linear%20ballistic%20accumulator&amp;journal=J.%20Math.%20Psychol.&amp;doi=10.1016%2Fj.jmp.2010.10.001&amp;volume=55&amp;pages=140-151&amp;publication_year=2011&amp;author=Donkin%2CC&amp;author=Brown%2CS&amp;author=Heathcote%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="43."><p class="c-article-references__text" id="ref-CR43">Verwey, W. <i>et al.</i> Buffer loading and chunking in sequential keypressing. <i>J. Exp. Psychol.</i> <b>00</b>, 544–562. <a href="https://doi.org/10.1037//0096-1523.22.3.544" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1037//0096-1523.22.3.544">https://doi.org/10.1037//0096-1523.22.3.544</a> (1996).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1037//0096-1523.22.3.544" data-track-item_id="10.1037//0096-1523.22.3.544" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1037%2F%2F0096-1523.22.3.544" aria-label="Article reference 43" data-doi="10.1037//0096-1523.22.3.544">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 43" href="http://scholar.google.com/scholar_lookup?&amp;title=Buffer%20loading%20and%20chunking%20in%20sequential%20keypressing&amp;journal=J.%20Exp.%20Psychol.&amp;doi=10.1037%2F%2F0096-1523.22.3.544&amp;volume=00&amp;pages=544-562&amp;publication_year=1996&amp;author=Verwey%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="44."><p class="c-article-references__text" id="ref-CR44">Du, Y. &amp; Clark, J. New insights into statistical learning and chunk learning in implicit sequence acquisition. <i>Psychon. Bull. Rev.</i> <b>24</b>, 1225–1233 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.3758/s13423-016-1193-4" data-track-item_id="10.3758/s13423-016-1193-4" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.3758%2Fs13423-016-1193-4" aria-label="Article reference 44" data-doi="10.3758/s13423-016-1193-4">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=27812961" aria-label="PubMed reference 44">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 44" href="http://scholar.google.com/scholar_lookup?&amp;title=New%20insights%20into%20statistical%20learning%20and%20chunk%20learning%20in%20implicit%20sequence%20acquisition&amp;journal=Psychon.%20Bull.%20Rev.&amp;doi=10.3758%2Fs13423-016-1193-4&amp;volume=24&amp;pages=1225-1233&amp;publication_year=2017&amp;author=Du%2CY&amp;author=Clark%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="45."><p class="c-article-references__text" id="ref-CR45">Minier, L., Fagot, J. &amp; Rey, A. The temporal dynamics of regularity extraction in non-human primates. <i>Cogn. Sci.</i> <b>40</b>, 1019–1030. <a href="https://doi.org/10.1111/cogs.12279" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1111/cogs.12279">https://doi.org/10.1111/cogs.12279</a> (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1111/cogs.12279" data-track-item_id="10.1111/cogs.12279" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1111%2Fcogs.12279" aria-label="Article reference 45" data-doi="10.1111/cogs.12279">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26303229" aria-label="PubMed reference 45">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 45" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20temporal%20dynamics%20of%20regularity%20extraction%20in%20non-human%20primates&amp;journal=Cogn.%20Sci.&amp;doi=10.1111%2Fcogs.12279&amp;volume=40&amp;pages=1019-1030&amp;publication_year=2016&amp;author=Minier%2CL&amp;author=Fagot%2CJ&amp;author=Rey%2CA">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41598-023-31500-3?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgements"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>We thank Peter Dayan, Felix Wichmann, and Mirko Thalmann for helpful discussions. This work was supported by the Max Planck Society.</p></div></div></section><section data-title="Funding"><div class="c-article-section" id="Fun-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Fun">Funding</h2><div class="c-article-section__content" id="Fun-content"><p>Open Access funding enabled and organized by Projekt DEAL.</p></div></div></section><section aria-labelledby="author-information" data-title="Author information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Authors and Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">MPRG Computational Principles of Intelligence, Max Planck Institute for Biological Cybernetics, Tübingen, Germany</p><p class="c-article-author-affiliation__authors-list">Shuchen Wu &amp; Eric Schulz</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Department of Computational Neuroscience, Max Planck Institute for Biological Cybernetics, Tübingen, Germany</p><p class="c-article-author-affiliation__authors-list">Noémi Éltető</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Google DeepMind, New York City, NY, USA</p><p class="c-article-author-affiliation__authors-list">Ishita Dasgupta</p></li></ol><div class="u-js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Shuchen-Wu-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Shuchen Wu</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Shuchen%20Wu" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Shuchen%20Wu" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Shuchen%20Wu%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-No_mi-_ltet_-Aff2"><span class="c-article-authors-search__title u-h3 js-search-name">Noémi Éltető</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=No%C3%A9mi%20%C3%89ltet%C5%91" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=No%C3%A9mi%20%C3%89ltet%C5%91" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22No%C3%A9mi%20%C3%89ltet%C5%91%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Ishita-Dasgupta-Aff3"><span class="c-article-authors-search__title u-h3 js-search-name">Ishita Dasgupta</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Ishita%20Dasgupta" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ishita%20Dasgupta" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ishita%20Dasgupta%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Eric-Schulz-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Eric Schulz</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Eric%20Schulz" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Eric%20Schulz" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Eric%20Schulz%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="contributions">Contributions</h3><p>Conceptualization: S.W., N. É., I.D., E.S. Formal analysis: S.W., E.S. Software: S.W. Visualization: S.W., N. É. Writing-original draft: S.W., E.S. Writing-review &amp; editing: S.W., N. É., I.D., E.S.</p><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" aria-label="email Shuchen Wu" href="mailto:shuchen.wu@tue.mpg.de">Shuchen Wu</a>.</p></div></div></section><section data-title="Ethics declarations"><div class="c-article-section" id="ethics-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="ethics">Ethics declarations</h2><div class="c-article-section__content" id="ethics-content">
              
                <h3 class="c-article__sub-heading" id="FPar1">Competing Interests</h3>
                <p>The authors declare no competing interests.</p>
              
            </div></div></section><section data-title="Additional information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><h3 class="c-article__sub-heading">Publisher's note</h3><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section data-title="Supplementary Information"><div class="c-article-section" id="Sec32-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec32">Supplementary Information</h2><div class="c-article-section__content" id="Sec32-content"><div data-test="supplementary-info"><div id="figshareContainer" class="c-article-figshare-container" data-test="figshare-container"></div><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM1"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary information." href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41598-023-31500-3/MediaObjects/41598_2023_31500_MOESM1_ESM.pdf" data-supp-info-image="">Supplementary Information.</a></h3></div></div></div></div></section><section data-title="Rights and permissions"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content">
                <p><b>Open Access</b> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">http://creativecommons.org/licenses/by/4.0/</a>.</p>
              <p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Chunking%20as%20a%20rational%20solution%20to%20the%20speed%E2%80%93accuracy%20trade-off%20in%20a%20serial%20reaction%20time%20task&amp;author=Shuchen%20Wu%20et%20al&amp;contentID=10.1038%2Fs41598-023-31500-3&amp;copyright=The%20Author%28s%29&amp;publication=2045-2322&amp;publicationDate=2023-05-11&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints and permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1038/s41598-023-31500-3" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1038/s41598-023-31500-3" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img loading="lazy" width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Wu, S., Éltető, N., Dasgupta, I. <i>et al.</i> Chunking as a rational solution to the speed–accuracy trade-off in a serial reaction time task.
                    <i>Sci Rep</i> <b>13</b>, 7680 (2023). https://doi.org/10.1038/s41598-023-31500-3</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41598-023-31500-3?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2022-03-28">28 March 2022</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2023-03-13">13 March 2023</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2023-05-11">11 May 2023</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Version of record<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2023-05-11">11 May 2023</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--full-width"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value">https://doi.org/10.1038/s41598-023-31500-3</span></p></li></ul><div data-component="share-box"><div class="c-article-share-box u-display-none" hidden=""><h3 class="c-article__sub-heading">Share this article</h3><p class="c-article-share-box__description">Anyone you share the following link with will be able to read this content:</p><button class="js-get-share-url c-article-share-box__button" type="button" id="get-share-url" data-track="click" data-track-label="button" data-track-external="" data-track-action="get shareable link">Get shareable link</button><div class="js-no-share-url-container u-display-none" hidden=""><p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">Sorry, a shareable link is not currently available for this article.</p></div><div class="js-share-url-container u-display-none" hidden=""><p class="js-share-url c-article-share-box__only-read-input" id="share-url" data-track="click" data-track-label="button" data-track-action="select share url"></p><button class="js-copy-share-url c-article-share-box__button--link-like" type="button" id="copy-share-url" data-track="click" data-track-label="button" data-track-action="copy share url" data-track-external="">Copy shareable link to clipboard</button></div><p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        </p></div></div><div data-component="article-info-list"></div></div></div></div></div></section>
            </div>

            
        <section>
            <div class="c-article-section js-article-section" id="further-reading-section" data-test="further-reading-section">
                <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="further-reading">This article is cited by</h2>
                <div class="c-article-section__content js-collapsible-section" id="further-reading-content">
                    <ul class="c-article-further-reading__list" id="further-reading-list">
                        
                            <li class="c-article-further-reading__item js-ref-item">
                            
                                <h3 class="c-article-further-reading__title" data-test="article-title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Frontal Theta Modulation in Sequential Working Memory: the Impact of Spatial Regularity and Scenario" href="https://doi.org/10.1007/s10548-025-01152-9">
                                        Frontal Theta Modulation in Sequential Working Memory: the Impact of Spatial Regularity and Scenario
                                    </a>
                                </h3>
                            
                                
                                    <ul data-test="author-list" class="app-author-list app-author-list--compact app-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Yichao Huang</li><li>Yufeng Ke</li><li>Dong Ming</li>
                                    </ul>
                                
                                <p class="c-article-further-reading__journal-title"><i>Brain Topography</i> (2025)</p>
                            </li>
                        
                    </ul>
                </div>
            </div>
        </section>
    
            

        </div>
</article>
</main>

<aside class="c-article-extras u-hide-print" aria-label="Article navigation" data-component-reading-companion data-container-type="reading-companion" data-track-component="reading companion">
    <div class="u-mb-48 js-context-bar-sticky-point-desktop" data-track-context="reading companion">
        

        
            
                
                    
    
        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41598-023-31500-3.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="content_download" data-track-type="article pdf download" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>
    

                    
                
            
        
    </div>

    
        
    

    
    

    <div class="c-reading-companion">
        <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
            <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                <div class="u-lazy-ad-wrapper u-mt-16 u-hide" data-component-mpu>
                    <div class="c-ad c-ad--300x250">
                        <div class="c-ad__inner">
                            <p class="c-ad__label">Advertisement</p>
                            
    <div id="div-gpt-ad-right-2"
         class="div-gpt-ad advert medium-rectangle js-ad text-center hide-print grade-c-hide"
         data-ad-type="right"
         data-test="right-ad"
         data-pa11y-ignore
         data-gpt
         data-gpt-unitpath="/285/scientific_reports/article"
         data-gpt-sizes="300x250"
         data-gpt-targeting="type=article;pos=right;artid=s41598-023-31500-3;doi=10.1038/s41598-023-31500-3;subjmeta=116,1595,2649,2811,378,477,631;kwrd=Cognitive+neuroscience,Computational+neuroscience,Human+behaviour,Learning+and+memory,Psychology">
        
        <script>
            window.SN = window.SN || {};
            window.SN.libs = window.SN.libs || {};
            window.SN.libs.ads = window.SN.libs.ads || {};
            window.SN.libs.ads.slotConfig = window.SN.libs.ads.slotConfig || {};
            
                window.SN.libs.ads.slotConfig['right'] = {
                    'pos': 'right',
                    'type': 'article',
                    'path': 's41598-023-31500-3'
                };
            
            
            window.SN.libs.ads.slotConfig['kwrd'] = 'Cognitive+neuroscience,Computational+neuroscience,Human+behaviour,Learning+and+memory,Psychology';
            
            
            window.SN.libs.ads.slotConfig['subjmeta'] = '116,1595,2649,2811,378,477,631';
            
            
        </script>
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/scientific_reports/article&amp;sz=300x250&amp;c=-2019227678&amp;t=pos%3Dright%26type%3Darticle%26artid%3Ds41598-023-31500-3%26doi%3D10.1038/s41598-023-31500-3%26subjmeta%3D116,1595,2649,2811,378,477,631%26kwrd%3DCognitive+neuroscience,Computational+neuroscience,Human+behaviour,Learning+and+memory,Psychology">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/scientific_reports/article&amp;sz=300x250&amp;c=-2019227678&amp;t=pos%3Dright%26type%3Darticle%26artid%3Ds41598-023-31500-3%26doi%3D10.1038/s41598-023-31500-3%26subjmeta%3D116,1595,2649,2811,378,477,631%26kwrd%3DCognitive+neuroscience,Computational+neuroscience,Human+behaviour,Learning+and+memory,Psychology"
                     alt="Advertisement"
                     width="300"
                     height="250"></a>
        </noscript>
    </div>

                        </div>
                    </div>
                </div>
            </div>
            <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
            <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
        </div>
    </div>
</aside>
</div>


    
        <nav class="c-header__dropdown" aria-labelledby="Explore-content" data-test="Explore-content" id="explore" data-track-component="nature-150-split-header">
            <div class="c-header__container">
                <h2 id="Explore-content" class="c-header__heading c-header__heading--js-hide">Explore content</h2>
                <ul class="c-header__list c-header__list--js-stack">
                    
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/research-articles"
                                   data-track="click"
                                   data-track-action="research articles"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Research articles
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/news-and-comment"
                                   data-track="click"
                                   data-track-action="news &amp; comment"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    News &amp; Comment
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/collections"
                                   data-track="click"
                                   data-track-action="collections"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Collections
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/browse-subjects"
                                   data-track="click"
                                   data-track-action="subjects"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Subjects
                                </a>
                            </li>
                        
                    
                </ul>
                <ul class="c-header__list c-header__list--js-stack">
                    
                        <li class="c-header__item">
                            <a class="c-header__link"
                               href="https://www.facebook.com/scientificreports"
                               data-track="click"
                               data-track-action="facebook"
                               data-track-label="link">Follow us on Facebook
                            </a>
                        </li>
                    
                    
                    
                        <li class="c-header__item">
                            <a class="c-header__link"
                               href="https://twitter.com/SciReports"
                               data-track="click"
                               data-track-action="twitter"
                               data-track-label="link">Follow us on X
                            </a>
                        </li>
                    
                    
                    
                        <li class="c-header__item c-header__item--hide-lg">
                            <a class="c-header__link"
                               href="https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41598"
                               rel="nofollow"
                               data-track="nav_sign_up_for_alerts"
                               data-track-action="Sign up for alerts"
                               data-track-external
                               data-track-label="link (mobile dropdown)">Sign up for alerts<svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill="#fff"/></svg>
                            </a>
                        </li>
                    
                    
                        <li class="c-header__item c-header__item--hide-lg">
                            <a class="c-header__link"
                               href="https://www.nature.com/srep.rss"
                               data-track="click"
                               data-track-action="rss feed"
                               data-track-label="link">
                                <span>RSS feed</span>
                            </a>
                        </li>
                    
                </ul>
            </div>
        </nav>
    
    
        
            <nav class="c-header__dropdown" aria-labelledby="About-the-journal" id="about-the-journal" data-test="about-the-journal" data-track-component="nature-150-split-header">
                <div class="c-header__container">
                    <h2 id="About-the-journal" class="c-header__heading c-header__heading--js-hide">About the journal</h2>
                    <ul class="c-header__list c-header__list--js-stack">
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/about"
                                   data-track="click"
                                   data-track-action="about scientific reports"
                                   data-track-label="link">
                                    About Scientific Reports
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/contact"
                                   data-track="click"
                                   data-track-action="contact"
                                   data-track-label="link">
                                    Contact
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/journal-policies"
                                   data-track="click"
                                   data-track-action="journal policies"
                                   data-track-label="link">
                                    Journal policies
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/guide-to-referees"
                                   data-track="click"
                                   data-track-action="guide to referees"
                                   data-track-label="link">
                                    Guide to referees
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/calls-for-papers"
                                   data-track="click"
                                   data-track-action="calls for papers"
                                   data-track-label="link">
                                    Calls for Papers
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/editorschoice"
                                   data-track="click"
                                   data-track-action="editor&#x27;s choice"
                                   data-track-label="link">
                                    Editor&#x27;s Choice
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/highlights"
                                   data-track="click"
                                   data-track-action="journal highlights"
                                   data-track-label="link">
                                    Journal highlights
                                </a>
                            </li>
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/open-access"
                                   data-track="click"
                                   data-track-action="open access fees and funding"
                                   data-track-label="link">
                                    Open Access Fees and Funding
                                </a>
                            </li>
                        
                    </ul>
                </div>
            </nav>
        

        
            <nav class="c-header__dropdown" aria-labelledby="Publish-with-us-label" id="publish-with-us" data-test="publish-with-us" data-track-component="nature-150-split-header">
                <div class="c-header__container">
                    <h2 id="Publish-with-us-label" class="c-header__heading c-header__heading--js-hide">Publish with us</h2>
                    <ul class="c-header__list c-header__list--js-stack">
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/author-instructions"
                                   data-track="click"
                                   data-track-action="for authors"
                                   data-track-label="link">
                                    For authors
                                </a>
                            </li>
                        
                        
                            <li class="c-header__item">
                                <a class="c-header__link" data-test="nature-author-services"
                                   data-track="nav_language_services"
                                   data-track-context="header publish with us dropdown menu"
                                   data-track-action="manuscript author services"
                                   data-track-label="link manuscript author services"
                                   href="https://authorservices.springernature.com/go/sn/?utm_source=For+Authors&utm_medium=Website_Nature&utm_campaign=Platform+Experimentation+2022&utm_id=PE2022">
                                    Language editing services
                                </a>
                            </li>
                        
                        
                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/open-access-funding"
                                   data-test="funding-eligibility-link"
                                   data-track="click_explore_funding"
                                   data-track-context="header publish with us"
                                   data-track-action="funding eligibility">Open access funding</a>
                            </li>
                        
                        
                            <li class="c-header__item c-header__item--keyline">
                                <a class="c-header__link"
                                   href="https://author-welcome.nature.com/41598"
                                   data-track="click_submit_manuscript"
                                   data-track-context="submit link in Nature header dropdown menu"
                                   data-track-action="submit manuscript"
                                   data-track-label="link (publish with us dropdown menu)"
                                   data-track-external
                                   data-gtm-criteo="submit-manuscript">Submit manuscript<svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m15 0c1.1045695 0 2 .8954305 2 2v5.5c0 .27614237-.2238576.5-.5.5s-.5-.22385763-.5-.5v-5.5c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-9v3c0 1.1045695-.8954305 2-2 2h-3v10c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h7.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-7.5c-1.1045695 0-2-.8954305-2-2v-10.17157288c0-.53043297.21071368-1.0391408.58578644-1.41421356l3.82842712-3.82842712c.37507276-.37507276.88378059-.58578644 1.41421356-.58578644zm-.5442863 8.18867991 3.3545404 3.35454039c.2508994.2508994.2538696.6596433.0035959.909917-.2429543.2429542-.6561449.2462671-.9065387-.0089489l-2.2609825-2.3045251.0010427 7.2231989c0 .3569916-.2898381.6371378-.6473715.6371378-.3470771 0-.6473715-.2852563-.6473715-.6371378l-.0010428-7.2231995-2.2611222 2.3046654c-.2531661.2580415-.6562868.2592444-.9065605.0089707-.24295423-.2429542-.24865597-.6576651.0036132-.9099343l3.3546673-3.35466731c.2509089-.25090888.6612706-.25227691.9135302-.00001728zm-.9557137-3.18867991c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm-8.5-3.587-3.587 3.587h2.587c.55228475 0 1-.44771525 1-1zm8.5 1.587c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill="#fff"/></svg>
                                </a>
                            </li>
                        
                    </ul>
                </div>
            </nav>
        
    
    <script>
        window.dataLayer = window.dataLayer || [];
        window.dataLayer.push({
            page: {
                content: {
                    fundingWidget: "true",
                        }
                    }
                });
    </script>


<div id="search-menu" class="c-header__dropdown c-header__dropdown--full-width" data-track-component="nature-150-split-header">
    <div class="c-header__container">
        <h2 class="c-header__visually-hidden">Search</h2>
        <form class="c-header__search-form" action="/search" method="get" role="search" autocomplete="off" data-test="inline-search">
            <label class="c-header__heading" for="keywords">Search articles by subject, keyword or author</label>
            <div class="c-header__search-layout c-header__search-layout--max-width">
                <div>
                    <input type="text" required="" class="c-header__input" id="keywords" name="q" value="">
                </div>
                <div class="c-header__search-layout">
                    <div>
                        <label for="results-from" class="c-header__visually-hidden">Show results from</label>
                        <select id="results-from" name="journal" class="c-header__select">
                            
                                
                                    <option value="" selected>All journals</option>
                                    <option value="srep">This journal</option>
                                
                            
                        </select>
                    </div>
                    <div>
                        <button type="submit" class="c-header__search-button">Search</button>
                    </div>
                </div>

            </div>
        </form>

        <div class="c-header__flush">
            <a class="c-header__link" href="/search/advanced"
               data-track="click" data-track-action="advanced search" data-track-label="link">
                Advanced search
            </a>
        </div>

        <h3 class="c-header__heading c-header__heading--keyline">Quick links</h3>
        <ul class="c-header__list">
            <li><a class="c-header__link" href="/subjects" data-track="click" data-track-action="explore articles by subject" data-track-label="link">Explore articles by subject</a></li>
            <li><a class="c-header__link" href="/naturecareers" data-track="click" data-track-action="find a job" data-track-label="link">Find a job</a></li>
            <li><a class="c-header__link" href="/authors/index.html" data-track="click" data-track-action="guide to authors" data-track-label="link">Guide to authors</a></li>
            <li><a class="c-header__link" href="/authors/editorial_policies/" data-track="click" data-track-action="editorial policies" data-track-label="link">Editorial policies</a></li>
        </ul>
    </div>
</div>

<footer class="composite-layer" itemscope itemtype="http://schema.org/Periodical">
        <meta itemprop="publisher" content="Springer Nature">
        

        <div class="u-mt-16 u-mb-16">
    <div class="u-container">
        <div class="u-display-flex u-flex-wrap u-justify-content-space-between">
            

            <div class="c-meta u-ma-0 u-flex-shrink">
                <p class="c-meta__item c-meta__type u-mt-0">
                    <span itemprop="name">
                        Scientific Reports
                    </span>
                    (<i itemprop="alternateName">Sci Rep</i>)
                </p>
                
    
        <p class="c-meta__item u-mt-0">
            <abbr title="International Standard Serial Number">ISSN</abbr> <span itemprop="issn">2045-2322</span> (online)
        </p>
    

                
    

            </div>
        </div>
    </div>
</div>

    <div class="c-footer">
        <div class="u-hide-print" data-track-component="footer">
    <h2 class="u-visually-hidden">nature.com sitemap</h2>
    <div class="c-footer__container">
        <div class="c-footer__grid c-footer__group--separator">
            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">About Nature Portfolio</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/npg_/company_info/index.html"
                                                  data-track="click" data-track-action="about us"
                                                  data-track-label="link">About us</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/npg_/press_room/press_releases.html"
                                                  data-track="click" data-track-action="press releases"
                                                  data-track-label="link">Press releases</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://press.nature.com/"
                                                  data-track="click" data-track-action="press office"
                                                  data-track-label="link">Press office</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://support.nature.com/support/home"
                                                  data-track="click" data-track-action="contact us"
                                                  data-track-label="link">Contact us</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Discover content</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/siteindex"
                                                  data-track="click" data-track-action="journals a-z"
                                                  data-track-label="link">Journals A-Z</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/subjects"
                                                  data-track="click" data-track-action="article by subject"
                                                  data-track-label="link">Articles by subject</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.protocols.io/"
                                                  data-track="click" data-track-action="protocols.io"
                                                  data-track-label="link">protocols.io</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureindex.com/"
                                                  data-track="click" data-track-action="nature index"
                                                  data-track-label="link">Nature Index</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Publishing policies</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/authors/editorial_policies"
                                                  data-track="click" data-track-action="Nature portfolio policies"
                                                  data-track-label="link">Nature portfolio policies</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/nature-research/open-access"
                                                  data-track="click" data-track-action="open access"
                                                  data-track-label="link">Open access</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Author &amp; Researcher services</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/reprints"
                                                  data-track="click" data-track-action="reprints and permissions"
                                                  data-track-label="link">Reprints &amp; permissions</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/authors/research-data"
                                                  data-track="click" data-track-action="data research service"
                                                  data-track-label="link">Research data</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://authorservices.springernature.com/language-editing/"
                                                  data-track="click" data-track-action="language editing"
                                                  data-track-label="link">Language editing</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://authorservices.springernature.com/scientific-editing/"
                                                  data-track="click" data-track-action="scientific editing"
                                                  data-track-label="link">Scientific editing</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://masterclasses.nature.com/"
                                                  data-track="click" data-track-action="nature masterclasses"
                                                  data-track-label="link">Nature Masterclasses</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://solutions.springernature.com/"
                                                  data-track="click" data-track-action="research solutions"
                                                  data-track-label="link">Research Solutions</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Libraries &amp; institutions</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/tools-services"
                                                  data-track="click" data-track-action="librarian service and tools"
                                                  data-track-label="link">Librarian service &amp; tools</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/manage-your-account/librarianportal"
                                                  data-track="click" data-track-action="librarian portal"
                                                  data-track-label="link">Librarian portal</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/openresearch/about-open-access/information-for-institutions"
                                                  data-track="click" data-track-action="open research"
                                                  data-track-label="link">Open research</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/recommend-to-your-library"
                                                  data-track="click" data-track-action="Recommend to library"
                                                  data-track-label="link">Recommend to library</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Advertising &amp; partnerships</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/product/digital-advertising/"
                                                  data-track="click" data-track-action="advertising"
                                                  data-track-label="link">Advertising</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://partnerships.nature.com/"
                                                  data-track="click" data-track-action="partnerships and services"
                                                  data-track-label="link">Partnerships &amp; Services</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/media-kits/" data-track="click"
                                                  data-track-action="media kits" data-track-label="link">Media kits</a>
                    </li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/product/branded-content-native-advertising/"
                                                  data-track-action="branded content" data-track-label="link">Branded
                        content</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Professional development</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/immersive/natureawards/index.html"
                                                  data-track="click" data-track-action="nature awards"
                                                  data-track-label="link">Nature Awards</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/naturecareers/"
                                                  data-track="click" data-track-action="nature careers"
                                                  data-track-label="link">Nature Careers</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://conferences.nature.com"
                                                  data-track="click" data-track-action="nature conferences"
                                                  data-track-label="link">Nature<span class="u-visually-hidden"> </span>
                        Conferences</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Regional websites</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/natafrica"
                                                  data-track="click" data-track-action="nature africa"
                                                  data-track-label="link">Nature Africa</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="http://www.naturechina.com"
                                                  data-track="click" data-track-action="nature china"
                                                  data-track-label="link">Nature China</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/nindia"
                                                  data-track="click" data-track-action="nature india"
                                                  data-track-label="link">Nature India</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureasia.com/ja-jp"
                                                  data-track="click" data-track-action="nature japan"
                                                  data-track-label="link">Nature Japan</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/nmiddleeast"
                                                  data-track="click" data-track-action="nature middle east"
                                                  data-track-label="link">Nature Middle East</a></li>
                </ul>
            </div>

        </div>
    </div>
    <div class="c-footer__container">
        <ul class="c-footer__links">
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/privacy"
                                          data-track="click" data-track-action="privacy policy" data-track-label="link">Privacy
                Policy</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/cookies"
                                          data-track="click" data-track-action="use of cookies" data-track-label="link">Use
                of cookies</a></li>
            <li class="c-footer__item">
                <button class="optanon-toggle-display c-footer__link" onclick="javascript:;"
                        data-cc-action="preferences" data-track="click" data-track-action="manage cookies"
                        data-track-label="link">Your privacy choices/Manage cookies
                </button>
            </li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/legal-notice"
                                          data-track="click" data-track-action="legal notice" data-track-label="link">Legal
                notice</a></li>
            <li class="c-footer__item"><a class="c-footer__link"
                                          href="https://www.nature.com/info/accessibility-statement" data-track="click"
                                          data-track-action="accessibility statement" data-track-label="link">Accessibility
                statement</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/terms-and-conditions"
                                          data-track="click" data-track-action="terms and conditions"
                                          data-track-label="link">Terms &amp; Conditions</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.springernature.com/ccpa"
                                          data-track="click" data-track-action="california privacy statement"
                                          data-track-label="link">Your US state privacy rights</a></li>
            
                <li class="c-footer__item"><a class="c-footer__link"
                                              href="https://support.nature.com/de/support/solutions/articles/6000255911-kündigungsformular">Cancel
                    contracts here</a></li>
            
        </ul>
    </div>
</div>


        <div class="c-footer__container">
    <a href="https://www.springernature.com/" class="c-footer__link">
        <img src="/static/images/logos/sn-logo-white-c8f7a9c061.svg" alt="Springer Nature" loading="lazy" width="200" height="20"/>
    </a>
    <p class="c-footer__legal" data-test="copyright">&copy; 2026 Springer Nature Limited</p>
</div>

    </div>
    <div class="u-visually-hidden" aria-hidden="true">
    
    <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="a" d="M0 .74h56.72v55.24H0z"/></defs><symbol id="icon-access" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M14 8a1 1 0 0 1 1 1v7h2.5a.5.5 0 0 1 .5.5V18H0v-1.5a.5.5 0 0 1 .5-.5H3V9a1 1 0 1 1 2 0v7h8V9a1 1 0 0 1 1-1M6 8l2 1v5l-2 1zm6 0v7l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0 1 16.081 7H1.92a.92.92 0 0 1-.528-1.674L8.427.401a1 1 0 0 1 1.146 0M9 2.441 5.345 5h7.31z"/></symbol><symbol id="icon-account" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M10.238 16.905a7.96 7.96 0 0 0 3.53-1.48c-.874-2.514-2.065-3.936-3.768-4.319V9.83a3.001 3.001 0 1 0-2 0v1.277c-1.703.383-2.894 1.805-3.767 4.319A7.96 7.96 0 0 0 9 17q.629 0 1.238-.095m4.342-2.172a8 8 0 1 0-11.16 0c.757-2.017 1.84-3.608 3.49-4.322a4 4 0 1 1 4.182 0c1.649.714 2.731 2.305 3.488 4.322M9 18A9 9 0 1 1 9 0a9 9 0 0 1 0 18"/></symbol><symbol id="icon-alert" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M4 10h2.5a.5.5 0 1 1 0 1H3.414l-1.121 1.121a1 1 0 0 0-.293.707V13h14v-.172a1 1 0 0 0-.293-.707L14 10.414V7A5 5 0 0 0 4 7zm3 4a2 2 0 1 0 4 0zm-5 0a1 1 0 0 1-1-1v-.172a2 2 0 0 1 .586-1.414L3 10V7a6 6 0 1 1 12 0v3l1.414 1.414A2 2 0 0 1 17 12.828V13a1 1 0 0 1-1 1h-4a3 3 0 0 1-6 0z"/></symbol><symbol id="icon-arrow-broad" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M7.897 12.028v-7.69l-2.45 2.446c-.387.388-1.009.395-1.385.02a.986.986 0 0 1 0-1.397l4.123-4.118a.99.99 0 0 1 1.398 0l4.123 4.118a.976.976 0 0 1 .016 1.38.99.99 0 0 1-1.401-.002l-2.45-2.447v8.676c0 .541-.437.98-.985.982L.987 14a.987.987 0 1 1 0-1.972z"/></symbol><symbol id="icon-arrow-down" viewBox="0 0 16 16"><path fill-rule="evenodd" d="m3.283 11.53 4.031 4.176a.943.943 0 0 0 1.367 0l4.031-4.176c.378-.391.384-1.02.016-1.4a.94.94 0 0 0-1.37.003l-2.395 2.482V1L8.956.883C8.901.386 8.493 0 7.998 0s-.903.386-.959.883L7.033 1v11.615l-2.396-2.482c-.379-.393-.986-.4-1.354-.02a1.027 1.027 0 0 0 0 1.417"/></symbol><symbol id="icon-arrow-left" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4.47 3.283.294 7.314a.943.943 0 0 0 0 1.367l4.176 4.031c.391.378 1.02.384 1.4.016a.94.94 0 0 0-.003-1.37L3.385 8.963H15l.117-.007c.497-.055.883-.463.883-.958s-.386-.903-.883-.959L15 7.033H3.385l2.482-2.396c.393-.379.4-.986.02-1.354a1.027 1.027 0 0 0-1.417 0"/></symbol><symbol id="icon-arrow-right" viewBox="0 0 16 16"><path fill-rule="evenodd" d="m11.53 12.717 4.176-4.031a.943.943 0 0 0 0-1.367L11.53 3.288c-.391-.378-1.02-.384-1.4-.016a.94.94 0 0 0 .003 1.37l2.482 2.395H1l-.117.007C.386 7.099 0 7.507 0 8.002s.386.903.883.959L1 8.967h11.615l-2.482 2.396c-.393.379-.4.986-.02 1.354a1.027 1.027 0 0 0 1.417 0"/></symbol><symbol id="icon-arrow-sub" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M7.897 4.972v7.69l-2.45-2.446c-.387-.388-1.009-.395-1.385-.02a.986.986 0 0 0 0 1.397l4.123 4.118a.99.99 0 0 0 1.398 0l4.123-4.118a.976.976 0 0 0 .016-1.38.99.99 0 0 0-1.401.002l-2.45 2.447V3.986a.98.98 0 0 0-.985-.982L.987 3a.987.987 0 1 0 0 1.972z"/></symbol><symbol id="icon-arrow-up" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M12.717 4.47 8.686.294a.943.943 0 0 0-1.367 0L3.288 4.47c-.378.391-.384 1.02-.016 1.4a.94.94 0 0 0 1.37-.003l2.395-2.482V15l.007.117c.055.497.463.883.958.883s.903-.386.959-.883L8.967 15V3.385l2.396 2.482c.379.393.986.4 1.354.02a1.027 1.027 0 0 0 0-1.417"/></symbol><symbol id="icon-article" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M13 15V2.01q-.002-.011.001-.01H2v13.006c0 .548.446.994.994.994h10.274A2 2 0 0 1 13 15m-12 .006V2C1 1.445 1.447 1 1.999 1H13c.552 0 .999.452.999 1.01V5h3v9.991A2.003 2.003 0 0 1 15.006 17H2.994C1.894 17 1 16.107 1 15.006M14 6v9a1 1 0 0 0 2 0V6zM4 4h7v4H4zm1 1v2h5V5zM4 9h7v1H4zm0 2h7v1H4zm0 2h7v1H4z"/></symbol><symbol id="icon-audio" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M13.096 13.559a.503.503 0 0 1-.707 0 .493.493 0 0 1 0-.702A5.41 5.41 0 0 0 13.998 9a5.41 5.41 0 0 0-1.609-3.856.493.493 0 0 1 0-.701.503.503 0 0 1 .708 0A6.4 6.4 0 0 1 15 9a6.4 6.4 0 0 1-1.904 4.559m2.123 2.102a.503.503 0 0 1-.707 0 .493.493 0 0 1 0-.7A8.37 8.37 0 0 0 17 9a8.37 8.37 0 0 0-2.488-5.96c-.195-.193-.195-.507 0-.7s.513-.194.708 0A9.36 9.36 0 0 1 18 9a9.36 9.36 0 0 1-2.78 6.661M1 11.976c-.552 0-1-.444-1-.992V7.016a.996.996 0 0 1 1-.992h2l5.42-3.84a1.005 1.005 0 0 1 1.395.231c.122.169.187.37.187.577v12.016a.996.996 0 0 1-1 .992c-.209 0-.412-.065-.582-.185L3 11.975zm0-.992h2.321l5.68 4.024V2.992l-5.68 4.024h-2.32z"/></symbol><symbol id="icon-block" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M0 0h24v24H0z"/></symbol><symbol id="icon-book" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M4 13V2h1v11h11V2H3a1 1 0 0 0-1 1v10.268A2 2 0 0 1 3 13zm12 1H3a1 1 0 0 0 0 2h13zm0 3H3a2 2 0 0 1-2-2V3a2 2 0 0 1 2-2h13a1 1 0 0 1 1 1v14a1 1 0 0 1-1 1M7.5 4h6a.5.5 0 1 1 0 1h-6a.5.5 0 0 1 0-1m1 2h4a.5.5 0 1 1 0 1h-4a.5.5 0 0 1 0-1"/></symbol><symbol id="icon-broad" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M11 16.995v-7.8l-2.482 2.482c-.392.393-1.022.4-1.403.02a1 1 0 0 1 0-1.417l4.177-4.176a1 1 0 0 1 1.416 0l4.177 4.176a.99.99 0 0 1 .016 1.4 1 1 0 0 1-1.42-.003L13 9.195v8.8c0 .55-.443.995-.998.996L4 18.995a1 1 0 0 1 0-2z"/></symbol><symbol id="icon-calendar" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M12.5 0c.276 0 .5.215.5.49V1h2c1.107 0 2 .895 2 2v12c0 1.107-.895 2-2 2H3c-1.107 0-2-.895-2-2V3c0-1.107.895-2 2-2h1v1H3c-.554 0-1 .446-1 1v3h14V3c0-.554-.446-1-1-1h-2v1.51a.5.5 0 0 1-.5.49.49.49 0 0 1-.5-.49V.49a.5.5 0 0 1 .5-.49M16 7H2v8c0 .554.446 1 1 1h12c.554 0 1-.446 1-1zM5 13v1H4v-1zm3 0v1H7v-1zm3 0v1h-1v-1zm-6-2v1H4v-1zm3 0v1H7v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zM8 9v1H7V9zm6 0v1h-1V9zm-3 0v1h-1V9zM5.5 0c.276 0 .5.215.5.49V1h5v1H6v1.51a.5.5 0 0 1-.5.49.49.49 0 0 1-.5-.49V.49A.5.5 0 0 1 5.5 0"/></symbol><symbol id="icon-cart" viewBox="0 0 18 18"><path d="M5 14a2 2 0 1 1 0 4 2 2 0 0 1 0-4m10 0a2 2 0 1 1 0 4 2 2 0 0 1 0-4M5 15a1 1 0 1 0 0 2 1 1 0 0 0 0-2m10 0a1 1 0 1 0 0 2 1 1 0 0 0 0-2M2.18 0a1 1 0 0 1 .98.804L3.4 2H16.76a1 1 0 0 1 .978 1.21l-1.29 5.986c-.102.468-.544.804-1.058.804H5.143c-.592 0-1.072.448-1.072 1 0 .513.414.936.947.993l.125.007h10.36a.498.498 0 0 1 0 .996L5.142 13C3.959 13 3 12.105 3 11c0-.663.345-1.25.877-1.614l-1.638-8.19a1 1 0 0 1-.02-.195L.5 1a.5.5 0 1 1 0-1zm14.532 3H3.599l1.205 6.025A2 2 0 0 1 5.143 9h10.27z"/></symbol><symbol id="icon-chevron-less" viewBox="0 0 10 10"><path fill-rule="evenodd" d="m5 3.414 3.293 3.293a1 1 0 0 0 1.414-1.414l-4-4a1 1 0 0 0-1.414 0l-4 4a1 1 0 0 0 1.414 1.414z"/></symbol><symbol id="icon-chevron-more" viewBox="0 0 10 10"><path fill-rule="evenodd" d="m5 6.586 3.293-3.293a1 1 0 0 1 1.414 1.414l-4 4a1 1 0 0 1-1.414 0l-4-4a1 1 0 1 1 1.414-1.414z"/></symbol><symbol id="icon-chevron-right" viewBox="0 0 10 10"><path fill-rule="evenodd" d="M4.706 4.033 2.292 1.637A.953.953 0 0 1 2.273.283a.98.98 0 0 1 1.378 0l4.063 4.031c.381.378.381.99 0 1.367L3.651 9.712a.97.97 0 0 1-1.362.016.96.96 0 0 1 .003-1.37l2.414-2.395L5.693 5z"/></symbol><symbol id="icon-circle-fill" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8 14A6 6 0 1 1 8 2a6 6 0 0 1 0 12"/></symbol><symbol id="icon-circle" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8m0 2A6 6 0 1 1 8 2a6 6 0 0 1 0 12"/></symbol><symbol id="icon-citation" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M8.636 6a4 4 0 0 1 2.174 7.358 6.23 6.23 0 0 1 3.462 3.641h-1.077a5.22 5.22 0 0 0-2.769-2.718l-.79-.33v-1.023l.63-.41a3 3 0 1 0-3.26 0l.63.41v1.024l-.79.329a5.22 5.22 0 0 0-2.77 2.718H3a6.23 6.23 0 0 1 3.461-3.641A4 4 0 0 1 8.636 6m7.369-5C17.107 1 18 1.894 18 3.003v5.994A2 2 0 0 1 16.005 11L13 10.999v-1h3.005c.549 0 .995-.447.995-1.002V3.003A.997.997 0 0 0 16.005 2H1.995A1 1 0 0 0 1 3.003V9q0 1 1 1l2-.001v1H2c-1 0-2-1-2-2V3.004C0 1.897.893 1 1.995 1zM7.5 4a.5.5 0 1 1 0 1h-4a.5.5 0 0 1 0-1zm3 0a.5.5 0 1 1 0 1h-1a.5.5 0 0 1 0-1zm4 0a.5.5 0 1 1 0 1h-2a.5.5 0 0 1 0-1z"/></symbol><symbol id="icon-close" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M2.297 12.277a1.007 1.007 0 0 0 0 1.427 1.006 1.006 0 0 0 1.426 0L8 9.425l4.277 4.277a1.007 1.007 0 0 0 1.427 0 1.006 1.006 0 0 0 0-1.426L9.425 8l4.277-4.277a1.007 1.007 0 0 0 0-1.427 1.006 1.006 0 0 0-1.426 0L8 6.575 3.723 2.297a1.007 1.007 0 0 0-1.427 0 1.006 1.006 0 0 0 0 1.426L6.575 8z"/></symbol><symbol id="icon-collections" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M15 4a2 2 0 0 1 2 2v9a2 2 0 0 1-2 2H7a2 2 0 0 1-2-2h1a1 1 0 0 0 .883.993L7 16h8a1 1 0 0 0 .993-.883L16 15V6a1 1 0 0 0-.883-.993L15 5h-1V4zm-4-3a2 2 0 0 1 2 2v9a2 2 0 0 1-2 2H3a2 2 0 0 1-2-2V3a2 2 0 0 1 2-2zm0 1H3a1 1 0 0 0-.993.883L2 3v9a1 1 0 0 0 .883.993L3 13h8a1 1 0 0 0 .993-.883L12 12V3a1 1 0 0 0-.883-.993zM9.5 9a.5.5 0 0 1 0 1h-5a.5.5 0 0 1 0-1zm0-2a.5.5 0 0 1 0 1h-5a.5.5 0 0 1 0-1zm0-2a.5.5 0 0 1 0 1h-5a.5.5 0 0 1 0-1z"/></symbol><symbol id="icon-compare" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M12 3a6 6 0 1 1-3 11.197A6 6 0 1 1 9 3.803a5.96 5.96 0 0 1 3-.803M6 4a5 5 0 1 0 2.085 9.546C6.808 12.446 6 10.817 6 9s.808-3.446 2.084-4.546A5 5 0 0 0 6 4m6 0c-.744 0-1.45.162-2.085.454C11.192 5.554 12 7.183 12 9s-.808 3.446-2.084 4.546A5 5 0 1 0 12 4m-1.416 7H7.416q.236.538.585 1.001h1.998q.35-.463.585-1M11 9H7q.001.515.1 1.001h3.8q.099-.486.1-1.001m-.416-2H7.416A5 5 0 0 0 7.1 8h3.8a5 5 0 0 0-.316-1M9.001 5l-.083.063A5 5 0 0 0 8 6h2a5 5 0 0 0-.999-1"/></symbol><symbol id="icon-download-file" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M10.005 0c.55 0 1.318.323 1.707.712l4.576 4.576c.393.393.712 1.168.712 1.714v8.99C17 17.1 16.105 18 15.006 18H2.994A1.994 1.994 0 0 1 1 16.005V1.995C1 .893 1.887 0 3 0zm0 1H3c-.557 0-1 .443-1 .995v14.01c0 .55.445.995.994.995h12.012c.546 0 .994-.45.994-1.009V7.002c0-.283-.215-.803-.419-1.007L11.005 1.42c-.204-.204-.719-.419-1-.419M8.5 5c.276 0 .5.216.5.492v6.148l1.746-1.78a.497.497 0 0 1 .7-.006.5.5 0 0 1-.002.702l-2.591 2.591a.5.5 0 0 1-.706 0l-2.59-2.59a.5.5 0 0 1-.004-.704.49.49 0 0 1 .7.007L8 11.64V5.492C8 5.22 8.232 5 8.5 5"/></symbol><symbol id="icon-download" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M12.998 13a.999.999 0 1 1 0 2H3.002A1.006 1.006 0 0 1 2 14c0-.557.449-1 1.002-1zM8 1c.552 0 1 .445 1 .996v6.802l2.482-2.482c.392-.393 1.022-.401 1.403-.02a1 1 0 0 1 0 1.417l-4.177 4.178a1 1 0 0 1-1.416 0L3.115 7.713a.99.99 0 0 1-.016-1.4 1 1 0 0 1 1.42.002L7 8.798V1.996C7 1.446 7.444 1 8 1"/></symbol><symbol id="icon-editors" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M8.726 2.546A3 3 0 1 0 5.37 7.519l.63.409v1.024l-.79.329A5.22 5.22 0 0 0 2 14.099V15H1v-.901a6.22 6.22 0 0 1 3.825-5.741 4 4 0 1 1 4.976-6.213 5 5 0 0 0-1.075.4M6 17H5v-.901a6.22 6.22 0 0 1 3.825-5.741 4 4 0 1 1 4.349 0A6.22 6.22 0 0 1 17 16.099V17h-1v-.901a5.22 5.22 0 0 0-3.21-4.818l-.79-.33V9.929l.63-.409a3 3 0 1 0-3.26 0l.63.409v1.024l-.79.329A5.22 5.22 0 0 0 6 16.099z"/></symbol><symbol id="icon-email" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M16.005 2A2 2 0 0 1 18 4.006v9.988A2 2 0 0 1 16.005 16H1.995A2 2 0 0 1 0 13.994V4.006A2 2 0 0 1 1.995 2zm0 1H1.995A1 1 0 0 0 1 4.006v9.988A1 1 0 0 0 1.995 15h14.01A1 1 0 0 0 17 13.994V4.006A1 1 0 0 0 16.005 3M16 5.557V7l-7 4-7-4V5.557l7 4z"/></symbol><symbol id="icon-error" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M9 0a9 9 0 1 1 0 18A9 9 0 0 1 9 0m2.863 4.711L9 7.574 6.137 4.711a1.007 1.007 0 0 0-1.427 0 1.006 1.006 0 0 0 .001 1.426L7.574 9l-2.863 2.863a1.007 1.007 0 0 0 0 1.427 1.006 1.006 0 0 0 1.426-.001L9 10.426l2.863 2.863a1.007 1.007 0 0 0 1.427 0 1.006 1.006 0 0 0-.001-1.426L10.426 9l2.863-2.863a1.007 1.007 0 0 0 0-1.427 1.006 1.006 0 0 0-1.426.001"/></symbol><symbol id="icon-ethics" viewBox="0 0 18 18"><path fill-rule="evenodd" d="m6.764 1.414.833-.833a1.984 1.984 0 0 1 2.806 0l.833.833A2 2 0 0 0 12.65 2H14a2 2 0 0 1 2 2v1.35a2 2 0 0 0 .586 1.414l.833.833a1.984 1.984 0 0 1 0 2.806l-.833.833A2 2 0 0 0 16 12.65V14a2 2 0 0 1-2 2h-1.35a2 2 0 0 0-1.414.586l-.833.833a1.984 1.984 0 0 1-2.806 0l-.833-.833A2 2 0 0 0 5.35 16H4a2 2 0 0 1-2-2v-1.35a2 2 0 0 0-.586-1.414l-.833-.833a1.984 1.984 0 0 1 0-2.806l.833-.833A2 2 0 0 0 2 5.35V4a2 2 0 0 1 2-2h1.35a2 2 0 0 0 1.414-.586M5.35 3H4a1 1 0 0 0-1 1v1.35a3 3 0 0 1-.879 2.121l-.833.833a.984.984 0 0 0 0 1.392l.833.833A3 3 0 0 1 3 12.65V14a1 1 0 0 0 1 1h1.35a3 3 0 0 1 2.121.879l.833.833a.984.984 0 0 0 1.392 0l.833-.833A3 3 0 0 1 12.65 15H14a1 1 0 0 0 1-1v-1.35a3 3 0 0 1 .879-2.121l.833-.833a.984.984 0 0 0 0-1.392l-.833-.833A3 3 0 0 1 15 5.35V4a1 1 0 0 0-1-1h-1.35a3 3 0 0 1-2.121-.879l-.833-.833a.984.984 0 0 0-1.392 0l-.833.833A3 3 0 0 1 5.35 3m3.587 11.496a1 1 0 0 1-.064.003 5.5 5.5 0 0 1-2.759-.816.5.5 0 0 1 .526-.85 4.5 4.5 0 0 0 2.256.666q.026 0 .052.004L9 13.5a4.5 4.5 0 0 0 3.493-7.338.5.5 0 1 1 .775-.631A5.5 5.5 0 0 1 9 14.5zm1.58-10.784a.5.5 0 0 1-.276.961 4.5 4.5 0 0 0-1.156-.172C6.527 4.5 4.5 6.503 4.5 9c0 .627.128 1.235.373 1.796a.5.5 0 1 1-.917.4A5.5 5.5 0 0 1 3.5 9c0-3.052 2.477-5.5 5.594-5.5.486.009.963.08 1.422.212m-1.977 6.41 2.698-2.946a.5.5 0 0 1 .762.648l-3.016 3.343a.5.5 0 0 1-.693.047L6.36 9.592A.5.5 0 1 1 7 8.824z"/></symbol><symbol id="icon-expand"><path fill-rule="evenodd" d="M7.498 11.918a.997.997 0 0 0-.003-1.411.995.995 0 0 0-1.412-.003l-4.102 4.102v-3.51A1 1 0 0 0 .98 10.09.99.99 0 0 0 0 11.092V17c0 .554.448 1.002 1.002 1.002h5.907c.554 0 1.002-.45 1.002-1.003 0-.539-.45-.978-1.006-.978h-3.51zm3.005-5.835a.997.997 0 0 0 .003 1.412.995.995 0 0 0 1.411.003l4.103-4.103v3.51a1 1 0 0 0 1.001 1.006A.99.99 0 0 0 18 6.91V1.002A1 1 0 0 0 17 0h-5.907a1.003 1.003 0 0 0-1.002 1.003c0 .539.45.978 1.006.978h3.51z"/></symbol><symbol id="icon-explore" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M9 17A8 8 0 1 0 9 1a8 8 0 0 0 0 16m0 1A9 9 0 1 1 9 0a9 9 0 0 1 0 18m0-2.5a.5.5 0 1 1 0-1 5.5 5.5 0 0 0 5.497-5.317.5.5 0 1 1 1 .032A6.5 6.5 0 0 1 9 15.5M8.71 2.506a.5.5 0 1 1 .043 1A5.5 5.5 0 0 0 3.5 9a.5.5 0 0 1-1 0 6.5 6.5 0 0 1 6.21-6.494m1.137 8.048-1.2-1.2a.5.5 0 1 1 .707-.708l1.2 1.2 1.6-4-4.506 1.802-1.803 4.507zM13.95 4.05l-2.67 6.673a1 1 0 0 1-.557.557L4.05 13.95l2.67-6.673a1 1 0 0 1 .557-.557z"/></symbol><symbol id="icon-filter" viewBox="0 0 16 16"><path d="M14.974 0C15.54 0 16 .448 16 1c0 .242-.09.476-.254.658L10.052 8v5.5a1 1 0 0 1-.41.8L7.59 15.8a1.044 1.044 0 0 1-1.437-.2 1 1 0 0 1-.205-.6V8L.254 1.658A.98.98 0 0 1 .35.248C.537.087.778 0 1.026 0zM9.052 8.5H6.947v6.491l.012.007.02.002.02-.007 2.054-1.5zM1.026 1l-.015.002L6.843 7.5h2.313l5.836-6.499z"/></symbol><symbol id="icon-home" viewBox="0 0 18 18"><path fill-rule="evenodd" d="m9 5-6 6v5h4v-4h4v4h4v-5zm7 6.586V16a1 1 0 0 1-1 1h-5v-4H8v4H3a1 1 0 0 1-1-1v-4.414a1 1 0 0 1-.707-.293l-.586-.586a1 1 0 0 1 0-1.414L9 1l8.293 8.293a1 1 0 0 1 0 1.414l-.586.586a1 1 0 0 1-.707.293M9 2.414 1.414 10l.586.586 7-7 7 7 .586-.586z"/></symbol><symbol id="icon-image" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M10.005 0c.55 0 1.318.323 1.707.712l4.576 4.576c.393.393.712 1.168.712 1.714v8.99C17 17.1 16.105 18 15.006 18H2.994A1.994 1.994 0 0 1 1 16.005V1.995C1 .893 1.887 0 3 0zM6.508 10.175l-3.894 6.75a1 1 0 0 0 .38.075h10.603l-2.433-4.215-1.575 2.728zM15.006 17c.546 0 .994-.45.994-1.009V7.002c0-.283-.215-.803-.419-1.007L11.005 1.42c-.204-.204-.719-.419-1-.419H3c-.557 0-1 .443-1 .995v13.99l4.508-7.81 3.081 5.338 1.575-2.729L14.752 17zM12 6a2 2 0 1 1 0 4 2 2 0 0 1 0-4m0 1a1 1 0 1 0 0 2 1 1 0 0 0 0-2"/></symbol><symbol id="icon-info" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M9 0a9 9 0 1 1 0 18A9 9 0 0 1 9 0m0 7H7.5l-.117.007a1 1 0 0 0-.112 1.967l.112.02L7.5 9H8v3h-.5l-.117.007a1 1 0 0 0-.857.764l-.02.112L6.5 13l.007.117a1 1 0 0 0 .764.857l.112.02L7.5 14h3l.117-.007a1 1 0 0 0 .112-1.967l-.112-.02L10.5 12H10V8l-.007-.117a1 1 0 0 0-.764-.857l-.112-.02zm0-3.25a1.25 1.25 0 1 0 0 2.5 1.25 1.25 0 0 0 0-2.5"/></symbol><symbol id="icon-institution" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M7 17v-2h4v2h2v-3H5v3zM4 6.998H2.474a.5.5 0 0 1-.339-.868l1.232-1.132H1V17h3v-2H1v-1h3v-1h1V8.997H4zm10 0v2h-1V13h1v1h3v1h-3v2h3V4.999h-2.37l1.232 1.132a.5.5 0 0 1-.338.868zm3-3a1 1 0 0 1 1 1V18H0V4.998a1 1 0 0 1 1-1h3.455L8.66.131a.5.5 0 0 1 .677 0l4.205 3.865zm-4 3H5v1h8zm-2 6.001h1V9h-1zm-1 0V9H8v4zm-3 0V9H6v4zm8-4a1 1 0 0 1 1 1v2h-2v-2a1 1 0 0 1 1-1m-12 0a1 1 0 0 1 1 1v2H2v-2a1 1 0 0 1 1-1m5.999-7.82L3.757 5.998H14.24zM9 4.998a1 1 0 1 1 0-2 1 1 0 0 1 0 2M8 16v1h2v-1z"/></symbol><symbol id="icon-location" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M9.395 16.269a32 32 0 0 0 2.333-2.53c1.402-1.702 2.432-3.362 2.936-4.872C14.886 8.2 15 7.574 15 7a6 6 0 0 0-6-6C5.636 1 3 3.602 3 7c0 .574.114 1.199.336 1.867.504 1.51 1.534 3.17 2.936 4.872A32 32 0 0 0 9 16.65zM9 18S2 12 2 7c0-4 3.134-7 7-7a7 7 0 0 1 7 7c0 5-7 11-7 11m0-8a3 3 0 1 1 0-6 3 3 0 0 1 0 6m0-1a2 2 0 1 0 0-4 2 2 0 0 0 0 4"/></symbol><symbol id="icon-minus" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M2 7h12c.552 0 1 .444 1 1 0 .552-.445 1-1 1H2c-.552 0-1-.444-1-1 0-.552.445-1 1-1"/></symbol><symbol id="icon-newsletter" viewBox="0 0 18 18"><path fill-rule="evenodd" d="m9 11.848 2-1.143V9H7v1.705zm-3-1.714V8h6v2.134l3-1.714V2H3v6.42zM16 4.75l1.53.956a1 1 0 0 1 .47.848V15a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2V6.554a1 1 0 0 1 .47-.848L2 4.75V2a1 1 0 0 1 1-1h12a1 1 0 0 1 1 1zm0 1.18V9l-7 4-7-4V5.93l-1 .624V15a1 1 0 0 0 1 1h14a1 1 0 0 0 1-1V6.554zM6 4h6v1H6zM5 6h8v1H5z"/></symbol><symbol id="icon-orcid" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M9 1a8 8 0 1 1 0 16A8 8 0 0 1 9 1M6.099 6.273H4.68v7.171H6.1zm4.559.025H7.665v7.146h2.91q1.152 0 2.042-.445c.89-.445 1.051-.715 1.373-1.255q.484-.81.484-1.86 0-1.028-.481-1.852a3.3 3.3 0 0 0-1.35-1.278q-.87-.456-1.985-.456m-.19 1.08q1.182 0 1.88.67t.698 1.826q0 1.114-.693 1.803t-1.839.688H9.046V7.378zM5.382 3.667a.79.79 0 0 0-.577.253.8.8 0 0 0-.25.58q0 .336.25.59a.818.818 0 0 0 1.177.004.8.8 0 0 0 .25-.593q0-.351-.25-.593c-.25-.242-.366-.241-.6-.241"/></symbol><symbol id="icon-plus" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M2 7h5V2c0-.552.444-1 1-1 .552 0 1 .445 1 1v5h5c.552 0 1 .444 1 1 0 .552-.445 1-1 1H9v5c0 .552-.444 1-1 1-.552 0-1-.445-1-1V9H2c-.552 0-1-.444-1-1 0-.552.445-1 1-1"/></symbol><symbol id="icon-print" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M16.005 5H1.995A.997.997 0 0 0 1 6v6c0 .557.443 1 .995 1H3v-3h12v3h1.005c.55 0 .995-.447.995-1V6c0-.557-.443-1-.995-1M14 4V1.998C14 1.45 13.548 1 12.991 1H5.01C4.452 1 4 1.45 4 1.998V4zm1 10v2.002A2.005 2.005 0 0 1 12.991 18H5.01A2 2 0 0 1 3 16.002V14H1.995A1.99 1.99 0 0 1 0 12V6c0-1.105.893-2 1.995-2H3V1.998C3 .894 3.902 0 5.009 0h7.982C14.101 0 15 .898 15 1.998V4h1.005C17.107 4 18 4.887 18 6v6c0 1.105-.893 2-1.995 2zm-1-3H4v5.002c0 .555.447.998 1.009.998h7.982c.557 0 1.009-.45 1.009-.998zm-9 1h8v1H5zm0 2h5v1H5zm9-5a1 1 0 1 1 0-2 1 1 0 0 1 0 2"/></symbol><symbol id="icon-search" viewBox="0 0 22 22"><path fill-rule="evenodd" d="M21.697 20.261a1.03 1.03 0 0 1 .01 1.448 1.034 1.034 0 0 1-1.448-.01l-4.267-4.267A9.812 9.812 0 0 1 0 9.812a9.812 9.811 0 1 1 17.43 6.182zM9.812 18.222A8.41 8.41 0 1 0 9.81 1.403a8.41 8.41 0 0 0 0 16.82z"/></symbol><symbol id="icon-social-facebook" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M6.004 20A2 2 0 0 1 4 18.006V5.994C4 4.893 4.895 4 5.994 4h12.012C19.107 4 20 4.895 20 5.994v12.012A1.99 1.99 0 0 1 17.997 20H15.04v-6.196h2.08l.31-2.415h-2.39V9.848c0-.7.194-1.176 1.196-1.176h1.28v-2.16l-.235-.025c-.344-.032-.95-.07-1.63-.07-1.843 0-3.105 1.125-3.105 3.191v1.781h-2.085v2.415h2.085V20z"/></symbol><symbol id="icon-social-twitter" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M18.877 6.874a3.77 3.77 0 0 0 1.625-2.096 7.2 7.2 0 0 1-2.347.917A3.64 3.64 0 0 0 15.463 4.5c-2.04 0-3.693 1.696-3.693 3.786q0 .448.094.862C8.8 8.99 6.076 7.483 4.254 5.191c-.317.56-.5 1.211-.5 1.905 0 1.314.653 2.472 1.643 3.152a3.65 3.65 0 0 1-1.672-.473v.045c0 1.834 1.273 3.366 2.964 3.713a3.5 3.5 0 0 1-.975.133c-.236 0-.471-.023-.694-.067.471 1.507 1.832 2.604 3.45 2.633a7.3 7.3 0 0 1-4.59 1.62q-.446 0-.88-.051A10.3 10.3 0 0 0 8.661 19.5c6.793 0 10.506-5.771 10.506-10.775l-.009-.49A7.5 7.5 0 0 0 21 6.275a7.3 7.3 0 0 1-2.122.596z"/></symbol><symbol id="icon-social-youtube" viewBox="0 0 24 24"><path fill-rule="evenodd" d="m10.142 14.397-.001-5.193 4.863 2.606zm9.963-7.927c-.685-.737-1.452-.74-1.804-.783-2.519-.187-6.297-.187-6.297-.187-.008 0-3.786 0-6.305.187-.352.043-1.119.046-1.804.783-.54.56-.715 1.834-.715 1.834S3 9.8 3 11.296v1.402c0 1.496.18 2.991.18 2.991s.176 1.274.715 1.835c.685.736 1.585.713 1.985.79 1.44.142 6.12.186 6.12.186s3.782-.006 6.301-.193c.352-.043 1.119-.047 1.804-.783.539-.561.715-1.835.715-1.835s.18-1.495.18-2.991v-1.402c0-1.496-.18-2.992-.18-2.992s-.176-1.273-.715-1.834"/></symbol><symbol id="icon-subject-medicine" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M12.5 8H6a3 3 0 0 0-3 3v1a3 3 0 0 0 3 3h1v-2h-.5a1.5 1.5 0 0 1 0-3H13a3 3 0 0 0 3-3V6a3 3 0 0 0-3-3h-2v2h1.5a1.5 1.5 0 0 1 0 3M7 7V6H3.5a2.5 2.5 0 0 1 0-5h1.028a3 3 0 0 1 1.342.317l1.133.567A2 2 0 0 1 11 2h2a4 4 0 0 1 4 4v1a4 4 0 0 1-4 4h-2v1h2a2 2 0 1 1 0 4h-2a2 2 0 1 1-4 0H6a4 4 0 0 1-4-4v-1a4 4 0 0 1 4-4zm0-2V2.943a2 2 0 0 1-.422-.154L5.422 2.21A2 2 0 0 0 4.528 2H3.5a1.5 1.5 0 0 0 0 3zm4 1v1h1.5a.5.5 0 1 0 0-1zm-1 1V2a1 1 0 1 0-2 0v5zm-2 4v5a1 1 0 0 0 2 0v-5zm3 2v2h2a1 1 0 0 0 0-2zm-4-1v-1h-.5a.5.5 0 1 0 0 1zM3.5 3h1a.5.5 0 0 1 0 1h-1a.5.5 0 0 1 0-1"/></symbol><symbol id="icon-success" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M9 0a9 9 0 1 1 0 18A9 9 0 0 1 9 0m3.486 4.982-4.718 5.506L5.14 8.465a.99.99 0 0 0-1.423.133 1.06 1.06 0 0 0 .13 1.463l3.407 2.733a1 1 0 0 0 1.387-.133l5.385-6.334a1.06 1.06 0 0 0-.116-1.464.99.99 0 0 0-1.424.119"/></symbol><symbol id="icon-table" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M16.005 2A2 2 0 0 1 18 4.006v9.988A2 2 0 0 1 16.005 16l-4.006-.001L12 16h-1l-.001-.001h-5L6 16H5l-.001-.001L1.995 16A2 2 0 0 1 0 13.994V4.006A2 2 0 0 1 1.995 2zM4.999 7H1v6.994A1 1 0 0 0 1.995 15H5zm6 0h-5v8h5zm5.006-4h-4.006v3H17v1h-5.001v7.999l4.006.001A1 1 0 0 0 17 13.994V4.006A1 1 0 0 0 16.005 3M3.5 12a.5.5 0 1 1 0 1h-1a.5.5 0 1 1 0-1zm12 0a.5.5 0 1 1 0 1h-2a.5.5 0 1 1 0-1zm-6 0a.5.5 0 1 1 0 1h-2a.5.5 0 1 1 0-1zm-6-2a.5.5 0 1 1 0 1h-1a.5.5 0 1 1 0-1zm12 0a.5.5 0 1 1 0 1h-2a.5.5 0 1 1 0-1zm-6 0a.5.5 0 1 1 0 1h-2a.5.5 0 1 1 0-1zm-6-2a.5.5 0 0 1 0 1h-1a.5.5 0 0 1 0-1zm12 0a.5.5 0 1 1 0 1h-2a.5.5 0 1 1 0-1zm-6 0a.5.5 0 0 1 0 1h-2a.5.5 0 0 1 0-1zm1.499-5h-5v3h5zm-6 0H1.995A1 1 0 0 0 1 4.006V6h3.999z"/></symbol><symbol id="icon-tick-circle" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M12 2c5.523 0 10 4.477 10 10s-4.477 10-10 10S2 17.523 2 12 6.477 2 12 2m0 1a9 9 0 1 0 0 18 9 9 0 0 0 0-18m4.22 5.366c.361-.437.999-.49 1.424-.119s.477 1.028.115 1.465l-6.093 6.944a1 1 0 0 1-1.405.097l-3.897-3.367c-.43-.368-.487-1.023-.13-1.464s.994-.5 1.423-.133l3.111 2.7z"/></symbol><symbol id="icon-tick" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M6.768 9.211 3.657 6.628c-.429-.352-1.066-.295-1.423.127s-.3 1.049.13 1.4l3.915 3.24a1 1 0 0 0 1.375-.096l6.105-6.66a.984.984 0 0 0-.115-1.402 1.02 1.02 0 0 0-1.424.113z"/></symbol><symbol id="icon-update" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M1 13v1a1 1 0 0 0 1 1h14a1 1 0 0 0 1-1v-1h-1V3H2v10zm16-1h1v2a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2v-2h1V3a1 1 0 0 1 1-1h14a1 1 0 0 1 1 1zm-1 0v1h-4.586l-1 1H7.586l-1-1H2v-1h5l1 1h2l1-1zM3 4h12v7H3zm1 1v5h10V5zm1 1h4v1H5zm0 2h4v1H5z"/></symbol><symbol id="icon-upload" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M10.005 0c.55 0 1.318.323 1.707.712l4.576 4.576c.393.393.712 1.168.712 1.714v8.99C17 17.1 16.105 18 15.006 18H2.994A1.994 1.994 0 0 1 1 16.005V1.995C1 .893 1.887 0 3 0zm0 1H3c-.557 0-1 .443-1 .995v14.01c0 .55.445.995.994.995h12.012c.546 0 .994-.45.994-1.009V7.002c0-.283-.215-.803-.419-1.007L11.005 1.42c-.204-.204-.719-.419-1-.419M8.149 5.146a.5.5 0 0 1 .705 0l2.591 2.59c.195.196.19.516.003.704a.49.49 0 0 1-.7-.007l-1.746-1.78V12.8a.501.501 0 0 1-1 0V6.653l-1.747 1.78a.497.497 0 0 1-.7.006.5.5 0 0 1 .003-.702z"/></symbol><symbol id="icon-video" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M16.005 2A2 2 0 0 1 18 4.006v9.988A2 2 0 0 1 16.005 16H1.995A2 2 0 0 1 0 13.994V4.006A2 2 0 0 1 1.995 2zm0 1H1.995A1 1 0 0 0 1 4.006v9.988A1 1 0 0 0 1.995 15h14.01A1 1 0 0 0 17 13.994V4.006A1 1 0 0 0 16.005 3m-8.31 2.25L12.3 7.99c.937.557.93 1.464 0 2.017l-4.604 2.74c-.937.557-1.696.165-1.696-.894v-5.71c0-1.05.767-1.446 1.696-.894m-.674.96v5.575c0 .3-.108.245.166.085l4.584-2.675c.305-.178.305-.216 0-.394L7.187 6.126c-.267-.156-.166-.207-.166.084"/></symbol><symbol id="icon-warning" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M9 11.75a1.25 1.25 0 1 1 0 2.5 1.25 1.25 0 0 1 0-2.5M9.413 4a1 1 0 0 1 1 1l-.003.083-.334 4A1 1 0 0 1 9.08 10h-.16a1 1 0 0 1-.996-.917l-.334-4a1 1 0 0 1 .914-1.08l.041-.002zM9 18A9 9 0 1 1 9 0a9 9 0 0 1 0 18"/></symbol><symbol id="icon-checklist-banner" viewBox="0 0 56.69 56.69"><path d="M0 0h56.69v56.69H0z" style="fill:none"/><clipPath id="b"><use xlink:href="#a" style="overflow:visible"/></clipPath><path d="M21.14 34.46c0-6.77 5.48-12.26 12.24-12.26s12.24 5.49 12.24 12.26-5.48 12.26-12.24 12.26c-6.76-.01-12.24-5.49-12.24-12.26zm19.33 10.66 10.23 9.22s1.21 1.09 2.3-.12l2.09-2.32s1.09-1.21-.12-2.3l-10.23-9.22m-19.29-5.92c0-4.38 3.55-7.94 7.93-7.94s7.93 3.55 7.93 7.94c0 4.38-3.55 7.94-7.93 7.94-4.38-.01-7.93-3.56-7.93-7.94zm17.58 12.99 4.14-4.81" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round"/><path d="M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5m14.42-5.2V4.86s0-2.93-2.93-2.93H4.13s-2.93 0-2.93 2.93v37.57s0 2.93 2.93 2.93h15.01M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round;stroke-linejoin:round"/></symbol><symbol id="icon-chevron-down" viewBox="0 0 16 16"><path fill-rule="evenodd" d="m8 6.586 3.293-3.293a1 1 0 0 1 1.414 1.414l-4 4a1 1 0 0 1-1.414 0l-4-4a1 1 0 0 1 1.414-1.414z"/></symbol><symbol id="icon-eds-i-arrow-right-medium" viewBox="0 0 24 24"><path d="m12.728 3.293 7.98 7.99a1 1 0 0 1 .281.561l.011.157c0 .32-.15.605-.384.788l-7.908 7.918a1 1 0 0 1-1.416-1.414L17.576 13H4a1 1 0 0 1 0-2h13.598l-6.285-6.293a1 1 0 0 1-.082-1.32l.083-.095a1 1 0 0 1 1.414.001"/></symbol><symbol id="icon-eds-i-check-circle-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1m0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18m5.125 4.72a1 1 0 0 1 .156 1.405l-6 7.5a1 1 0 0 1-1.421.143l-3-2.5a1 1 0 0 1 1.28-1.536l2.217 1.846 5.362-6.703a1 1 0 0 1 1.406-.156Z"/></symbol><symbol id="icon-eds-i-chevron-down-medium" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M2 7h5V2c0-.552.444-1 1-1 .552 0 1 .445 1 1v5h5c.552 0 1 .444 1 1 0 .552-.445 1-1 1H9v5c0 .552-.444 1-1 1-.552 0-1-.445-1-1V9H2c-.552 0-1-.444-1-1 0-.552.445-1 1-1"/></symbol><symbol id="icon-eds-i-chevron-down-small" viewBox="0 0 16 16"><path d="M13.692 5.278a1 1 0 0 1 .03 1.414L9.103 11.51a1.49 1.49 0 0 1-2.188.019L2.278 6.692a1 1 0 0 1 1.444-1.384L8 9.771l4.278-4.463a1 1 0 0 1 1.318-.111z"/></symbol><symbol id="icon-eds-i-chevron-right-medium" viewBox="0 0 10 10"><path fill-rule="evenodd" d="M4.706 4.033 2.292 1.637A.953.953 0 0 1 2.273.283a.98.98 0 0 1 1.378 0l4.063 4.031c.381.378.381.99 0 1.367L3.651 9.712a.97.97 0 0 1-1.362.016.96.96 0 0 1 .003-1.37l2.414-2.395L5.693 5z"/></symbol><symbol id="icon-eds-i-chevron-right-small" viewBox="0 0 10 10"><path fill-rule="evenodd" d="M4.706 4.033 2.292 1.637A.953.953 0 0 1 2.273.283a.98.98 0 0 1 1.378 0l4.063 4.031c.381.378.381.99 0 1.367L3.651 9.712a.97.97 0 0 1-1.362.016.96.96 0 0 1 .003-1.37l2.414-2.395L5.693 5z"/></symbol><symbol id="icon-eds-i-chevron-up-medium" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M2 7h12c.552 0 1 .444 1 1 0 .552-.445 1-1 1H2c-.552 0-1-.444-1-1 0-.552.445-1 1-1"/></symbol><symbol id="icon-eds-i-close-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1m0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18M8.707 7.293 12 10.585l3.293-3.292a1 1 0 0 1 1.414 1.414L13.415 12l3.292 3.293a1 1 0 0 1-1.414 1.414L12 13.415l-3.293 3.292a1 1 0 1 1-1.414-1.414L10.585 12 7.293 8.707a1 1 0 0 1 1.414-1.414"/></symbol><symbol id="icon-eds-i-copy-link" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M19.46 8.57a1 1 0 0 1 0-1.414l.833-.833A1.946 1.946 0 0 0 17.54 3.57l-.833.833a1 1 0 0 1-1.414-1.414l.833-.833a3.946 3.946 0 1 1 5.581 5.58l-.833.834a1 1 0 0 1-1.415 0" clip-rule="evenodd"/><path fill-rule="evenodd" d="M18.094 5.906a1 1 0 0 1 0 1.414l-1.666 1.667a1 1 0 1 1-1.414-1.414l1.666-1.667a1 1 0 0 1 1.414 0" clip-rule="evenodd"/><path fill-rule="evenodd" d="M13.511 6.322a1 1 0 0 1 0 1.415l-.833.833a1.946 1.946 0 1 0 2.752 2.752l.834-.833a1 1 0 0 1 1.414 1.414l-.834.834a3.946 3.946 0 1 1-5.58-5.581l.833-.834a1 1 0 0 1 1.414 0" clip-rule="evenodd"/><path d="M8 20v2h11.462c.674 0 1.32-.269 1.796-.747A2.55 2.55 0 0 0 22 19.455V15a1 1 0 1 0-2 0v4.455a.55.55 0 0 1-.16.387.54.54 0 0 1-.378.158z"/><path d="M4 13H2v6.462c0 .674.269 1.32.747 1.796A2.55 2.55 0 0 0 4.545 22H9a1 1 0 1 0 0-2H4.545a.55.55 0 0 1-.387-.16.54.54 0 0 1-.158-.378zM4 13H2V4.538c0-.674.269-1.32.747-1.796A2.55 2.55 0 0 1 4.545 2H9a1 1 0 0 1 0 2H4.545a.55.55 0 0 0-.387.16.54.54 0 0 0-.158.378z"/></symbol><symbol id="icon-eds-i-download-medium" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M12.998 13a.999.999 0 1 1 0 2H3.002A1.006 1.006 0 0 1 2 14c0-.557.449-1 1.002-1zM8 1c.552 0 1 .445 1 .996v6.802l2.482-2.482c.392-.393 1.022-.401 1.403-.02a1 1 0 0 1 0 1.417l-4.177 4.178a1 1 0 0 1-1.416 0L3.115 7.713a.99.99 0 0 1-.016-1.4 1 1 0 0 1 1.42.002L7 8.798V1.996C7 1.446 7.444 1 8 1"/></symbol><symbol id="icon-eds-i-info-filled-medium" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M9 0a9 9 0 1 1 0 18A9 9 0 0 1 9 0m0 7H7.5l-.117.007a1 1 0 0 0-.112 1.967l.112.02L7.5 9H8v3h-.5l-.117.007a1 1 0 0 0-.857.764l-.02.112L6.5 13l.007.117a1 1 0 0 0 .764.857l.112.02L7.5 14h3l.117-.007a1 1 0 0 0 .112-1.967l-.112-.02L10.5 12H10V8l-.007-.117a1 1 0 0 0-.764-.857l-.112-.02zm0-3.25a1.25 1.25 0 1 0 0 2.5 1.25 1.25 0 0 0 0-2.5"/></symbol><symbol id="icon-eds-i-institution-medium" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M11.997 1c-.36 0-.718.09-1.036.265a1 1 0 0 0-.085.052L2.798 6.87a1.7 1.7 0 0 0-.72.897c-.13.389-.098.79.049 1.134C2.42 9.592 3.128 10 3.9 10H5v6h-.5A2.5 2.5 0 0 0 2 18.5v1A2.5 2.5 0 0 0 4.5 22h15a2.5 2.5 0 0 0 2.5-2.5v-1a2.5 2.5 0 0 0-2.5-2.5H19v-6h1.092c.773 0 1.48-.407 1.775-1.1.146-.344.178-.744.048-1.133a1.7 1.7 0 0 0-.72-.897l-8.078-5.553a1 1 0 0 0-.085-.052A2.15 2.15 0 0 0 11.997 1M4.684 8l7.263-4.992a.16.16 0 0 1 .1 0L19.308 8zM17 16v-6h-2v6zm-4 0v-6h-2v6zm-4 0v-6H7v6zm-5 2.5a.5.5 0 0 1 .5-.5h15a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-15a.5.5 0 0 1-.5-.5z" clip-rule="evenodd"/></symbol><symbol id="icon-eds-i-mail-medium" viewBox="0 0 22 18"><path d="M19.462 0C20.875 0 22 1.184 22 2.619v12.762C22 16.816 20.875 18 19.462 18H2.538C1.125 18 0 16.816 0 15.381V2.619C0 1.184 1.125 0 2.538 0zM20 5.158l-7.378 6.258a2.55 2.55 0 0 1-3.253-.008L2 5.16v10.222c0 .353.253.619.538.619h16.924c.285 0 .538-.266.538-.619zM19.462 2H2.538c-.264 0-.5.228-.534.542l8.65 7.334c.2.165.492.165.684.007l8.656-7.342-.001-.025c-.044-.3-.274-.516-.531-.516"/></symbol><symbol id="icon-eds-i-marker-unfilled"><path d="M17.8 3.578c0-.248-.153-.4-.4-.4H6.5c-.248 0-.4.152-.4.4v17.046l5.325-3.737.135-.08a1 1 0 0 1 1.014.08l5.226 3.666zm2 17.527c0 .661-.347 1.231-.895 1.521a1.75 1.75 0 0 1-1.78-.102l-5.126-3.598-5.125 3.598-.06.039a1.84 1.84 0 0 1-1.73.084 1.7 1.7 0 0 1-.984-1.542V3.578c0-1.353 1.048-2.4 2.4-2.4h10.9c1.352 0 2.4 1.047 2.4 2.4z"/></symbol><symbol id="icon-eds-i-menu-medium" viewBox="0 0 24 24"><path d="M21 4a1 1 0 0 1 0 2H3a1 1 0 1 1 0-2zm-4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2zm4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2z"/></symbol><symbol id="icon-eds-i-search-medium" viewBox="0 0 24 24"><path d="M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.96 9.96 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1m0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16"/></symbol><symbol id="icon-eds-i-user-single-medium" viewBox="0 0 24 24"><path d="M12 1a5 5 0 1 1 0 10 5 5 0 0 1 0-10m0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6m-.406 9.008a8.97 8.97 0 0 1 6.596 2.494A9.16 9.16 0 0 1 21 21.025V22a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1v-.985c.05-4.825 3.815-8.777 8.594-9.007m.39 1.992-.299.006c-3.63.175-6.518 3.127-6.678 6.775L5 21h13.998l-.009-.268a7.16 7.16 0 0 0-1.97-4.573l-.214-.213A6.97 6.97 0 0 0 11.984 14"/></symbol><symbol id="icon-eds-i-warning-filled-medium" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M9 11.75a1.25 1.25 0 1 1 0 2.5 1.25 1.25 0 0 1 0-2.5M9.413 4a1 1 0 0 1 1 1l-.003.083-.334 4A1 1 0 0 1 9.08 10h-.16a1 1 0 0 1-.996-.917l-.334-4a1 1 0 0 1 .914-1.08l.041-.002zM9 18A9 9 0 1 1 9 0a9 9 0 0 1 0 18"/></symbol><symbol id="icon-expand-image" viewBox="0 0 18 18"><path fill-rule="evenodd" d="M7.498 11.918a.997.997 0 0 0-.003-1.411.995.995 0 0 0-1.412-.003l-4.102 4.102v-3.51A1 1 0 0 0 .98 10.09.99.99 0 0 0 0 11.092V17c0 .554.448 1.002 1.002 1.002h5.907c.554 0 1.002-.45 1.002-1.003 0-.539-.45-.978-1.006-.978h-3.51zm3.005-5.835a.997.997 0 0 0 .003 1.412.995.995 0 0 0 1.411.003l4.103-4.103v3.51a1 1 0 0 0 1.001 1.006A.99.99 0 0 0 18 6.91V1.002C18 .448 17.553 0 17 0h-5.907c-.554.001-1.002.45-1.002 1.003 0 .539.45.978 1.006.978h3.51z"/></symbol><symbol id="icon-github" viewBox="0 0 100 100"><path fill-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a47 47 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0" clip-rule="evenodd"/></symbol><symbol id="icon-orcid-logo" viewBox="0 0 40 40"><path fill-rule="evenodd" d="M12.281 10.453c.875 0 1.578-.719 1.578-1.578s-.703-1.578-1.578-1.578-1.578.703-1.578 1.578c0 .86.703 1.578 1.578 1.578m-1.203 18.641h2.406V12.359h-2.406z"/><path fill-rule="evenodd" d="M17.016 12.36h6.5c6.187 0 8.906 4.421 8.906 8.374 0 4.297-3.36 8.375-8.875 8.375h-6.531zm6.234 14.578h-3.828V14.53h3.703c4.688 0 6.828 2.844 6.828 6.203 0 2.063-1.25 6.203-6.703 6.203Z" clip-rule="evenodd"/></symbol><symbol id="icon-springer-arrow-left"><path d="M15 7a1 1 0 0 0 0-2H3.385l2.482-2.482a.994.994 0 0 0 .02-1.403 1 1 0 0 0-1.417 0L.294 5.292a1 1 0 0 0 0 1.416l4.176 4.177a.99.99 0 0 0 1.4.016 1 1 0 0 0-.003-1.42L3.385 7z"/></symbol><symbol id="icon-springer-arrow-right"><path d="M1 7a1 1 0 0 1 0-2h11.615l-2.482-2.482a.994.994 0 0 1-.02-1.403 1 1 0 0 1 1.417 0l4.176 4.177a1 1 0 0 1 0 1.416l-4.176 4.177a.99.99 0 0 1-1.4.016 1 1 0 0 1 .003-1.42L12.615 7z"/></symbol><symbol id="icon-submit-open" viewBox="0 0 16 17"><path d="M12 0a2 2 0 0 1 2 2v5a.5.5 0 0 1-1 0V2a1 1 0 0 0-.883-.993L12 1H6v3a2 2 0 0 1-2 2H1v8a1 1 0 0 0 .883.993L2 15h6.5a.5.5 0 0 1 0 1H2a2 2 0 0 1-2-2V5.828a2 2 0 0 1 .586-1.414L4.414.586A2 2 0 0 1 5.828 0zm3.41 11.14c.25.25.25.66 0 .91a.636.636 0 0 1-.9-.01l-1.864-1.9.001 5.87a.64.64 0 0 1-.647.636.65.65 0 0 1-.647-.637l-.001-5.87L9.5 12.04a.627.627 0 0 1-.9.01.65.65 0 0 1 0-.91l2.942-2.951a.647.647 0 0 1 .914 0zM5 1.413 1.413 5H4a1 1 0 0 0 1-1zM11 3a.5.5 0 0 1 0 1H7.5a.5.5 0 0 1 0-1zm0 2a.5.5 0 0 1 0 1H7.5a.5.5 0 0 1 0-1z"/></symbol></svg>
</div>
</footer>




    

    

<div class="c-site-messages message u-hide u-hide-print c-site-messages--nature-briefing c-site-messages--nature-briefing-email-variant c-site-messages--nature-briefing-redesign-2020 sans-serif "
data-component-id="nature-briefing-banner"
data-component-expirydays="30"
data-component-trigger-scroll-percentage="15"
data-track="in-view"
data-track-action="in-view"
data-track-category="nature briefing"
data-track-label="Briefing banner visible: Flagship">

    
    <div class="c-site-messages__banner-large">

        
<div class="c-site-messages__close-container">
    <button class="c-site-messages__close"
        data-track="click"
        data-track-category="nature briefing"
        data-track-label="Briefing banner dismiss: Flagship">
        <svg width="25px" height="25px" focusable="false" aria-hidden="true" viewBox="0 0 25 25" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <title>Close banner</title>
            <defs></defs>
            <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                <rect opacity="0" x="0" y="0" width="25" height="25"></rect>
                <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#ffffff"></path>
            </g>
        </svg>
        <span class="visually-hidden">Close</span>
    </button>
</div>


        <div class="c-site-messages__form-container">

            <div class="grid grid-12 last">
                <div class="grid grid-4">
                    <img alt="Nature Briefing" src="/static/images/logos/nature-briefing-logo-n150-white-afc2e6ccc7.svg" width="250" height="40">
                    <p class="c-site-messages--nature-briefing__strapline extra-tight-line-height">Sign up for the <em>Nature Briefing</em> newsletter — what matters in science, free to your inbox daily.</p>
                </div>
                <div class="grid grid-8 last">
                    <form action="https://www.nature.com/briefing/briefing" method="post" data-location="banner" data-track="signup_nature_briefing_banner" data-track-action="transmit-form" data-track-category="nature briefing" data-track-label="Briefing banner submit: Flagship">
                        <input id="briefing-banner-signup-form-input-track-originReferralPoint" type="hidden" name="track_originReferralPoint" value="MainBriefingBanner">
                        <input id="briefing-banner-signup-form-input-track-formType" type="hidden" name="track_formType" value="DirectEmailBanner">

                        <input type="hidden" value="false" name="gdpr_tick" id="gdpr_tick_banner">
                        <input type="hidden" value="false" name="marketing" id="marketing_input_banner">
                        <input type="hidden" value="false" name="marketing_tick" id="marketing_tick_banner">
                        <input type="hidden" value="MainBriefingBanner" name="brieferEntryPoint" id="brieferEntryPoint_banner">

                        <label class="nature-briefing-banner__email-label" for="emailAddress">Email address</label>

                        <div class="nature-briefing-banner__email-wrapper">
                            <input class="nature-briefing-banner__email-input box-sizing text14" type="email" id="emailAddress" name="emailAddress" value="" placeholder="e.g. jo.smith@university.ac.uk" required data-test-element="briefing-emailbanner-email-input">
                            
                            <input type="hidden" value="true" name="N:nature_briefing_daily" id="defaultNewsletter_banner">
                            <button type="submit" class="nature-briefing-banner__submit-button box-sizing text14" data-test-element="briefing-emailbanner-signup-button">Sign up</button>
                        </div>

                        <div class="nature-briefing-banner__checkbox-wrapper grid grid-12 last">
                            <input class="nature-briefing-banner__checkbox-checkbox" id="gdpr-briefing-banner-checkbox" type="checkbox" name="gdpr" value="true" data-test-element="briefing-emailbanner-gdpr-checkbox" required>
                            <label class="nature-briefing-banner__checkbox-label box-sizing text13 sans-serif block tighten-line-height" for="gdpr-briefing-banner-checkbox">I agree my information will be processed in accordance with the <em>Nature</em> and Springer Nature Limited <a href="https://www.nature.com/info/privacy">Privacy Policy</a>.</label>
                        </div>
                    </form>
                </div>
            </div>

        </div>

    </div>

    
    <div class="c-site-messages__banner-small">

        
<div class="c-site-messages__close-container">
    <button class="c-site-messages__close"
        data-track="click"
        data-track-category="nature briefing"
        data-track-label="Briefing banner dismiss: Flagship">
        <svg width="25px" height="25px" focusable="false" aria-hidden="true" viewBox="0 0 25 25" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <title>Close banner</title>
            <defs></defs>
            <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                <rect opacity="0" x="0" y="0" width="25" height="25"></rect>
                <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#ffffff"></path>
            </g>
        </svg>
        <span class="visually-hidden">Close</span>
    </button>
</div>


        <div class="c-site-messages__content text14">
            <span class="c-site-messages--nature-briefing__strapline strong">Get the most important science stories of the day, free in your inbox.</span>
            <a class="nature-briefing__link text14 sans-serif"
                data-track="click"
                data-track-category="nature briefing"
                data-track-label="Small-screen banner CTA to site"
                data-test-element="briefing-banner-link"
                target="_blank"
                rel="noreferrer noopener"
                href="https://www.nature.com/briefing/signup/?brieferEntryPoint=MainBriefingBanner">Sign up for Nature Briefing
            </a>
        </div>

    </div>

</div>






<noscript>
    <img hidden src="https://verify.nature.com/verify/nature.png" width="0" height="0" style="display: none" alt="">
</noscript>




<script src="//content.readcube.com/ping?doi=10.1038/s41598-023-31500-3&amp;format=js&amp;last_modified=2023-05-11" async></script>
</body>
</html>